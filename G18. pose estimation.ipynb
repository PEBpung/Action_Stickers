{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 18. 행동 스티커 만들기\n",
    "\n",
    "### STEP 1 : simplebaseline 모델 완성하기\n",
    "simplebaseline.py 파일 내용을 완성합니다.\n",
    "\n",
    "### STEP 2 : simplebaseline 모델로 변경하여 훈련하기\n",
    "train.py 218라인의 모델 선언 부분을 simplebaseline 모델로 변경한 후 다시 학습을 진행합니다.\n",
    "\n",
    "### STEP 3 : 두 모델의 비교\n",
    "실습에서 다룬 StackedHourglass Network와 Simplebaseline 모델을 둘 다 동일한 Epoch 수만큼 학습하여 그 결과를 비교해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackedHourglass Network 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "WARNING:tensorflow:From /home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "WARNING:tensorflow:From train.py:111: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "renamed to `run`\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 4.85859823 epoch total loss 4.85859823\n",
      "Trained batch 2 batch loss 4.39312506 epoch total loss 4.62586164\n",
      "Trained batch 3 batch loss 4.88813591 epoch total loss 4.7132864\n",
      "Trained batch 4 batch loss 4.55008078 epoch total loss 4.67248487\n",
      "Trained batch 5 batch loss 4.79106712 epoch total loss 4.69620132\n",
      "Trained batch 6 batch loss 4.58076715 epoch total loss 4.67696238\n",
      "Trained batch 7 batch loss 4.55900049 epoch total loss 4.66011047\n",
      "Trained batch 8 batch loss 4.64461708 epoch total loss 4.65817356\n",
      "Trained batch 9 batch loss 4.32287 epoch total loss 4.62091732\n",
      "Trained batch 10 batch loss 4.52796 epoch total loss 4.61162186\n",
      "Trained batch 11 batch loss 4.17739487 epoch total loss 4.57214689\n",
      "Trained batch 12 batch loss 4.36352539 epoch total loss 4.55476141\n",
      "Trained batch 13 batch loss 4.15680599 epoch total loss 4.52415\n",
      "Trained batch 14 batch loss 4.38350201 epoch total loss 4.51410341\n",
      "Trained batch 15 batch loss 4.38776112 epoch total loss 4.50568104\n",
      "Trained batch 16 batch loss 4.49342728 epoch total loss 4.50491524\n",
      "Trained batch 17 batch loss 4.19440126 epoch total loss 4.48665\n",
      "Trained batch 18 batch loss 4.30542707 epoch total loss 4.47658205\n",
      "Trained batch 19 batch loss 4.3569088 epoch total loss 4.47028351\n",
      "Trained batch 20 batch loss 4.14023399 epoch total loss 4.45378113\n",
      "Trained batch 21 batch loss 4.16071796 epoch total loss 4.43982601\n",
      "Trained batch 22 batch loss 4.25290108 epoch total loss 4.43132925\n",
      "Trained batch 23 batch loss 3.88631392 epoch total loss 4.40763283\n",
      "Trained batch 24 batch loss 3.71389818 epoch total loss 4.37872744\n",
      "Trained batch 25 batch loss 3.39416313 epoch total loss 4.33934498\n",
      "Trained batch 26 batch loss 3.41458344 epoch total loss 4.30377674\n",
      "Trained batch 27 batch loss 3.35569191 epoch total loss 4.26866245\n",
      "Trained batch 28 batch loss 3.51313496 epoch total loss 4.24167967\n",
      "Trained batch 29 batch loss 3.23772407 epoch total loss 4.20706034\n",
      "Trained batch 30 batch loss 3.47770238 epoch total loss 4.18274832\n",
      "Trained batch 31 batch loss 3.86615419 epoch total loss 4.17253542\n",
      "Trained batch 32 batch loss 3.62483025 epoch total loss 4.15542\n",
      "Trained batch 33 batch loss 3.58487988 epoch total loss 4.13813066\n",
      "Trained batch 34 batch loss 3.49031925 epoch total loss 4.11907768\n",
      "Trained batch 35 batch loss 3.41022134 epoch total loss 4.0988245\n",
      "Trained batch 36 batch loss 3.27393866 epoch total loss 4.07591105\n",
      "Trained batch 37 batch loss 3.32937336 epoch total loss 4.05573463\n",
      "Trained batch 38 batch loss 3.58476663 epoch total loss 4.04334068\n",
      "Trained batch 39 batch loss 3.61417103 epoch total loss 4.03233624\n",
      "Trained batch 40 batch loss 3.7670598 epoch total loss 4.02570438\n",
      "Trained batch 41 batch loss 3.64445925 epoch total loss 4.01640558\n",
      "Trained batch 42 batch loss 3.0313127 epoch total loss 3.99295092\n",
      "Trained batch 43 batch loss 3.35539889 epoch total loss 3.9781239\n",
      "Trained batch 44 batch loss 2.99469328 epoch total loss 3.95577312\n",
      "Trained batch 45 batch loss 2.74674511 epoch total loss 3.92890596\n",
      "Trained batch 46 batch loss 3.10032177 epoch total loss 3.9108932\n",
      "Trained batch 47 batch loss 2.66052675 epoch total loss 3.88428974\n",
      "Trained batch 48 batch loss 3.65007925 epoch total loss 3.87941051\n",
      "Trained batch 49 batch loss 3.46997237 epoch total loss 3.87105441\n",
      "Trained batch 50 batch loss 3.57896662 epoch total loss 3.86521268\n",
      "Trained batch 51 batch loss 3.15303874 epoch total loss 3.85124874\n",
      "Trained batch 52 batch loss 3.62330842 epoch total loss 3.84686518\n",
      "Trained batch 53 batch loss 3.84280252 epoch total loss 3.84678841\n",
      "Trained batch 54 batch loss 3.7715168 epoch total loss 3.84539461\n",
      "Trained batch 55 batch loss 3.64392185 epoch total loss 3.84173131\n",
      "Trained batch 56 batch loss 3.73747277 epoch total loss 3.83986974\n",
      "Trained batch 57 batch loss 3.75245237 epoch total loss 3.83833599\n",
      "Trained batch 58 batch loss 3.53665638 epoch total loss 3.83313465\n",
      "Trained batch 59 batch loss 3.42615032 epoch total loss 3.82623649\n",
      "Trained batch 60 batch loss 3.8898139 epoch total loss 3.82729626\n",
      "Trained batch 61 batch loss 3.65891147 epoch total loss 3.82453561\n",
      "Trained batch 62 batch loss 3.55077529 epoch total loss 3.82012033\n",
      "Trained batch 63 batch loss 3.6391933 epoch total loss 3.81724834\n",
      "Trained batch 64 batch loss 3.50662065 epoch total loss 3.81239486\n",
      "Trained batch 65 batch loss 3.56079531 epoch total loss 3.80852413\n",
      "Trained batch 66 batch loss 3.41110182 epoch total loss 3.80250239\n",
      "Trained batch 67 batch loss 3.39051318 epoch total loss 3.79635358\n",
      "Trained batch 68 batch loss 3.20490503 epoch total loss 3.78765559\n",
      "Trained batch 69 batch loss 3.48786664 epoch total loss 3.78331065\n",
      "Trained batch 70 batch loss 3.58271217 epoch total loss 3.78044486\n",
      "Trained batch 71 batch loss 3.49931169 epoch total loss 3.77648497\n",
      "Trained batch 72 batch loss 3.44271779 epoch total loss 3.77184939\n",
      "Trained batch 73 batch loss 3.45021296 epoch total loss 3.76744342\n",
      "Trained batch 74 batch loss 3.53750944 epoch total loss 3.76433635\n",
      "Trained batch 75 batch loss 3.44310665 epoch total loss 3.7600534\n",
      "Trained batch 76 batch loss 3.46874142 epoch total loss 3.75622034\n",
      "Trained batch 77 batch loss 3.65334463 epoch total loss 3.75488448\n",
      "Trained batch 78 batch loss 3.70681167 epoch total loss 3.75426817\n",
      "Trained batch 79 batch loss 3.56598186 epoch total loss 3.7518847\n",
      "Trained batch 80 batch loss 3.6461277 epoch total loss 3.75056267\n",
      "Trained batch 81 batch loss 3.09034348 epoch total loss 3.74241161\n",
      "Trained batch 82 batch loss 3.22376108 epoch total loss 3.73608661\n",
      "Trained batch 83 batch loss 3.28474188 epoch total loss 3.73064852\n",
      "Trained batch 84 batch loss 3.31636262 epoch total loss 3.72571683\n",
      "Trained batch 85 batch loss 3.06059957 epoch total loss 3.71789193\n",
      "Trained batch 86 batch loss 3.05778646 epoch total loss 3.71021652\n",
      "Trained batch 87 batch loss 3.06385803 epoch total loss 3.70278692\n",
      "Trained batch 88 batch loss 3.35297894 epoch total loss 3.69881153\n",
      "Trained batch 89 batch loss 3.61304402 epoch total loss 3.69784784\n",
      "Trained batch 90 batch loss 3.58960629 epoch total loss 3.69664502\n",
      "Trained batch 91 batch loss 3.60288477 epoch total loss 3.69561458\n",
      "Trained batch 92 batch loss 3.68764591 epoch total loss 3.69552803\n",
      "Trained batch 93 batch loss 3.66278267 epoch total loss 3.69517589\n",
      "Trained batch 94 batch loss 3.69116163 epoch total loss 3.69513321\n",
      "Trained batch 95 batch loss 3.26298451 epoch total loss 3.69058418\n",
      "Trained batch 96 batch loss 2.99327183 epoch total loss 3.68332076\n",
      "Trained batch 97 batch loss 3.12035036 epoch total loss 3.67751694\n",
      "Trained batch 98 batch loss 2.86375713 epoch total loss 3.66921353\n",
      "Trained batch 99 batch loss 2.89319801 epoch total loss 3.66137481\n",
      "Trained batch 100 batch loss 3.20427418 epoch total loss 3.65680385\n",
      "Trained batch 101 batch loss 3.59947777 epoch total loss 3.65623641\n",
      "Trained batch 102 batch loss 3.65878105 epoch total loss 3.65626144\n",
      "Trained batch 103 batch loss 3.63674545 epoch total loss 3.6560719\n",
      "Trained batch 104 batch loss 3.6756928 epoch total loss 3.65626049\n",
      "Trained batch 105 batch loss 3.63236594 epoch total loss 3.6560328\n",
      "Trained batch 106 batch loss 3.68080235 epoch total loss 3.65626669\n",
      "Trained batch 107 batch loss 3.70819664 epoch total loss 3.65675187\n",
      "Trained batch 108 batch loss 3.66205525 epoch total loss 3.65680099\n",
      "Trained batch 109 batch loss 3.59068871 epoch total loss 3.65619445\n",
      "Trained batch 110 batch loss 3.63184595 epoch total loss 3.6559732\n",
      "Trained batch 111 batch loss 3.66076756 epoch total loss 3.65601635\n",
      "Trained batch 112 batch loss 3.68251252 epoch total loss 3.6562531\n",
      "Trained batch 113 batch loss 3.58532381 epoch total loss 3.65562534\n",
      "Trained batch 114 batch loss 3.64365816 epoch total loss 3.6555202\n",
      "Trained batch 115 batch loss 3.54784298 epoch total loss 3.65458393\n",
      "Trained batch 116 batch loss 3.55946612 epoch total loss 3.65376425\n",
      "Trained batch 117 batch loss 3.50713205 epoch total loss 3.65251088\n",
      "Trained batch 118 batch loss 3.43987322 epoch total loss 3.65070891\n",
      "Trained batch 119 batch loss 3.47529626 epoch total loss 3.64923501\n",
      "Trained batch 120 batch loss 3.39902425 epoch total loss 3.6471498\n",
      "Trained batch 121 batch loss 3.48845291 epoch total loss 3.6458385\n",
      "Trained batch 122 batch loss 3.34252262 epoch total loss 3.64335227\n",
      "Trained batch 123 batch loss 3.56600285 epoch total loss 3.64272356\n",
      "Trained batch 124 batch loss 3.64316511 epoch total loss 3.64272714\n",
      "Trained batch 125 batch loss 3.53778577 epoch total loss 3.64188743\n",
      "Trained batch 126 batch loss 3.58618426 epoch total loss 3.6414454\n",
      "Trained batch 127 batch loss 3.45133734 epoch total loss 3.63994837\n",
      "Trained batch 128 batch loss 3.56092024 epoch total loss 3.63933086\n",
      "Trained batch 129 batch loss 3.48527455 epoch total loss 3.63813663\n",
      "Trained batch 130 batch loss 3.23916769 epoch total loss 3.63506746\n",
      "Trained batch 131 batch loss 3.64357352 epoch total loss 3.63513255\n",
      "Trained batch 132 batch loss 3.279778 epoch total loss 3.63244057\n",
      "Trained batch 133 batch loss 3.18877172 epoch total loss 3.62910461\n",
      "Trained batch 134 batch loss 3.48081 epoch total loss 3.62799811\n",
      "Trained batch 135 batch loss 3.18350363 epoch total loss 3.62470555\n",
      "Trained batch 136 batch loss 3.44197607 epoch total loss 3.62336183\n",
      "Trained batch 137 batch loss 3.52607727 epoch total loss 3.62265182\n",
      "Trained batch 138 batch loss 3.28471351 epoch total loss 3.62020278\n",
      "Trained batch 139 batch loss 3.11080074 epoch total loss 3.61653805\n",
      "Trained batch 140 batch loss 2.87947369 epoch total loss 3.61127353\n",
      "Trained batch 141 batch loss 3.27097726 epoch total loss 3.60885978\n",
      "Trained batch 142 batch loss 3.16889572 epoch total loss 3.60576153\n",
      "Trained batch 143 batch loss 3.30338621 epoch total loss 3.60364699\n",
      "Trained batch 144 batch loss 3.64229798 epoch total loss 3.60391521\n",
      "Trained batch 145 batch loss 3.36960983 epoch total loss 3.60229945\n",
      "Trained batch 146 batch loss 3.51887608 epoch total loss 3.60172796\n",
      "Trained batch 147 batch loss 3.62192225 epoch total loss 3.60186553\n",
      "Trained batch 148 batch loss 3.44306397 epoch total loss 3.60079265\n",
      "Trained batch 149 batch loss 3.46332216 epoch total loss 3.59987\n",
      "Trained batch 150 batch loss 3.34597349 epoch total loss 3.59817719\n",
      "Trained batch 151 batch loss 3.32760477 epoch total loss 3.596385\n",
      "Trained batch 152 batch loss 3.40555096 epoch total loss 3.59512973\n",
      "Trained batch 153 batch loss 3.28735065 epoch total loss 3.59311819\n",
      "Trained batch 154 batch loss 3.42865658 epoch total loss 3.59205\n",
      "Trained batch 155 batch loss 3.40187216 epoch total loss 3.59082317\n",
      "Trained batch 156 batch loss 3.11466312 epoch total loss 3.58777094\n",
      "Trained batch 157 batch loss 3.13208866 epoch total loss 3.58486843\n",
      "Trained batch 158 batch loss 3.19761443 epoch total loss 3.58241749\n",
      "Trained batch 159 batch loss 3.32600737 epoch total loss 3.58080482\n",
      "Trained batch 160 batch loss 3.37931514 epoch total loss 3.5795455\n",
      "Trained batch 161 batch loss 3.41878891 epoch total loss 3.578547\n",
      "Trained batch 162 batch loss 3.36291 epoch total loss 3.57721591\n",
      "Trained batch 163 batch loss 3.46752095 epoch total loss 3.57654285\n",
      "Trained batch 164 batch loss 3.60166788 epoch total loss 3.57669616\n",
      "Trained batch 165 batch loss 3.60478115 epoch total loss 3.57686663\n",
      "Trained batch 166 batch loss 3.59771609 epoch total loss 3.57699227\n",
      "Trained batch 167 batch loss 3.65909553 epoch total loss 3.57748389\n",
      "Trained batch 168 batch loss 3.77090478 epoch total loss 3.57863545\n",
      "Trained batch 169 batch loss 3.5732162 epoch total loss 3.57860351\n",
      "Trained batch 170 batch loss 3.42562771 epoch total loss 3.57770348\n",
      "Trained batch 171 batch loss 3.52717566 epoch total loss 3.57740784\n",
      "Trained batch 172 batch loss 3.49035025 epoch total loss 3.57690191\n",
      "Trained batch 173 batch loss 3.12618303 epoch total loss 3.57429647\n",
      "Trained batch 174 batch loss 3.23800373 epoch total loss 3.57236338\n",
      "Trained batch 175 batch loss 3.36592531 epoch total loss 3.57118368\n",
      "Trained batch 176 batch loss 3.37659 epoch total loss 3.57007813\n",
      "Trained batch 177 batch loss 3.32705855 epoch total loss 3.56870532\n",
      "Trained batch 178 batch loss 3.39071298 epoch total loss 3.56770515\n",
      "Trained batch 179 batch loss 3.32064581 epoch total loss 3.56632471\n",
      "Trained batch 180 batch loss 3.52760291 epoch total loss 3.56610966\n",
      "Trained batch 181 batch loss 3.37644029 epoch total loss 3.56506181\n",
      "Trained batch 182 batch loss 3.49706697 epoch total loss 3.56468821\n",
      "Trained batch 183 batch loss 3.21785069 epoch total loss 3.56279278\n",
      "Trained batch 184 batch loss 3.48566651 epoch total loss 3.56237364\n",
      "Trained batch 185 batch loss 3.40232301 epoch total loss 3.56150866\n",
      "Trained batch 186 batch loss 3.25996256 epoch total loss 3.55988741\n",
      "Trained batch 187 batch loss 3.39286041 epoch total loss 3.55899429\n",
      "Trained batch 188 batch loss 3.04302931 epoch total loss 3.55624986\n",
      "Trained batch 189 batch loss 3.3128202 epoch total loss 3.55496168\n",
      "Trained batch 190 batch loss 3.43634844 epoch total loss 3.55433726\n",
      "Trained batch 191 batch loss 3.06747913 epoch total loss 3.55178857\n",
      "Trained batch 192 batch loss 3.15689111 epoch total loss 3.54973197\n",
      "Trained batch 193 batch loss 3.35517478 epoch total loss 3.5487237\n",
      "Trained batch 194 batch loss 3.44206619 epoch total loss 3.54817414\n",
      "Trained batch 195 batch loss 3.30524 epoch total loss 3.54692817\n",
      "Trained batch 196 batch loss 3.54498935 epoch total loss 3.54691839\n",
      "Trained batch 197 batch loss 3.45573616 epoch total loss 3.54645538\n",
      "Trained batch 198 batch loss 3.37672138 epoch total loss 3.54559827\n",
      "Trained batch 199 batch loss 3.41504097 epoch total loss 3.54494214\n",
      "Trained batch 200 batch loss 3.55009341 epoch total loss 3.54496789\n",
      "Trained batch 201 batch loss 3.37822676 epoch total loss 3.54413843\n",
      "Trained batch 202 batch loss 3.65820837 epoch total loss 3.54470301\n",
      "Trained batch 203 batch loss 3.15360284 epoch total loss 3.54277658\n",
      "Trained batch 204 batch loss 3.05697012 epoch total loss 3.54039502\n",
      "Trained batch 205 batch loss 3.41553879 epoch total loss 3.5397861\n",
      "Trained batch 206 batch loss 3.3345356 epoch total loss 3.53878951\n",
      "Trained batch 207 batch loss 3.30998707 epoch total loss 3.53768444\n",
      "Trained batch 208 batch loss 3.45683336 epoch total loss 3.53729582\n",
      "Trained batch 209 batch loss 3.44416857 epoch total loss 3.53685\n",
      "Trained batch 210 batch loss 3.29359031 epoch total loss 3.5356915\n",
      "Trained batch 211 batch loss 3.29547977 epoch total loss 3.53455305\n",
      "Trained batch 212 batch loss 3.25833964 epoch total loss 3.53325033\n",
      "Trained batch 213 batch loss 3.22128463 epoch total loss 3.53178596\n",
      "Trained batch 214 batch loss 3.07751846 epoch total loss 3.52966309\n",
      "Trained batch 215 batch loss 3.2641542 epoch total loss 3.52842808\n",
      "Trained batch 216 batch loss 3.47089219 epoch total loss 3.52816176\n",
      "Trained batch 217 batch loss 3.33707547 epoch total loss 3.52728128\n",
      "Trained batch 218 batch loss 3.22217226 epoch total loss 3.52588177\n",
      "Trained batch 219 batch loss 3.38941145 epoch total loss 3.52525854\n",
      "Trained batch 220 batch loss 3.23525357 epoch total loss 3.52394032\n",
      "Trained batch 221 batch loss 3.39186502 epoch total loss 3.52334261\n",
      "Trained batch 222 batch loss 3.2248807 epoch total loss 3.52199793\n",
      "Trained batch 223 batch loss 3.23687577 epoch total loss 3.52071929\n",
      "Trained batch 224 batch loss 3.50240517 epoch total loss 3.52063751\n",
      "Trained batch 225 batch loss 3.2819519 epoch total loss 3.51957679\n",
      "Trained batch 226 batch loss 3.29921794 epoch total loss 3.51860166\n",
      "Trained batch 227 batch loss 3.21656895 epoch total loss 3.51727104\n",
      "Trained batch 228 batch loss 3.50108576 epoch total loss 3.51720023\n",
      "Trained batch 229 batch loss 3.45263124 epoch total loss 3.51691818\n",
      "Trained batch 230 batch loss 3.40155315 epoch total loss 3.51641655\n",
      "Trained batch 231 batch loss 3.21986175 epoch total loss 3.51513267\n",
      "Trained batch 232 batch loss 3.21135 epoch total loss 3.51382351\n",
      "Trained batch 233 batch loss 3.34097219 epoch total loss 3.51308155\n",
      "Trained batch 234 batch loss 3.26837111 epoch total loss 3.51203561\n",
      "Trained batch 235 batch loss 3.42316151 epoch total loss 3.51165748\n",
      "Trained batch 236 batch loss 3.2243185 epoch total loss 3.51043987\n",
      "Trained batch 237 batch loss 3.42711592 epoch total loss 3.51008821\n",
      "Trained batch 238 batch loss 3.42515588 epoch total loss 3.50973153\n",
      "Trained batch 239 batch loss 3.43963718 epoch total loss 3.50943828\n",
      "Trained batch 240 batch loss 3.17412949 epoch total loss 3.50804114\n",
      "Trained batch 241 batch loss 3.12422276 epoch total loss 3.50644851\n",
      "Trained batch 242 batch loss 3.14429092 epoch total loss 3.50495195\n",
      "Trained batch 243 batch loss 3.35795212 epoch total loss 3.50434709\n",
      "Trained batch 244 batch loss 3.4596324 epoch total loss 3.50416398\n",
      "Trained batch 245 batch loss 3.33879137 epoch total loss 3.50348902\n",
      "Trained batch 246 batch loss 3.51995707 epoch total loss 3.50355601\n",
      "Trained batch 247 batch loss 3.42945075 epoch total loss 3.50325584\n",
      "Trained batch 248 batch loss 3.41695929 epoch total loss 3.50290775\n",
      "Trained batch 249 batch loss 3.57340765 epoch total loss 3.50319099\n",
      "Trained batch 250 batch loss 3.39851093 epoch total loss 3.50277233\n",
      "Trained batch 251 batch loss 3.53897071 epoch total loss 3.50291634\n",
      "Trained batch 252 batch loss 3.36580896 epoch total loss 3.50237203\n",
      "Trained batch 253 batch loss 3.38765478 epoch total loss 3.50191855\n",
      "Trained batch 254 batch loss 3.21339417 epoch total loss 3.50078273\n",
      "Trained batch 255 batch loss 3.35043645 epoch total loss 3.50019312\n",
      "Trained batch 256 batch loss 3.24718881 epoch total loss 3.49920487\n",
      "Trained batch 257 batch loss 3.38321137 epoch total loss 3.49875355\n",
      "Trained batch 258 batch loss 3.50089765 epoch total loss 3.49876213\n",
      "Trained batch 259 batch loss 3.44600844 epoch total loss 3.49855828\n",
      "Trained batch 260 batch loss 3.44702625 epoch total loss 3.49836\n",
      "Trained batch 261 batch loss 3.46090961 epoch total loss 3.49821663\n",
      "Trained batch 262 batch loss 3.45709491 epoch total loss 3.49805975\n",
      "Trained batch 263 batch loss 3.36826658 epoch total loss 3.49756622\n",
      "Trained batch 264 batch loss 3.24599218 epoch total loss 3.49661326\n",
      "Trained batch 265 batch loss 3.31718063 epoch total loss 3.49593616\n",
      "Trained batch 266 batch loss 3.23802042 epoch total loss 3.49496675\n",
      "Trained batch 267 batch loss 3.27615976 epoch total loss 3.4941473\n",
      "Trained batch 268 batch loss 3.12813044 epoch total loss 3.4927814\n",
      "Trained batch 269 batch loss 3.31693125 epoch total loss 3.4921279\n",
      "Trained batch 270 batch loss 3.28731132 epoch total loss 3.49136925\n",
      "Trained batch 271 batch loss 2.99179077 epoch total loss 3.48952579\n",
      "Trained batch 272 batch loss 2.91754103 epoch total loss 3.48742294\n",
      "Trained batch 273 batch loss 2.99012566 epoch total loss 3.48560119\n",
      "Trained batch 274 batch loss 3.07541251 epoch total loss 3.48410439\n",
      "Trained batch 275 batch loss 3.21913934 epoch total loss 3.48314071\n",
      "Trained batch 276 batch loss 3.22229433 epoch total loss 3.48219562\n",
      "Trained batch 277 batch loss 3.26759171 epoch total loss 3.48142076\n",
      "Trained batch 278 batch loss 3.22118139 epoch total loss 3.48048472\n",
      "Trained batch 279 batch loss 3.39970779 epoch total loss 3.48019528\n",
      "Trained batch 280 batch loss 3.18968153 epoch total loss 3.47915769\n",
      "Trained batch 281 batch loss 3.23155069 epoch total loss 3.47827673\n",
      "Trained batch 282 batch loss 3.28582048 epoch total loss 3.47759414\n",
      "Trained batch 283 batch loss 3.37961 epoch total loss 3.47724819\n",
      "Trained batch 284 batch loss 3.21407032 epoch total loss 3.47632146\n",
      "Trained batch 285 batch loss 3.36873293 epoch total loss 3.4759438\n",
      "Trained batch 286 batch loss 3.33675718 epoch total loss 3.47545695\n",
      "Trained batch 287 batch loss 3.48874235 epoch total loss 3.47550344\n",
      "Trained batch 288 batch loss 3.30262756 epoch total loss 3.47490311\n",
      "Trained batch 289 batch loss 3.25203204 epoch total loss 3.47413182\n",
      "Trained batch 290 batch loss 3.2898829 epoch total loss 3.47349644\n",
      "Trained batch 291 batch loss 3.12879133 epoch total loss 3.47231174\n",
      "Trained batch 292 batch loss 2.98794365 epoch total loss 3.47065306\n",
      "Trained batch 293 batch loss 3.00275183 epoch total loss 3.46905589\n",
      "Trained batch 294 batch loss 3.24323773 epoch total loss 3.46828794\n",
      "Trained batch 295 batch loss 3.21354294 epoch total loss 3.46742439\n",
      "Trained batch 296 batch loss 3.15693545 epoch total loss 3.46637535\n",
      "Trained batch 297 batch loss 3.39127469 epoch total loss 3.46612239\n",
      "Trained batch 298 batch loss 3.40104795 epoch total loss 3.46590376\n",
      "Trained batch 299 batch loss 3.34360027 epoch total loss 3.46549487\n",
      "Trained batch 300 batch loss 3.44847703 epoch total loss 3.46543813\n",
      "Trained batch 301 batch loss 3.47246599 epoch total loss 3.46546149\n",
      "Trained batch 302 batch loss 3.44773364 epoch total loss 3.46540284\n",
      "Trained batch 303 batch loss 3.24822259 epoch total loss 3.46468592\n",
      "Trained batch 304 batch loss 3.09262848 epoch total loss 3.46346211\n",
      "Trained batch 305 batch loss 2.67971826 epoch total loss 3.4608922\n",
      "Trained batch 306 batch loss 2.58518076 epoch total loss 3.45803046\n",
      "Trained batch 307 batch loss 2.71465945 epoch total loss 3.45560884\n",
      "Trained batch 308 batch loss 2.42784858 epoch total loss 3.45227218\n",
      "Trained batch 309 batch loss 2.90715265 epoch total loss 3.45050788\n",
      "Trained batch 310 batch loss 3.12258387 epoch total loss 3.44944978\n",
      "Trained batch 311 batch loss 3.3671 epoch total loss 3.44918489\n",
      "Trained batch 312 batch loss 3.47484279 epoch total loss 3.44926715\n",
      "Trained batch 313 batch loss 3.52826071 epoch total loss 3.44951987\n",
      "Trained batch 314 batch loss 3.72592974 epoch total loss 3.4504\n",
      "Trained batch 315 batch loss 3.75996256 epoch total loss 3.45138311\n",
      "Trained batch 316 batch loss 3.3488977 epoch total loss 3.45105863\n",
      "Trained batch 317 batch loss 2.91289711 epoch total loss 3.44936085\n",
      "Trained batch 318 batch loss 2.94667077 epoch total loss 3.44778\n",
      "Trained batch 319 batch loss 2.92068291 epoch total loss 3.44612765\n",
      "Trained batch 320 batch loss 3.27886534 epoch total loss 3.4456048\n",
      "Trained batch 321 batch loss 3.29558063 epoch total loss 3.44513726\n",
      "Trained batch 322 batch loss 3.37664795 epoch total loss 3.44492435\n",
      "Trained batch 323 batch loss 3.47460151 epoch total loss 3.44501615\n",
      "Trained batch 324 batch loss 3.54646778 epoch total loss 3.44532943\n",
      "Trained batch 325 batch loss 3.4267118 epoch total loss 3.44527221\n",
      "Trained batch 326 batch loss 3.4070282 epoch total loss 3.44515491\n",
      "Trained batch 327 batch loss 3.39612746 epoch total loss 3.44500494\n",
      "Trained batch 328 batch loss 3.44793224 epoch total loss 3.44501376\n",
      "Trained batch 329 batch loss 3.39581871 epoch total loss 3.44486427\n",
      "Trained batch 330 batch loss 3.38154817 epoch total loss 3.44467258\n",
      "Trained batch 331 batch loss 3.48971605 epoch total loss 3.44480872\n",
      "Trained batch 332 batch loss 3.39729166 epoch total loss 3.44466567\n",
      "Trained batch 333 batch loss 3.26039791 epoch total loss 3.4441123\n",
      "Trained batch 334 batch loss 3.40382791 epoch total loss 3.44399166\n",
      "Trained batch 335 batch loss 3.35604095 epoch total loss 3.44372916\n",
      "Trained batch 336 batch loss 3.24413061 epoch total loss 3.44313526\n",
      "Trained batch 337 batch loss 3.01101112 epoch total loss 3.44185281\n",
      "Trained batch 338 batch loss 3.27513838 epoch total loss 3.44135976\n",
      "Trained batch 339 batch loss 3.31552458 epoch total loss 3.44098854\n",
      "Trained batch 340 batch loss 3.21193218 epoch total loss 3.44031477\n",
      "Trained batch 341 batch loss 3.39157462 epoch total loss 3.44017196\n",
      "Trained batch 342 batch loss 3.44050264 epoch total loss 3.44017315\n",
      "Trained batch 343 batch loss 3.3842485 epoch total loss 3.44001\n",
      "Trained batch 344 batch loss 3.23179436 epoch total loss 3.43940496\n",
      "Trained batch 345 batch loss 3.21255255 epoch total loss 3.43874717\n",
      "Trained batch 346 batch loss 2.43237972 epoch total loss 3.4358387\n",
      "Trained batch 347 batch loss 3.1527648 epoch total loss 3.43502259\n",
      "Trained batch 348 batch loss 3.15087032 epoch total loss 3.43420625\n",
      "Trained batch 349 batch loss 3.25799036 epoch total loss 3.43370104\n",
      "Trained batch 350 batch loss 3.33004 epoch total loss 3.43340516\n",
      "Trained batch 351 batch loss 3.20691299 epoch total loss 3.43275976\n",
      "Trained batch 352 batch loss 3.10310555 epoch total loss 3.43182349\n",
      "Trained batch 353 batch loss 3.04058075 epoch total loss 3.43071485\n",
      "Trained batch 354 batch loss 3.06615067 epoch total loss 3.42968512\n",
      "Trained batch 355 batch loss 3.30125904 epoch total loss 3.42932343\n",
      "Trained batch 356 batch loss 3.34306717 epoch total loss 3.42908096\n",
      "Trained batch 357 batch loss 3.30351758 epoch total loss 3.42872906\n",
      "Trained batch 358 batch loss 3.24356508 epoch total loss 3.42821169\n",
      "Trained batch 359 batch loss 3.08410025 epoch total loss 3.42725325\n",
      "Trained batch 360 batch loss 3.26241565 epoch total loss 3.42679548\n",
      "Trained batch 361 batch loss 3.34286427 epoch total loss 3.42656302\n",
      "Trained batch 362 batch loss 3.06844616 epoch total loss 3.42557383\n",
      "Trained batch 363 batch loss 3.17485857 epoch total loss 3.42488313\n",
      "Trained batch 364 batch loss 3.34518313 epoch total loss 3.42466426\n",
      "Trained batch 365 batch loss 3.15855455 epoch total loss 3.42393517\n",
      "Trained batch 366 batch loss 3.21415973 epoch total loss 3.42336178\n",
      "Trained batch 367 batch loss 3.44992185 epoch total loss 3.42343426\n",
      "Trained batch 368 batch loss 3.37418795 epoch total loss 3.42330027\n",
      "Trained batch 369 batch loss 3.51978779 epoch total loss 3.42356181\n",
      "Trained batch 370 batch loss 3.422616 epoch total loss 3.42355919\n",
      "Trained batch 371 batch loss 3.43274236 epoch total loss 3.42358398\n",
      "Trained batch 372 batch loss 3.37068415 epoch total loss 3.42344189\n",
      "Trained batch 373 batch loss 3.28003454 epoch total loss 3.42305732\n",
      "Trained batch 374 batch loss 3.3799119 epoch total loss 3.42294192\n",
      "Trained batch 375 batch loss 3.43340015 epoch total loss 3.42296982\n",
      "Trained batch 376 batch loss 3.44149303 epoch total loss 3.42301917\n",
      "Trained batch 377 batch loss 3.43558168 epoch total loss 3.42305231\n",
      "Trained batch 378 batch loss 3.43562651 epoch total loss 3.42308569\n",
      "Trained batch 379 batch loss 3.41613436 epoch total loss 3.42306733\n",
      "Trained batch 380 batch loss 3.07999611 epoch total loss 3.42216444\n",
      "Trained batch 381 batch loss 3.35848904 epoch total loss 3.42199731\n",
      "Trained batch 382 batch loss 3.3257103 epoch total loss 3.4217453\n",
      "Trained batch 383 batch loss 3.44667149 epoch total loss 3.42181039\n",
      "Trained batch 384 batch loss 3.4804244 epoch total loss 3.42196298\n",
      "Trained batch 385 batch loss 3.46532536 epoch total loss 3.42207575\n",
      "Trained batch 386 batch loss 3.33897519 epoch total loss 3.42186046\n",
      "Trained batch 387 batch loss 3.40259123 epoch total loss 3.42181063\n",
      "Trained batch 388 batch loss 3.37425947 epoch total loss 3.42168808\n",
      "Trained batch 389 batch loss 3.17774558 epoch total loss 3.42106104\n",
      "Trained batch 390 batch loss 3.34980965 epoch total loss 3.42087841\n",
      "Trained batch 391 batch loss 3.16251802 epoch total loss 3.42021751\n",
      "Trained batch 392 batch loss 3.10687923 epoch total loss 3.41941833\n",
      "Trained batch 393 batch loss 3.1195178 epoch total loss 3.41865516\n",
      "Trained batch 394 batch loss 3.1878531 epoch total loss 3.41806936\n",
      "Trained batch 395 batch loss 3.09636688 epoch total loss 3.41725492\n",
      "Trained batch 396 batch loss 3.05958295 epoch total loss 3.41635156\n",
      "Trained batch 397 batch loss 3.38282871 epoch total loss 3.41626716\n",
      "Trained batch 398 batch loss 3.17721462 epoch total loss 3.41566658\n",
      "Trained batch 399 batch loss 3.1415174 epoch total loss 3.41497946\n",
      "Trained batch 400 batch loss 3.10666132 epoch total loss 3.41420865\n",
      "Trained batch 401 batch loss 3.07543063 epoch total loss 3.41336393\n",
      "Trained batch 402 batch loss 2.9382534 epoch total loss 3.41218185\n",
      "Trained batch 403 batch loss 3.03115082 epoch total loss 3.41123652\n",
      "Trained batch 404 batch loss 3.23529601 epoch total loss 3.41080093\n",
      "Trained batch 405 batch loss 3.04770517 epoch total loss 3.40990448\n",
      "Trained batch 406 batch loss 3.32124949 epoch total loss 3.40968633\n",
      "Trained batch 407 batch loss 3.19885206 epoch total loss 3.40916824\n",
      "Trained batch 408 batch loss 3.04306841 epoch total loss 3.40827107\n",
      "Trained batch 409 batch loss 2.93004012 epoch total loss 3.40710187\n",
      "Trained batch 410 batch loss 3.06707144 epoch total loss 3.40627241\n",
      "Trained batch 411 batch loss 3.45858 epoch total loss 3.40639973\n",
      "Trained batch 412 batch loss 3.42282915 epoch total loss 3.40643954\n",
      "Trained batch 413 batch loss 3.22145653 epoch total loss 3.40599155\n",
      "Trained batch 414 batch loss 3.05223346 epoch total loss 3.4051373\n",
      "Trained batch 415 batch loss 2.81914 epoch total loss 3.40372515\n",
      "Trained batch 416 batch loss 3.01920176 epoch total loss 3.40280056\n",
      "Trained batch 417 batch loss 3.31087065 epoch total loss 3.40258026\n",
      "Trained batch 418 batch loss 3.27280951 epoch total loss 3.40226984\n",
      "Trained batch 419 batch loss 3.37947893 epoch total loss 3.40221548\n",
      "Trained batch 420 batch loss 3.39638495 epoch total loss 3.40220165\n",
      "Trained batch 421 batch loss 3.311028 epoch total loss 3.40198517\n",
      "Trained batch 422 batch loss 3.30120945 epoch total loss 3.40174651\n",
      "Trained batch 423 batch loss 3.32049537 epoch total loss 3.40155411\n",
      "Trained batch 424 batch loss 3.3196013 epoch total loss 3.40136075\n",
      "Trained batch 425 batch loss 3.27623439 epoch total loss 3.40106654\n",
      "Trained batch 426 batch loss 2.96377897 epoch total loss 3.40004\n",
      "Trained batch 427 batch loss 3.0629952 epoch total loss 3.39925051\n",
      "Trained batch 428 batch loss 3.13583422 epoch total loss 3.39863515\n",
      "Trained batch 429 batch loss 3.1418035 epoch total loss 3.39803648\n",
      "Trained batch 430 batch loss 3.16716909 epoch total loss 3.39749956\n",
      "Trained batch 431 batch loss 3.2127285 epoch total loss 3.39707088\n",
      "Trained batch 432 batch loss 3.21035075 epoch total loss 3.39663863\n",
      "Trained batch 433 batch loss 2.77156448 epoch total loss 3.39519525\n",
      "Trained batch 434 batch loss 2.73860097 epoch total loss 3.39368248\n",
      "Trained batch 435 batch loss 2.84802842 epoch total loss 3.39242792\n",
      "Trained batch 436 batch loss 3.42137718 epoch total loss 3.39249444\n",
      "Trained batch 437 batch loss 3.21554136 epoch total loss 3.39208961\n",
      "Trained batch 438 batch loss 3.16782 epoch total loss 3.39157748\n",
      "Trained batch 439 batch loss 3.10351419 epoch total loss 3.39092135\n",
      "Trained batch 440 batch loss 3.12885094 epoch total loss 3.39032602\n",
      "Trained batch 441 batch loss 3.24326253 epoch total loss 3.38999248\n",
      "Trained batch 442 batch loss 3.30526805 epoch total loss 3.38980079\n",
      "Trained batch 443 batch loss 3.38558626 epoch total loss 3.38979149\n",
      "Trained batch 444 batch loss 3.31683159 epoch total loss 3.38962698\n",
      "Trained batch 445 batch loss 3.27042532 epoch total loss 3.389359\n",
      "Trained batch 446 batch loss 3.31666756 epoch total loss 3.38919592\n",
      "Trained batch 447 batch loss 3.22883439 epoch total loss 3.38883734\n",
      "Trained batch 448 batch loss 3.24084616 epoch total loss 3.38850713\n",
      "Trained batch 449 batch loss 3.25022173 epoch total loss 3.38819909\n",
      "Trained batch 450 batch loss 3.07538033 epoch total loss 3.3875041\n",
      "Trained batch 451 batch loss 3.20996 epoch total loss 3.38711047\n",
      "Trained batch 452 batch loss 3.32314682 epoch total loss 3.38696885\n",
      "Trained batch 453 batch loss 3.10208464 epoch total loss 3.38634\n",
      "Trained batch 454 batch loss 3.52485299 epoch total loss 3.38664508\n",
      "Trained batch 455 batch loss 3.4817431 epoch total loss 3.38685393\n",
      "Trained batch 456 batch loss 3.36630797 epoch total loss 3.38680887\n",
      "Trained batch 457 batch loss 3.27235436 epoch total loss 3.38655853\n",
      "Trained batch 458 batch loss 3.09704113 epoch total loss 3.38592625\n",
      "Trained batch 459 batch loss 3.0818491 epoch total loss 3.38526392\n",
      "Trained batch 460 batch loss 3.30582881 epoch total loss 3.3850913\n",
      "Trained batch 461 batch loss 3.19999886 epoch total loss 3.38468957\n",
      "Trained batch 462 batch loss 3.17062449 epoch total loss 3.38422632\n",
      "Trained batch 463 batch loss 3.26752806 epoch total loss 3.38397431\n",
      "Trained batch 464 batch loss 2.85513067 epoch total loss 3.38283467\n",
      "Trained batch 465 batch loss 2.92558026 epoch total loss 3.3818512\n",
      "Trained batch 466 batch loss 3.16477394 epoch total loss 3.38138533\n",
      "Trained batch 467 batch loss 3.2090528 epoch total loss 3.38101649\n",
      "Trained batch 468 batch loss 3.33716655 epoch total loss 3.38092279\n",
      "Trained batch 469 batch loss 3.0950079 epoch total loss 3.38031316\n",
      "Trained batch 470 batch loss 3.38916588 epoch total loss 3.38033199\n",
      "Trained batch 471 batch loss 3.31783772 epoch total loss 3.38019919\n",
      "Trained batch 472 batch loss 3.39572334 epoch total loss 3.3802321\n",
      "Trained batch 473 batch loss 3.20181656 epoch total loss 3.37985492\n",
      "Trained batch 474 batch loss 2.97227383 epoch total loss 3.37899518\n",
      "Trained batch 475 batch loss 2.93332219 epoch total loss 3.378057\n",
      "Trained batch 476 batch loss 3.06063843 epoch total loss 3.37739015\n",
      "Trained batch 477 batch loss 2.99025106 epoch total loss 3.37657857\n",
      "Trained batch 478 batch loss 3.16377544 epoch total loss 3.37613344\n",
      "Trained batch 479 batch loss 3.2066865 epoch total loss 3.37577963\n",
      "Trained batch 480 batch loss 3.42639041 epoch total loss 3.37588501\n",
      "Trained batch 481 batch loss 2.99263096 epoch total loss 3.37508821\n",
      "Trained batch 482 batch loss 2.79925799 epoch total loss 3.37389374\n",
      "Trained batch 483 batch loss 3.34382868 epoch total loss 3.37383151\n",
      "Trained batch 484 batch loss 3.07735395 epoch total loss 3.37321925\n",
      "Trained batch 485 batch loss 3.08956671 epoch total loss 3.37263441\n",
      "Trained batch 486 batch loss 2.55489206 epoch total loss 3.37095189\n",
      "Trained batch 487 batch loss 2.71849322 epoch total loss 3.36961222\n",
      "Trained batch 488 batch loss 2.83048892 epoch total loss 3.36850715\n",
      "Trained batch 489 batch loss 3.06392264 epoch total loss 3.3678844\n",
      "Trained batch 490 batch loss 2.99970222 epoch total loss 3.36713314\n",
      "Trained batch 491 batch loss 2.95051074 epoch total loss 3.36628485\n",
      "Trained batch 492 batch loss 3.06544137 epoch total loss 3.3656733\n",
      "Trained batch 493 batch loss 3.29918671 epoch total loss 3.36553836\n",
      "Trained batch 494 batch loss 3.30853081 epoch total loss 3.36542296\n",
      "Trained batch 495 batch loss 3.36413574 epoch total loss 3.36542034\n",
      "Trained batch 496 batch loss 3.40020847 epoch total loss 3.36549067\n",
      "Trained batch 497 batch loss 3.37957525 epoch total loss 3.36551881\n",
      "Trained batch 498 batch loss 3.38154221 epoch total loss 3.36555099\n",
      "Trained batch 499 batch loss 3.2879982 epoch total loss 3.36539555\n",
      "Trained batch 500 batch loss 3.20746565 epoch total loss 3.36507988\n",
      "Trained batch 501 batch loss 3.23292589 epoch total loss 3.36481595\n",
      "Trained batch 502 batch loss 3.26941085 epoch total loss 3.36462593\n",
      "Trained batch 503 batch loss 3.18800044 epoch total loss 3.36427474\n",
      "Trained batch 504 batch loss 3.29054594 epoch total loss 3.36412859\n",
      "Trained batch 505 batch loss 3.12334323 epoch total loss 3.36365151\n",
      "Trained batch 506 batch loss 3.0049448 epoch total loss 3.3629427\n",
      "Trained batch 507 batch loss 2.93421364 epoch total loss 3.36209702\n",
      "Trained batch 508 batch loss 2.7949729 epoch total loss 3.36098075\n",
      "Trained batch 509 batch loss 3.01306152 epoch total loss 3.3602972\n",
      "Trained batch 510 batch loss 3.32433248 epoch total loss 3.36022663\n",
      "Trained batch 511 batch loss 3.29656124 epoch total loss 3.36010194\n",
      "Trained batch 512 batch loss 3.25609255 epoch total loss 3.35989881\n",
      "Trained batch 513 batch loss 3.2573843 epoch total loss 3.35969877\n",
      "Trained batch 514 batch loss 3.25750208 epoch total loss 3.3595\n",
      "Trained batch 515 batch loss 3.2916162 epoch total loss 3.35936809\n",
      "Trained batch 516 batch loss 3.24525309 epoch total loss 3.35914683\n",
      "Trained batch 517 batch loss 3.14569044 epoch total loss 3.35873389\n",
      "Trained batch 518 batch loss 3.16658545 epoch total loss 3.35836315\n",
      "Trained batch 519 batch loss 3.19939637 epoch total loss 3.35805678\n",
      "Trained batch 520 batch loss 3.05347443 epoch total loss 3.35747099\n",
      "Trained batch 521 batch loss 2.9426744 epoch total loss 3.35667467\n",
      "Trained batch 522 batch loss 2.94425058 epoch total loss 3.35588455\n",
      "Trained batch 523 batch loss 2.91660357 epoch total loss 3.3550446\n",
      "Trained batch 524 batch loss 2.81652975 epoch total loss 3.35401702\n",
      "Trained batch 525 batch loss 3.06325102 epoch total loss 3.35346317\n",
      "Trained batch 526 batch loss 3.12721777 epoch total loss 3.35303283\n",
      "Trained batch 527 batch loss 3.31880522 epoch total loss 3.35296798\n",
      "Trained batch 528 batch loss 3.22246981 epoch total loss 3.35272074\n",
      "Trained batch 529 batch loss 3.27349949 epoch total loss 3.35257125\n",
      "Trained batch 530 batch loss 3.23943424 epoch total loss 3.35235763\n",
      "Trained batch 531 batch loss 3.17254233 epoch total loss 3.35201883\n",
      "Trained batch 532 batch loss 3.22644687 epoch total loss 3.3517828\n",
      "Trained batch 533 batch loss 3.25953221 epoch total loss 3.35160971\n",
      "Trained batch 534 batch loss 3.08751488 epoch total loss 3.35111523\n",
      "Trained batch 535 batch loss 3.45937681 epoch total loss 3.35131741\n",
      "Trained batch 536 batch loss 3.17606688 epoch total loss 3.3509903\n",
      "Trained batch 537 batch loss 3.09329939 epoch total loss 3.35051036\n",
      "Trained batch 538 batch loss 3.26991463 epoch total loss 3.35036063\n",
      "Trained batch 539 batch loss 3.35768962 epoch total loss 3.35037422\n",
      "Trained batch 540 batch loss 3.34966087 epoch total loss 3.35037279\n",
      "Trained batch 541 batch loss 3.32022333 epoch total loss 3.350317\n",
      "Trained batch 542 batch loss 3.28427362 epoch total loss 3.35019517\n",
      "Trained batch 543 batch loss 3.17757 epoch total loss 3.34987736\n",
      "Trained batch 544 batch loss 3.17386 epoch total loss 3.34955382\n",
      "Trained batch 545 batch loss 3.11787081 epoch total loss 3.34912872\n",
      "Trained batch 546 batch loss 3.23386335 epoch total loss 3.34891772\n",
      "Trained batch 547 batch loss 3.29947519 epoch total loss 3.34882712\n",
      "Trained batch 548 batch loss 3.4588232 epoch total loss 3.34902787\n",
      "Trained batch 549 batch loss 3.29897714 epoch total loss 3.3489368\n",
      "Trained batch 550 batch loss 3.29787803 epoch total loss 3.34884381\n",
      "Trained batch 551 batch loss 3.17959547 epoch total loss 3.34853673\n",
      "Trained batch 552 batch loss 3.17936301 epoch total loss 3.34823012\n",
      "Trained batch 553 batch loss 3.23244572 epoch total loss 3.34802079\n",
      "Trained batch 554 batch loss 3.09200144 epoch total loss 3.34755874\n",
      "Trained batch 555 batch loss 3.14305806 epoch total loss 3.34719014\n",
      "Trained batch 556 batch loss 3.1134057 epoch total loss 3.34676981\n",
      "Trained batch 557 batch loss 3.20491219 epoch total loss 3.34651518\n",
      "Trained batch 558 batch loss 3.14759588 epoch total loss 3.3461585\n",
      "Trained batch 559 batch loss 3.12161446 epoch total loss 3.34575677\n",
      "Trained batch 560 batch loss 3.20216155 epoch total loss 3.34550047\n",
      "Trained batch 561 batch loss 3.28495312 epoch total loss 3.34539247\n",
      "Trained batch 562 batch loss 3.26469493 epoch total loss 3.3452487\n",
      "Trained batch 563 batch loss 3.21517706 epoch total loss 3.34501767\n",
      "Trained batch 564 batch loss 3.26642466 epoch total loss 3.34487844\n",
      "Trained batch 565 batch loss 3.21938396 epoch total loss 3.34465623\n",
      "Trained batch 566 batch loss 3.2371583 epoch total loss 3.34446645\n",
      "Trained batch 567 batch loss 3.18575954 epoch total loss 3.34418654\n",
      "Trained batch 568 batch loss 3.15200448 epoch total loss 3.34384823\n",
      "Trained batch 569 batch loss 2.99071622 epoch total loss 3.34322762\n",
      "Trained batch 570 batch loss 3.17422748 epoch total loss 3.34293103\n",
      "Trained batch 571 batch loss 3.1055665 epoch total loss 3.34251547\n",
      "Trained batch 572 batch loss 2.78097129 epoch total loss 3.34153366\n",
      "Trained batch 573 batch loss 3.1538868 epoch total loss 3.34120631\n",
      "Trained batch 574 batch loss 3.47770643 epoch total loss 3.34144402\n",
      "Trained batch 575 batch loss 3.38526726 epoch total loss 3.34152031\n",
      "Trained batch 576 batch loss 3.27524471 epoch total loss 3.34140515\n",
      "Trained batch 577 batch loss 3.72810602 epoch total loss 3.34207559\n",
      "Trained batch 578 batch loss 3.49734139 epoch total loss 3.34234405\n",
      "Trained batch 579 batch loss 3.4000752 epoch total loss 3.3424437\n",
      "Trained batch 580 batch loss 3.32227159 epoch total loss 3.3424089\n",
      "Trained batch 581 batch loss 3.25182819 epoch total loss 3.34225297\n",
      "Trained batch 582 batch loss 3.48788118 epoch total loss 3.34250331\n",
      "Trained batch 583 batch loss 3.0717597 epoch total loss 3.34203887\n",
      "Trained batch 584 batch loss 3.177495 epoch total loss 3.34175706\n",
      "Trained batch 585 batch loss 3.16812634 epoch total loss 3.34146023\n",
      "Trained batch 586 batch loss 3.51242352 epoch total loss 3.34175205\n",
      "Trained batch 587 batch loss 3.25559926 epoch total loss 3.34160542\n",
      "Trained batch 588 batch loss 3.15672827 epoch total loss 3.34129095\n",
      "Trained batch 589 batch loss 3.23461 epoch total loss 3.34110975\n",
      "Trained batch 590 batch loss 3.18997288 epoch total loss 3.34085369\n",
      "Trained batch 591 batch loss 3.22156382 epoch total loss 3.34065175\n",
      "Trained batch 592 batch loss 3.11959386 epoch total loss 3.34027839\n",
      "Trained batch 593 batch loss 3.16725397 epoch total loss 3.33998656\n",
      "Trained batch 594 batch loss 3.32612133 epoch total loss 3.33996344\n",
      "Trained batch 595 batch loss 3.24246597 epoch total loss 3.3397994\n",
      "Trained batch 596 batch loss 3.26045108 epoch total loss 3.33966637\n",
      "Trained batch 597 batch loss 3.54168248 epoch total loss 3.34000468\n",
      "Trained batch 598 batch loss 3.43270874 epoch total loss 3.34015965\n",
      "Trained batch 599 batch loss 3.35900688 epoch total loss 3.34019113\n",
      "Trained batch 600 batch loss 3.34247875 epoch total loss 3.34019518\n",
      "Trained batch 601 batch loss 3.15302849 epoch total loss 3.3398838\n",
      "Trained batch 602 batch loss 3.04721308 epoch total loss 3.33939767\n",
      "Trained batch 603 batch loss 3.16351461 epoch total loss 3.33910608\n",
      "Trained batch 604 batch loss 3.10976315 epoch total loss 3.33872628\n",
      "Trained batch 605 batch loss 3.14291286 epoch total loss 3.33840275\n",
      "Trained batch 606 batch loss 3.12241268 epoch total loss 3.33804631\n",
      "Trained batch 607 batch loss 3.12281394 epoch total loss 3.33769178\n",
      "Trained batch 608 batch loss 3.23578405 epoch total loss 3.33752418\n",
      "Trained batch 609 batch loss 3.16288161 epoch total loss 3.33723736\n",
      "Trained batch 610 batch loss 3.21062732 epoch total loss 3.3370297\n",
      "Trained batch 611 batch loss 3.243788 epoch total loss 3.33687711\n",
      "Trained batch 612 batch loss 3.38334751 epoch total loss 3.33695292\n",
      "Trained batch 613 batch loss 3.53922319 epoch total loss 3.3372829\n",
      "Trained batch 614 batch loss 3.53963804 epoch total loss 3.33761239\n",
      "Trained batch 615 batch loss 3.47078896 epoch total loss 3.33782864\n",
      "Trained batch 616 batch loss 3.41817689 epoch total loss 3.33795929\n",
      "Trained batch 617 batch loss 3.38684034 epoch total loss 3.33803821\n",
      "Trained batch 618 batch loss 3.40691519 epoch total loss 3.33814979\n",
      "Trained batch 619 batch loss 3.39258289 epoch total loss 3.33823776\n",
      "Trained batch 620 batch loss 2.9729166 epoch total loss 3.33764839\n",
      "Trained batch 621 batch loss 3.1924057 epoch total loss 3.3374145\n",
      "Trained batch 622 batch loss 3.17727137 epoch total loss 3.33715701\n",
      "Trained batch 623 batch loss 3.16779518 epoch total loss 3.33688498\n",
      "Trained batch 624 batch loss 3.12975264 epoch total loss 3.33655286\n",
      "Trained batch 625 batch loss 3.11806488 epoch total loss 3.33620358\n",
      "Trained batch 626 batch loss 3.21243858 epoch total loss 3.33600569\n",
      "Trained batch 627 batch loss 3.28657389 epoch total loss 3.33592701\n",
      "Trained batch 628 batch loss 3.31587386 epoch total loss 3.33589506\n",
      "Trained batch 629 batch loss 3.39572144 epoch total loss 3.33599019\n",
      "Trained batch 630 batch loss 3.10761595 epoch total loss 3.33562779\n",
      "Trained batch 631 batch loss 3.32990122 epoch total loss 3.33561873\n",
      "Trained batch 632 batch loss 3.12546 epoch total loss 3.33528614\n",
      "Trained batch 633 batch loss 3.18827677 epoch total loss 3.33505392\n",
      "Trained batch 634 batch loss 2.93296576 epoch total loss 3.33441949\n",
      "Trained batch 635 batch loss 3.22204852 epoch total loss 3.33424282\n",
      "Trained batch 636 batch loss 3.13597941 epoch total loss 3.33393097\n",
      "Trained batch 637 batch loss 3.09366131 epoch total loss 3.33355403\n",
      "Trained batch 638 batch loss 3.29428482 epoch total loss 3.33349228\n",
      "Trained batch 639 batch loss 3.36026812 epoch total loss 3.33353424\n",
      "Trained batch 640 batch loss 3.54997897 epoch total loss 3.33387256\n",
      "Trained batch 641 batch loss 3.60667109 epoch total loss 3.33429813\n",
      "Trained batch 642 batch loss 3.45568 epoch total loss 3.3344872\n",
      "Trained batch 643 batch loss 3.34336448 epoch total loss 3.33450079\n",
      "Trained batch 644 batch loss 3.49400258 epoch total loss 3.33474827\n",
      "Trained batch 645 batch loss 3.27992415 epoch total loss 3.33466339\n",
      "Trained batch 646 batch loss 2.9802227 epoch total loss 3.33411479\n",
      "Trained batch 647 batch loss 3.3077147 epoch total loss 3.33407378\n",
      "Trained batch 648 batch loss 3.51065516 epoch total loss 3.33434653\n",
      "Trained batch 649 batch loss 3.56645155 epoch total loss 3.33470392\n",
      "Trained batch 650 batch loss 3.58507442 epoch total loss 3.33508897\n",
      "Trained batch 651 batch loss 3.3556459 epoch total loss 3.33512068\n",
      "Trained batch 652 batch loss 3.27530241 epoch total loss 3.33502913\n",
      "Trained batch 653 batch loss 3.45023322 epoch total loss 3.33520555\n",
      "Trained batch 654 batch loss 2.88846946 epoch total loss 3.33452225\n",
      "Trained batch 655 batch loss 3.19159698 epoch total loss 3.33430409\n",
      "Trained batch 656 batch loss 3.42716146 epoch total loss 3.33444595\n",
      "Trained batch 657 batch loss 3.23211288 epoch total loss 3.33429027\n",
      "Trained batch 658 batch loss 3.07666874 epoch total loss 3.33389878\n",
      "Trained batch 659 batch loss 2.93470025 epoch total loss 3.33329296\n",
      "Trained batch 660 batch loss 3.15292525 epoch total loss 3.33301973\n",
      "Trained batch 661 batch loss 3.22278023 epoch total loss 3.33285308\n",
      "Trained batch 662 batch loss 3.31512642 epoch total loss 3.33282638\n",
      "Trained batch 663 batch loss 3.14706755 epoch total loss 3.332546\n",
      "Trained batch 664 batch loss 2.87738514 epoch total loss 3.33186054\n",
      "Trained batch 665 batch loss 3.12630892 epoch total loss 3.33155131\n",
      "Trained batch 666 batch loss 3.00916553 epoch total loss 3.33106756\n",
      "Trained batch 667 batch loss 3.08084416 epoch total loss 3.33069229\n",
      "Trained batch 668 batch loss 3.18099117 epoch total loss 3.33046818\n",
      "Trained batch 669 batch loss 3.19608688 epoch total loss 3.33026719\n",
      "Trained batch 670 batch loss 3.24093199 epoch total loss 3.33013391\n",
      "Trained batch 671 batch loss 3.05528879 epoch total loss 3.32972407\n",
      "Trained batch 672 batch loss 3.17487192 epoch total loss 3.32949352\n",
      "Trained batch 673 batch loss 3.238379 epoch total loss 3.3293581\n",
      "Trained batch 674 batch loss 3.37976837 epoch total loss 3.32943296\n",
      "Trained batch 675 batch loss 3.07058811 epoch total loss 3.32904959\n",
      "Trained batch 676 batch loss 3.21755171 epoch total loss 3.3288846\n",
      "Trained batch 677 batch loss 3.22855973 epoch total loss 3.32873631\n",
      "Trained batch 678 batch loss 3.15243578 epoch total loss 3.32847619\n",
      "Trained batch 679 batch loss 3.22582126 epoch total loss 3.32832503\n",
      "Trained batch 680 batch loss 3.30298948 epoch total loss 3.3282876\n",
      "Trained batch 681 batch loss 3.36237955 epoch total loss 3.32833767\n",
      "Trained batch 682 batch loss 3.10685205 epoch total loss 3.32801294\n",
      "Trained batch 683 batch loss 3.36110568 epoch total loss 3.32806134\n",
      "Trained batch 684 batch loss 3.1824367 epoch total loss 3.32784843\n",
      "Trained batch 685 batch loss 3.21743 epoch total loss 3.32768726\n",
      "Trained batch 686 batch loss 3.14624405 epoch total loss 3.32742286\n",
      "Trained batch 687 batch loss 3.11517143 epoch total loss 3.32711387\n",
      "Trained batch 688 batch loss 3.06868887 epoch total loss 3.32673812\n",
      "Trained batch 689 batch loss 3.21012163 epoch total loss 3.32656908\n",
      "Trained batch 690 batch loss 3.07604432 epoch total loss 3.32620573\n",
      "Trained batch 691 batch loss 2.92394137 epoch total loss 3.32562351\n",
      "Trained batch 692 batch loss 2.78028059 epoch total loss 3.32483554\n",
      "Trained batch 693 batch loss 2.98659968 epoch total loss 3.32434726\n",
      "Trained batch 694 batch loss 3.34447575 epoch total loss 3.32437634\n",
      "Trained batch 695 batch loss 3.04848051 epoch total loss 3.32397962\n",
      "Trained batch 696 batch loss 3.07062101 epoch total loss 3.32361531\n",
      "Trained batch 697 batch loss 3.01379704 epoch total loss 3.32317114\n",
      "Trained batch 698 batch loss 3.26265979 epoch total loss 3.32308435\n",
      "Trained batch 699 batch loss 2.88813829 epoch total loss 3.32246232\n",
      "Trained batch 700 batch loss 2.93881249 epoch total loss 3.32191396\n",
      "Trained batch 701 batch loss 3.32226086 epoch total loss 3.32191467\n",
      "Trained batch 702 batch loss 3.28854513 epoch total loss 3.32186699\n",
      "Trained batch 703 batch loss 3.32554293 epoch total loss 3.32187223\n",
      "Trained batch 704 batch loss 3.38707352 epoch total loss 3.3219645\n",
      "Trained batch 705 batch loss 3.37791872 epoch total loss 3.3220439\n",
      "Trained batch 706 batch loss 3.2564137 epoch total loss 3.32195091\n",
      "Trained batch 707 batch loss 3.46472168 epoch total loss 3.32215309\n",
      "Trained batch 708 batch loss 3.49119902 epoch total loss 3.32239175\n",
      "Trained batch 709 batch loss 3.25260448 epoch total loss 3.32229352\n",
      "Trained batch 710 batch loss 2.95841503 epoch total loss 3.32178116\n",
      "Trained batch 711 batch loss 2.67620015 epoch total loss 3.32087326\n",
      "Trained batch 712 batch loss 2.78352404 epoch total loss 3.32011843\n",
      "Trained batch 713 batch loss 3.13797641 epoch total loss 3.31986284\n",
      "Trained batch 714 batch loss 3.06387234 epoch total loss 3.3195045\n",
      "Trained batch 715 batch loss 3.1216414 epoch total loss 3.3192277\n",
      "Trained batch 716 batch loss 3.24533129 epoch total loss 3.31912446\n",
      "Trained batch 717 batch loss 2.92817926 epoch total loss 3.31857944\n",
      "Trained batch 718 batch loss 3.14986 epoch total loss 3.31834435\n",
      "Trained batch 719 batch loss 3.13531971 epoch total loss 3.31808972\n",
      "Trained batch 720 batch loss 3.12013841 epoch total loss 3.31781483\n",
      "Trained batch 721 batch loss 3.16983271 epoch total loss 3.31760979\n",
      "Trained batch 722 batch loss 3.16048455 epoch total loss 3.31739187\n",
      "Trained batch 723 batch loss 3.15339088 epoch total loss 3.3171649\n",
      "Trained batch 724 batch loss 3.19227529 epoch total loss 3.31699276\n",
      "Trained batch 725 batch loss 3.198277 epoch total loss 3.31682897\n",
      "Trained batch 726 batch loss 3.09499645 epoch total loss 3.31652331\n",
      "Trained batch 727 batch loss 3.05091286 epoch total loss 3.31615806\n",
      "Trained batch 728 batch loss 3.0343852 epoch total loss 3.3157711\n",
      "Trained batch 729 batch loss 3.04170632 epoch total loss 3.31539512\n",
      "Trained batch 730 batch loss 3.2570982 epoch total loss 3.31531525\n",
      "Trained batch 731 batch loss 3.21133423 epoch total loss 3.31517315\n",
      "Trained batch 732 batch loss 3.26588511 epoch total loss 3.31510592\n",
      "Trained batch 733 batch loss 3.19386148 epoch total loss 3.31494045\n",
      "Trained batch 734 batch loss 3.05943727 epoch total loss 3.31459212\n",
      "Trained batch 735 batch loss 3.12940693 epoch total loss 3.31434\n",
      "Trained batch 736 batch loss 2.99869752 epoch total loss 3.31391144\n",
      "Trained batch 737 batch loss 2.95226908 epoch total loss 3.31342053\n",
      "Trained batch 738 batch loss 3.10025573 epoch total loss 3.31313181\n",
      "Trained batch 739 batch loss 3.09123659 epoch total loss 3.31283164\n",
      "Trained batch 740 batch loss 3.27504539 epoch total loss 3.31278086\n",
      "Trained batch 741 batch loss 2.94323921 epoch total loss 3.31228232\n",
      "Trained batch 742 batch loss 2.88317394 epoch total loss 3.31170368\n",
      "Trained batch 743 batch loss 2.95921898 epoch total loss 3.31122947\n",
      "Trained batch 744 batch loss 2.8671968 epoch total loss 3.31063247\n",
      "Trained batch 745 batch loss 2.95274591 epoch total loss 3.31015205\n",
      "Trained batch 746 batch loss 2.66828465 epoch total loss 3.30929136\n",
      "Trained batch 747 batch loss 2.89670825 epoch total loss 3.30873919\n",
      "Trained batch 748 batch loss 2.76221132 epoch total loss 3.30800843\n",
      "Trained batch 749 batch loss 3.08930182 epoch total loss 3.30771661\n",
      "Trained batch 750 batch loss 3.76308918 epoch total loss 3.30832386\n",
      "Trained batch 751 batch loss 3.69939184 epoch total loss 3.3088448\n",
      "Trained batch 752 batch loss 3.44123554 epoch total loss 3.30902076\n",
      "Trained batch 753 batch loss 3.25106096 epoch total loss 3.30894351\n",
      "Trained batch 754 batch loss 3.23897481 epoch total loss 3.30885077\n",
      "Trained batch 755 batch loss 3.2376163 epoch total loss 3.30875635\n",
      "Trained batch 756 batch loss 3.10890627 epoch total loss 3.30849195\n",
      "Trained batch 757 batch loss 3.19425845 epoch total loss 3.30834126\n",
      "Trained batch 758 batch loss 3.28114462 epoch total loss 3.3083055\n",
      "Trained batch 759 batch loss 3.32564211 epoch total loss 3.30832839\n",
      "Trained batch 760 batch loss 3.21885777 epoch total loss 3.30821061\n",
      "Trained batch 761 batch loss 3.12464809 epoch total loss 3.30796933\n",
      "Trained batch 762 batch loss 2.97701955 epoch total loss 3.30753517\n",
      "Trained batch 763 batch loss 3.17517376 epoch total loss 3.30736184\n",
      "Trained batch 764 batch loss 3.13770247 epoch total loss 3.30713987\n",
      "Trained batch 765 batch loss 2.96834636 epoch total loss 3.30669689\n",
      "Trained batch 766 batch loss 3.19794345 epoch total loss 3.30655479\n",
      "Trained batch 767 batch loss 3.2387445 epoch total loss 3.30646658\n",
      "Trained batch 768 batch loss 3.13052082 epoch total loss 3.30623746\n",
      "Trained batch 769 batch loss 3.11589 epoch total loss 3.30599022\n",
      "Trained batch 770 batch loss 3.20813465 epoch total loss 3.30586314\n",
      "Trained batch 771 batch loss 3.22787666 epoch total loss 3.30576181\n",
      "Trained batch 772 batch loss 3.12725639 epoch total loss 3.30553055\n",
      "Trained batch 773 batch loss 3.22950459 epoch total loss 3.30543232\n",
      "Trained batch 774 batch loss 3.16305923 epoch total loss 3.30524826\n",
      "Trained batch 775 batch loss 3.17940474 epoch total loss 3.3050859\n",
      "Trained batch 776 batch loss 2.88302803 epoch total loss 3.30454206\n",
      "Trained batch 777 batch loss 3.11973166 epoch total loss 3.30430412\n",
      "Trained batch 778 batch loss 3.21292448 epoch total loss 3.30418658\n",
      "Trained batch 779 batch loss 3.15779781 epoch total loss 3.30399871\n",
      "Trained batch 780 batch loss 3.08269882 epoch total loss 3.30371499\n",
      "Trained batch 781 batch loss 3.33459115 epoch total loss 3.30375433\n",
      "Trained batch 782 batch loss 3.40631914 epoch total loss 3.30388546\n",
      "Trained batch 783 batch loss 3.2982378 epoch total loss 3.30387831\n",
      "Trained batch 784 batch loss 3.07562447 epoch total loss 3.3035872\n",
      "Trained batch 785 batch loss 3.21374416 epoch total loss 3.30347276\n",
      "Trained batch 786 batch loss 3.15742159 epoch total loss 3.30328703\n",
      "Trained batch 787 batch loss 3.16153 epoch total loss 3.30310702\n",
      "Trained batch 788 batch loss 3.13494563 epoch total loss 3.30289364\n",
      "Trained batch 789 batch loss 3.48165178 epoch total loss 3.30312014\n",
      "Trained batch 790 batch loss 3.21719098 epoch total loss 3.30301166\n",
      "Trained batch 791 batch loss 3.04348111 epoch total loss 3.30268335\n",
      "Trained batch 792 batch loss 3.15793347 epoch total loss 3.30250072\n",
      "Trained batch 793 batch loss 3.28292346 epoch total loss 3.30247617\n",
      "Trained batch 794 batch loss 3.10202 epoch total loss 3.30222368\n",
      "Trained batch 795 batch loss 3.04246187 epoch total loss 3.30189681\n",
      "Trained batch 796 batch loss 3.26052165 epoch total loss 3.30184484\n",
      "Trained batch 797 batch loss 3.19926357 epoch total loss 3.30171609\n",
      "Trained batch 798 batch loss 3.03849983 epoch total loss 3.30138636\n",
      "Trained batch 799 batch loss 3.1132493 epoch total loss 3.30115104\n",
      "Trained batch 800 batch loss 3.01063061 epoch total loss 3.30078793\n",
      "Trained batch 801 batch loss 3.17818165 epoch total loss 3.30063486\n",
      "Trained batch 802 batch loss 3.15181494 epoch total loss 3.30044937\n",
      "Trained batch 803 batch loss 3.02553439 epoch total loss 3.30010724\n",
      "Trained batch 804 batch loss 3.1488502 epoch total loss 3.29991913\n",
      "Trained batch 805 batch loss 2.91974449 epoch total loss 3.29944682\n",
      "Trained batch 806 batch loss 3.21177268 epoch total loss 3.29933786\n",
      "Trained batch 807 batch loss 3.24153733 epoch total loss 3.2992661\n",
      "Trained batch 808 batch loss 3.07702804 epoch total loss 3.2989912\n",
      "Trained batch 809 batch loss 2.8226409 epoch total loss 3.29840255\n",
      "Trained batch 810 batch loss 3.05958652 epoch total loss 3.29810786\n",
      "Trained batch 811 batch loss 3.06336045 epoch total loss 3.29781842\n",
      "Trained batch 812 batch loss 2.90285325 epoch total loss 3.29733205\n",
      "Trained batch 813 batch loss 2.95035219 epoch total loss 3.29690528\n",
      "Trained batch 814 batch loss 2.94054294 epoch total loss 3.2964673\n",
      "Trained batch 815 batch loss 2.90794 epoch total loss 3.29599071\n",
      "Trained batch 816 batch loss 3.05257177 epoch total loss 3.29569221\n",
      "Trained batch 817 batch loss 2.91778421 epoch total loss 3.29522967\n",
      "Trained batch 818 batch loss 3.12741184 epoch total loss 3.29502463\n",
      "Trained batch 819 batch loss 3.04698277 epoch total loss 3.2947216\n",
      "Trained batch 820 batch loss 2.96027327 epoch total loss 3.29431367\n",
      "Trained batch 821 batch loss 2.66594839 epoch total loss 3.29354835\n",
      "Trained batch 822 batch loss 2.9689641 epoch total loss 3.29315352\n",
      "Trained batch 823 batch loss 2.88873672 epoch total loss 3.29266191\n",
      "Trained batch 824 batch loss 3.02571344 epoch total loss 3.29233789\n",
      "Trained batch 825 batch loss 3.07363534 epoch total loss 3.29207301\n",
      "Trained batch 826 batch loss 3.15181637 epoch total loss 3.29190326\n",
      "Trained batch 827 batch loss 3.07285476 epoch total loss 3.29163814\n",
      "Trained batch 828 batch loss 3.10879779 epoch total loss 3.2914176\n",
      "Trained batch 829 batch loss 3.03540897 epoch total loss 3.29110861\n",
      "Trained batch 830 batch loss 3.04876566 epoch total loss 3.29081678\n",
      "Trained batch 831 batch loss 2.89389515 epoch total loss 3.29033899\n",
      "Trained batch 832 batch loss 3.04630375 epoch total loss 3.29004574\n",
      "Trained batch 833 batch loss 3.13516259 epoch total loss 3.28986\n",
      "Trained batch 834 batch loss 3.20194721 epoch total loss 3.28975463\n",
      "Trained batch 835 batch loss 3.0716095 epoch total loss 3.28949308\n",
      "Trained batch 836 batch loss 3.02274108 epoch total loss 3.28917408\n",
      "Trained batch 837 batch loss 3.01617718 epoch total loss 3.28884792\n",
      "Trained batch 838 batch loss 3.0787456 epoch total loss 3.28859735\n",
      "Trained batch 839 batch loss 3.06294775 epoch total loss 3.28832841\n",
      "Trained batch 840 batch loss 3.1054306 epoch total loss 3.28811073\n",
      "Trained batch 841 batch loss 3.1570034 epoch total loss 3.28795481\n",
      "Trained batch 842 batch loss 3.03110266 epoch total loss 3.28764963\n",
      "Trained batch 843 batch loss 2.93066549 epoch total loss 3.2872262\n",
      "Trained batch 844 batch loss 3.14460421 epoch total loss 3.28705692\n",
      "Trained batch 845 batch loss 3.08693981 epoch total loss 3.28682017\n",
      "Trained batch 846 batch loss 2.98748875 epoch total loss 3.28646636\n",
      "Trained batch 847 batch loss 3.20512342 epoch total loss 3.28637028\n",
      "Trained batch 848 batch loss 3.11558938 epoch total loss 3.28616881\n",
      "Trained batch 849 batch loss 2.84881973 epoch total loss 3.28565383\n",
      "Trained batch 850 batch loss 2.78832293 epoch total loss 3.28506875\n",
      "Trained batch 851 batch loss 3.12716889 epoch total loss 3.28488302\n",
      "Trained batch 852 batch loss 3.37686396 epoch total loss 3.28499126\n",
      "Trained batch 853 batch loss 3.38373876 epoch total loss 3.28510714\n",
      "Trained batch 854 batch loss 3.33995271 epoch total loss 3.28517103\n",
      "Trained batch 855 batch loss 3.2872057 epoch total loss 3.28517342\n",
      "Trained batch 856 batch loss 3.01844788 epoch total loss 3.2848618\n",
      "Trained batch 857 batch loss 3.43621945 epoch total loss 3.28503871\n",
      "Trained batch 858 batch loss 3.52571869 epoch total loss 3.28531909\n",
      "Trained batch 859 batch loss 3.45905209 epoch total loss 3.28552127\n",
      "Trained batch 860 batch loss 3.63244057 epoch total loss 3.28592443\n",
      "Trained batch 861 batch loss 3.34824 epoch total loss 3.28599668\n",
      "Trained batch 862 batch loss 3.32187581 epoch total loss 3.28603816\n",
      "Trained batch 863 batch loss 3.13654375 epoch total loss 3.28586483\n",
      "Trained batch 864 batch loss 3.2462585 epoch total loss 3.28581905\n",
      "Trained batch 865 batch loss 3.44322634 epoch total loss 3.28600097\n",
      "Trained batch 866 batch loss 3.07164836 epoch total loss 3.28575325\n",
      "Trained batch 867 batch loss 2.91678905 epoch total loss 3.28532767\n",
      "Trained batch 868 batch loss 3.06054831 epoch total loss 3.28506875\n",
      "Trained batch 869 batch loss 2.95349121 epoch total loss 3.28468704\n",
      "Trained batch 870 batch loss 3.06330824 epoch total loss 3.28443241\n",
      "Trained batch 871 batch loss 3.14974308 epoch total loss 3.28427768\n",
      "Trained batch 872 batch loss 3.07415366 epoch total loss 3.28403687\n",
      "Trained batch 873 batch loss 3.08723736 epoch total loss 3.28381133\n",
      "Trained batch 874 batch loss 3.06097746 epoch total loss 3.28355646\n",
      "Trained batch 875 batch loss 2.95463753 epoch total loss 3.28318048\n",
      "Trained batch 876 batch loss 3.31725717 epoch total loss 3.28321934\n",
      "Trained batch 877 batch loss 3.53538871 epoch total loss 3.28350687\n",
      "Trained batch 878 batch loss 3.34906912 epoch total loss 3.2835815\n",
      "Trained batch 879 batch loss 3.31105399 epoch total loss 3.28361273\n",
      "Trained batch 880 batch loss 3.31989288 epoch total loss 3.28365397\n",
      "Trained batch 881 batch loss 3.32197309 epoch total loss 3.28369761\n",
      "Trained batch 882 batch loss 3.18549824 epoch total loss 3.28358626\n",
      "Trained batch 883 batch loss 3.067518 epoch total loss 3.28334165\n",
      "Trained batch 884 batch loss 2.98168635 epoch total loss 3.28300047\n",
      "Trained batch 885 batch loss 3.18078446 epoch total loss 3.28288484\n",
      "Trained batch 886 batch loss 3.02514124 epoch total loss 3.28259397\n",
      "Trained batch 887 batch loss 3.34711647 epoch total loss 3.28266668\n",
      "Trained batch 888 batch loss 3.12542152 epoch total loss 3.28248978\n",
      "Trained batch 889 batch loss 3.10267496 epoch total loss 3.2822876\n",
      "Trained batch 890 batch loss 3.35492706 epoch total loss 3.28236914\n",
      "Trained batch 891 batch loss 3.2614336 epoch total loss 3.28234577\n",
      "Trained batch 892 batch loss 3.21557045 epoch total loss 3.28227091\n",
      "Trained batch 893 batch loss 3.43351555 epoch total loss 3.28244042\n",
      "Trained batch 894 batch loss 3.59029055 epoch total loss 3.2827847\n",
      "Trained batch 895 batch loss 3.35842967 epoch total loss 3.28286934\n",
      "Trained batch 896 batch loss 3.30691886 epoch total loss 3.28289604\n",
      "Trained batch 897 batch loss 3.24044704 epoch total loss 3.28284883\n",
      "Trained batch 898 batch loss 3.40282869 epoch total loss 3.28298235\n",
      "Trained batch 899 batch loss 3.44291592 epoch total loss 3.28316021\n",
      "Trained batch 900 batch loss 3.42240596 epoch total loss 3.28331494\n",
      "Trained batch 901 batch loss 3.30885029 epoch total loss 3.28334332\n",
      "Trained batch 902 batch loss 3.26782274 epoch total loss 3.28332591\n",
      "Trained batch 903 batch loss 3.44921017 epoch total loss 3.28350973\n",
      "Trained batch 904 batch loss 3.49568939 epoch total loss 3.28374434\n",
      "Trained batch 905 batch loss 3.39001679 epoch total loss 3.28386188\n",
      "Trained batch 906 batch loss 3.35322499 epoch total loss 3.28393841\n",
      "Trained batch 907 batch loss 3.37587976 epoch total loss 3.28404\n",
      "Trained batch 908 batch loss 3.09576511 epoch total loss 3.28383255\n",
      "Trained batch 909 batch loss 3.17645311 epoch total loss 3.28371453\n",
      "Trained batch 910 batch loss 2.95142341 epoch total loss 3.28334928\n",
      "Trained batch 911 batch loss 3.14353609 epoch total loss 3.28319597\n",
      "Trained batch 912 batch loss 3.29833293 epoch total loss 3.28321242\n",
      "Trained batch 913 batch loss 3.20825791 epoch total loss 3.28313041\n",
      "Trained batch 914 batch loss 3.45184612 epoch total loss 3.28331494\n",
      "Trained batch 915 batch loss 3.54956222 epoch total loss 3.28360605\n",
      "Trained batch 916 batch loss 3.39909434 epoch total loss 3.28373218\n",
      "Trained batch 917 batch loss 3.29973054 epoch total loss 3.28374982\n",
      "Trained batch 918 batch loss 3.13896751 epoch total loss 3.28359199\n",
      "Trained batch 919 batch loss 2.96869612 epoch total loss 3.28324938\n",
      "Trained batch 920 batch loss 2.86911654 epoch total loss 3.28279924\n",
      "Trained batch 921 batch loss 3.25257087 epoch total loss 3.28276658\n",
      "Trained batch 922 batch loss 2.99386716 epoch total loss 3.2824533\n",
      "Trained batch 923 batch loss 2.99956799 epoch total loss 3.28214669\n",
      "Trained batch 924 batch loss 3.22920847 epoch total loss 3.28208947\n",
      "Trained batch 925 batch loss 2.91839981 epoch total loss 3.28169632\n",
      "Trained batch 926 batch loss 3.21092892 epoch total loss 3.28161979\n",
      "Trained batch 927 batch loss 3.09721136 epoch total loss 3.28142095\n",
      "Trained batch 928 batch loss 2.97917 epoch total loss 3.28109527\n",
      "Trained batch 929 batch loss 2.96384788 epoch total loss 3.28075385\n",
      "Trained batch 930 batch loss 3.36934471 epoch total loss 3.28084922\n",
      "Trained batch 931 batch loss 3.39278 epoch total loss 3.28096938\n",
      "Trained batch 932 batch loss 2.9641161 epoch total loss 3.2806294\n",
      "Trained batch 933 batch loss 2.96983814 epoch total loss 3.28029609\n",
      "Trained batch 934 batch loss 2.89851093 epoch total loss 3.27988744\n",
      "Trained batch 935 batch loss 3.14826107 epoch total loss 3.27974653\n",
      "Trained batch 936 batch loss 3.3118186 epoch total loss 3.27978063\n",
      "Trained batch 937 batch loss 3.08888435 epoch total loss 3.27957702\n",
      "Trained batch 938 batch loss 3.16878724 epoch total loss 3.27945876\n",
      "Trained batch 939 batch loss 3.16156292 epoch total loss 3.27933335\n",
      "Trained batch 940 batch loss 3.0042243 epoch total loss 3.27904058\n",
      "Trained batch 941 batch loss 3.13022137 epoch total loss 3.27888227\n",
      "Trained batch 942 batch loss 3.11217546 epoch total loss 3.27870512\n",
      "Trained batch 943 batch loss 3.07383275 epoch total loss 3.27848792\n",
      "Trained batch 944 batch loss 3.06532526 epoch total loss 3.27826214\n",
      "Trained batch 945 batch loss 3.06363106 epoch total loss 3.27803516\n",
      "Trained batch 946 batch loss 3.05760646 epoch total loss 3.27780199\n",
      "Trained batch 947 batch loss 3.01580811 epoch total loss 3.27752542\n",
      "Trained batch 948 batch loss 3.01763558 epoch total loss 3.27725124\n",
      "Trained batch 949 batch loss 3.04568815 epoch total loss 3.27700734\n",
      "Trained batch 950 batch loss 3.03406286 epoch total loss 3.27675176\n",
      "Trained batch 951 batch loss 2.90147185 epoch total loss 3.27635694\n",
      "Trained batch 952 batch loss 3.00390458 epoch total loss 3.27607083\n",
      "Trained batch 953 batch loss 3.13585973 epoch total loss 3.27592349\n",
      "Trained batch 954 batch loss 3.31444073 epoch total loss 3.27596378\n",
      "Trained batch 955 batch loss 3.29715705 epoch total loss 3.27598596\n",
      "Trained batch 956 batch loss 3.07149601 epoch total loss 3.27577209\n",
      "Trained batch 957 batch loss 2.96714354 epoch total loss 3.27544951\n",
      "Trained batch 958 batch loss 3.22664905 epoch total loss 3.27539849\n",
      "Trained batch 959 batch loss 2.93323278 epoch total loss 3.27504182\n",
      "Trained batch 960 batch loss 3.28665829 epoch total loss 3.27505398\n",
      "Trained batch 961 batch loss 3.20925498 epoch total loss 3.27498531\n",
      "Trained batch 962 batch loss 3.12006879 epoch total loss 3.27482438\n",
      "Trained batch 963 batch loss 3.10704303 epoch total loss 3.27465\n",
      "Trained batch 964 batch loss 3.23420668 epoch total loss 3.27460814\n",
      "Trained batch 965 batch loss 3.2104423 epoch total loss 3.27454162\n",
      "Trained batch 966 batch loss 3.19948053 epoch total loss 3.27446389\n",
      "Trained batch 967 batch loss 3.08011222 epoch total loss 3.27426291\n",
      "Trained batch 968 batch loss 3.19113255 epoch total loss 3.27417707\n",
      "Trained batch 969 batch loss 3.18530869 epoch total loss 3.27408528\n",
      "Trained batch 970 batch loss 3.3647716 epoch total loss 3.27417874\n",
      "Trained batch 971 batch loss 3.15824699 epoch total loss 3.2740593\n",
      "Trained batch 972 batch loss 3.16284513 epoch total loss 3.27394485\n",
      "Trained batch 973 batch loss 3.2086513 epoch total loss 3.27387786\n",
      "Trained batch 974 batch loss 3.05898094 epoch total loss 3.27365732\n",
      "Trained batch 975 batch loss 3.29463291 epoch total loss 3.27367878\n",
      "Trained batch 976 batch loss 3.12406111 epoch total loss 3.27352548\n",
      "Trained batch 977 batch loss 3.12877321 epoch total loss 3.27337718\n",
      "Trained batch 978 batch loss 3.24718809 epoch total loss 3.27335048\n",
      "Trained batch 979 batch loss 3.03136683 epoch total loss 3.273103\n",
      "Trained batch 980 batch loss 2.94695711 epoch total loss 3.2727704\n",
      "Trained batch 981 batch loss 3.27460527 epoch total loss 3.27277231\n",
      "Trained batch 982 batch loss 3.15049028 epoch total loss 3.27264762\n",
      "Trained batch 983 batch loss 3.10184669 epoch total loss 3.27247381\n",
      "Trained batch 984 batch loss 3.08964467 epoch total loss 3.27228808\n",
      "Trained batch 985 batch loss 3.16996598 epoch total loss 3.27218413\n",
      "Trained batch 986 batch loss 3.06859493 epoch total loss 3.27197766\n",
      "Trained batch 987 batch loss 3.02933502 epoch total loss 3.27173162\n",
      "Trained batch 988 batch loss 3.37014699 epoch total loss 3.27183127\n",
      "Trained batch 989 batch loss 3.20918417 epoch total loss 3.27176809\n",
      "Trained batch 990 batch loss 2.93619347 epoch total loss 3.27142906\n",
      "Trained batch 991 batch loss 3.12540245 epoch total loss 3.27128196\n",
      "Trained batch 992 batch loss 2.94633722 epoch total loss 3.27095437\n",
      "Trained batch 993 batch loss 3.00987053 epoch total loss 3.27069116\n",
      "Trained batch 994 batch loss 2.93406963 epoch total loss 3.2703526\n",
      "Trained batch 995 batch loss 2.75423479 epoch total loss 3.2698338\n",
      "Trained batch 996 batch loss 2.69863939 epoch total loss 3.26926041\n",
      "Trained batch 997 batch loss 2.78699541 epoch total loss 3.26877689\n",
      "Trained batch 998 batch loss 2.85702848 epoch total loss 3.26836419\n",
      "Trained batch 999 batch loss 2.85878 epoch total loss 3.26795435\n",
      "Trained batch 1000 batch loss 3.08883071 epoch total loss 3.26777506\n",
      "Trained batch 1001 batch loss 2.97478795 epoch total loss 3.26748252\n",
      "Trained batch 1002 batch loss 3.0697093 epoch total loss 3.26728535\n",
      "Trained batch 1003 batch loss 3.00415 epoch total loss 3.26702285\n",
      "Trained batch 1004 batch loss 2.94398975 epoch total loss 3.26670122\n",
      "Trained batch 1005 batch loss 2.97042537 epoch total loss 3.26640654\n",
      "Trained batch 1006 batch loss 3.07481766 epoch total loss 3.26621604\n",
      "Trained batch 1007 batch loss 3.05826211 epoch total loss 3.26600957\n",
      "Trained batch 1008 batch loss 2.96511245 epoch total loss 3.26571107\n",
      "Trained batch 1009 batch loss 2.73252606 epoch total loss 3.2651825\n",
      "Trained batch 1010 batch loss 3.20224667 epoch total loss 3.26512\n",
      "Trained batch 1011 batch loss 3.1296339 epoch total loss 3.26498604\n",
      "Trained batch 1012 batch loss 2.95993567 epoch total loss 3.26468468\n",
      "Trained batch 1013 batch loss 2.73985648 epoch total loss 3.26416636\n",
      "Trained batch 1014 batch loss 2.54112744 epoch total loss 3.26345325\n",
      "Trained batch 1015 batch loss 2.85146499 epoch total loss 3.26304746\n",
      "Trained batch 1016 batch loss 2.53945494 epoch total loss 3.2623353\n",
      "Trained batch 1017 batch loss 2.22018218 epoch total loss 3.26131058\n",
      "Trained batch 1018 batch loss 2.31810164 epoch total loss 3.26038408\n",
      "Trained batch 1019 batch loss 2.49692702 epoch total loss 3.25963473\n",
      "Trained batch 1020 batch loss 3.26645899 epoch total loss 3.25964141\n",
      "Trained batch 1021 batch loss 3.39514256 epoch total loss 3.25977421\n",
      "Trained batch 1022 batch loss 3.19414377 epoch total loss 3.25971\n",
      "Trained batch 1023 batch loss 3.24634361 epoch total loss 3.25969696\n",
      "Trained batch 1024 batch loss 3.18743324 epoch total loss 3.25962639\n",
      "Trained batch 1025 batch loss 3.13405609 epoch total loss 3.25950384\n",
      "Trained batch 1026 batch loss 3.09452486 epoch total loss 3.25934291\n",
      "Trained batch 1027 batch loss 3.16911507 epoch total loss 3.25925517\n",
      "Trained batch 1028 batch loss 2.81391668 epoch total loss 3.25882196\n",
      "Trained batch 1029 batch loss 3.21062708 epoch total loss 3.25877523\n",
      "Trained batch 1030 batch loss 3.03297305 epoch total loss 3.25855613\n",
      "Trained batch 1031 batch loss 3.01595902 epoch total loss 3.25832057\n",
      "Trained batch 1032 batch loss 2.79919624 epoch total loss 3.25787592\n",
      "Trained batch 1033 batch loss 2.73162293 epoch total loss 3.25736642\n",
      "Trained batch 1034 batch loss 2.78099442 epoch total loss 3.25690579\n",
      "Trained batch 1035 batch loss 3.04557395 epoch total loss 3.25670171\n",
      "Trained batch 1036 batch loss 2.95345688 epoch total loss 3.25640893\n",
      "Trained batch 1037 batch loss 2.9910264 epoch total loss 3.25615287\n",
      "Trained batch 1038 batch loss 2.89843535 epoch total loss 3.25580835\n",
      "Trained batch 1039 batch loss 3.08288288 epoch total loss 3.2556417\n",
      "Trained batch 1040 batch loss 2.58381367 epoch total loss 3.25499582\n",
      "Trained batch 1041 batch loss 3.08803988 epoch total loss 3.25483537\n",
      "Trained batch 1042 batch loss 3.04280663 epoch total loss 3.254632\n",
      "Trained batch 1043 batch loss 2.96072412 epoch total loss 3.25435\n",
      "Trained batch 1044 batch loss 3.05776215 epoch total loss 3.25416183\n",
      "Trained batch 1045 batch loss 3.0204854 epoch total loss 3.2539382\n",
      "Trained batch 1046 batch loss 2.97516799 epoch total loss 3.25367165\n",
      "Trained batch 1047 batch loss 2.93016696 epoch total loss 3.25336266\n",
      "Trained batch 1048 batch loss 2.78174973 epoch total loss 3.25291276\n",
      "Trained batch 1049 batch loss 3.11604476 epoch total loss 3.25278211\n",
      "Trained batch 1050 batch loss 3.3124938 epoch total loss 3.25283909\n",
      "Trained batch 1051 batch loss 3.11671758 epoch total loss 3.25270939\n",
      "Trained batch 1052 batch loss 3.16311502 epoch total loss 3.25262427\n",
      "Trained batch 1053 batch loss 3.21278119 epoch total loss 3.2525866\n",
      "Trained batch 1054 batch loss 3.05594754 epoch total loss 3.2524\n",
      "Trained batch 1055 batch loss 3.13187242 epoch total loss 3.25228572\n",
      "Trained batch 1056 batch loss 3.02094555 epoch total loss 3.25206661\n",
      "Trained batch 1057 batch loss 2.90249157 epoch total loss 3.25173593\n",
      "Trained batch 1058 batch loss 2.73077512 epoch total loss 3.25124359\n",
      "Trained batch 1059 batch loss 2.66597652 epoch total loss 3.25069094\n",
      "Trained batch 1060 batch loss 2.79746914 epoch total loss 3.25026321\n",
      "Trained batch 1061 batch loss 2.96457767 epoch total loss 3.24999404\n",
      "Trained batch 1062 batch loss 3.13727641 epoch total loss 3.2498877\n",
      "Trained batch 1063 batch loss 2.93670654 epoch total loss 3.24959326\n",
      "Trained batch 1064 batch loss 3.11779356 epoch total loss 3.24946928\n",
      "Trained batch 1065 batch loss 2.97949982 epoch total loss 3.24921584\n",
      "Trained batch 1066 batch loss 3.26357841 epoch total loss 3.24922943\n",
      "Trained batch 1067 batch loss 3.38050389 epoch total loss 3.24935246\n",
      "Trained batch 1068 batch loss 3.0421195 epoch total loss 3.24915862\n",
      "Trained batch 1069 batch loss 3.07888937 epoch total loss 3.24899912\n",
      "Trained batch 1070 batch loss 3.15987134 epoch total loss 3.24891591\n",
      "Trained batch 1071 batch loss 3.10655284 epoch total loss 3.24878287\n",
      "Trained batch 1072 batch loss 3.05563545 epoch total loss 3.24860287\n",
      "Trained batch 1073 batch loss 3.06440711 epoch total loss 3.24843121\n",
      "Trained batch 1074 batch loss 2.92894578 epoch total loss 3.24813366\n",
      "Trained batch 1075 batch loss 2.90116596 epoch total loss 3.24781084\n",
      "Trained batch 1076 batch loss 3.13123608 epoch total loss 3.2477026\n",
      "Trained batch 1077 batch loss 2.85500717 epoch total loss 3.24733806\n",
      "Trained batch 1078 batch loss 3.14390898 epoch total loss 3.24724197\n",
      "Trained batch 1079 batch loss 3.00636125 epoch total loss 3.24701881\n",
      "Trained batch 1080 batch loss 2.86351228 epoch total loss 3.24666357\n",
      "Trained batch 1081 batch loss 3.0359602 epoch total loss 3.24646854\n",
      "Trained batch 1082 batch loss 2.90632653 epoch total loss 3.24615431\n",
      "Trained batch 1083 batch loss 3.3851006 epoch total loss 3.24628234\n",
      "Trained batch 1084 batch loss 3.25343466 epoch total loss 3.24628901\n",
      "Trained batch 1085 batch loss 3.58165073 epoch total loss 3.24659801\n",
      "Trained batch 1086 batch loss 3.51269054 epoch total loss 3.2468431\n",
      "Trained batch 1087 batch loss 3.51753974 epoch total loss 3.24709201\n",
      "Trained batch 1088 batch loss 3.36656833 epoch total loss 3.24720192\n",
      "Trained batch 1089 batch loss 3.43603945 epoch total loss 3.24737525\n",
      "Trained batch 1090 batch loss 2.81375599 epoch total loss 3.24697733\n",
      "Trained batch 1091 batch loss 2.78744698 epoch total loss 3.24655604\n",
      "Trained batch 1092 batch loss 2.80219221 epoch total loss 3.2461493\n",
      "Trained batch 1093 batch loss 2.98884296 epoch total loss 3.24591374\n",
      "Trained batch 1094 batch loss 2.69953156 epoch total loss 3.24541426\n",
      "Trained batch 1095 batch loss 3.00496435 epoch total loss 3.24519467\n",
      "Trained batch 1096 batch loss 3.06415367 epoch total loss 3.24502945\n",
      "Trained batch 1097 batch loss 3.05119562 epoch total loss 3.24485278\n",
      "Trained batch 1098 batch loss 2.84297204 epoch total loss 3.24448681\n",
      "Trained batch 1099 batch loss 3.05053282 epoch total loss 3.24431038\n",
      "Trained batch 1100 batch loss 3.0505805 epoch total loss 3.24413419\n",
      "Trained batch 1101 batch loss 2.83175945 epoch total loss 3.24375963\n",
      "Trained batch 1102 batch loss 3.01441407 epoch total loss 3.24355149\n",
      "Trained batch 1103 batch loss 3.06382132 epoch total loss 3.24338841\n",
      "Trained batch 1104 batch loss 3.20588088 epoch total loss 3.24335456\n",
      "Trained batch 1105 batch loss 3.23383 epoch total loss 3.24334598\n",
      "Trained batch 1106 batch loss 3.25801778 epoch total loss 3.24335909\n",
      "Trained batch 1107 batch loss 3.13269591 epoch total loss 3.24325943\n",
      "Trained batch 1108 batch loss 3.15958261 epoch total loss 3.24318385\n",
      "Trained batch 1109 batch loss 3.24611712 epoch total loss 3.24318647\n",
      "Trained batch 1110 batch loss 3.04310894 epoch total loss 3.24300647\n",
      "Trained batch 1111 batch loss 3.0453434 epoch total loss 3.24282861\n",
      "Trained batch 1112 batch loss 3.07064056 epoch total loss 3.24267364\n",
      "Trained batch 1113 batch loss 2.98772097 epoch total loss 3.24244452\n",
      "Trained batch 1114 batch loss 2.88206244 epoch total loss 3.24212098\n",
      "Trained batch 1115 batch loss 2.98541379 epoch total loss 3.24189091\n",
      "Trained batch 1116 batch loss 3.15802932 epoch total loss 3.24181557\n",
      "Trained batch 1117 batch loss 3.22092485 epoch total loss 3.24179697\n",
      "Trained batch 1118 batch loss 3.10726428 epoch total loss 3.24167657\n",
      "Trained batch 1119 batch loss 2.95201516 epoch total loss 3.24141765\n",
      "Trained batch 1120 batch loss 2.9769125 epoch total loss 3.24118137\n",
      "Trained batch 1121 batch loss 2.85670376 epoch total loss 3.24083829\n",
      "Trained batch 1122 batch loss 2.86295247 epoch total loss 3.24050164\n",
      "Trained batch 1123 batch loss 2.87589502 epoch total loss 3.24017692\n",
      "Trained batch 1124 batch loss 3.07110882 epoch total loss 3.24002647\n",
      "Trained batch 1125 batch loss 2.78022647 epoch total loss 3.23961782\n",
      "Trained batch 1126 batch loss 2.96085358 epoch total loss 3.23937035\n",
      "Trained batch 1127 batch loss 2.78862429 epoch total loss 3.23897028\n",
      "Trained batch 1128 batch loss 2.77308846 epoch total loss 3.23855734\n",
      "Trained batch 1129 batch loss 2.76568937 epoch total loss 3.23813844\n",
      "Trained batch 1130 batch loss 3.23259234 epoch total loss 3.23813367\n",
      "Trained batch 1131 batch loss 3.07415342 epoch total loss 3.23798871\n",
      "Trained batch 1132 batch loss 3.31089973 epoch total loss 3.23805308\n",
      "Trained batch 1133 batch loss 3.0938921 epoch total loss 3.23792601\n",
      "Trained batch 1134 batch loss 3.12874341 epoch total loss 3.23782969\n",
      "Trained batch 1135 batch loss 2.96399975 epoch total loss 3.23758841\n",
      "Trained batch 1136 batch loss 2.81976724 epoch total loss 3.23722076\n",
      "Trained batch 1137 batch loss 3.13904953 epoch total loss 3.23713446\n",
      "Trained batch 1138 batch loss 3.03211355 epoch total loss 3.23695445\n",
      "Trained batch 1139 batch loss 3.06542587 epoch total loss 3.23680377\n",
      "Trained batch 1140 batch loss 3.03297901 epoch total loss 3.23662496\n",
      "Trained batch 1141 batch loss 3.232059 epoch total loss 3.23662114\n",
      "Trained batch 1142 batch loss 2.93218803 epoch total loss 3.23635435\n",
      "Trained batch 1143 batch loss 2.9488945 epoch total loss 3.23610306\n",
      "Trained batch 1144 batch loss 3.038836 epoch total loss 3.23593044\n",
      "Trained batch 1145 batch loss 3.18955421 epoch total loss 3.23589\n",
      "Trained batch 1146 batch loss 3.18111324 epoch total loss 3.23584223\n",
      "Trained batch 1147 batch loss 3.20870543 epoch total loss 3.23581862\n",
      "Trained batch 1148 batch loss 3.16204762 epoch total loss 3.23575425\n",
      "Trained batch 1149 batch loss 2.8536849 epoch total loss 3.2354219\n",
      "Trained batch 1150 batch loss 2.43217683 epoch total loss 3.23472333\n",
      "Trained batch 1151 batch loss 3.08123159 epoch total loss 3.23459\n",
      "Trained batch 1152 batch loss 3.0417943 epoch total loss 3.23442268\n",
      "Trained batch 1153 batch loss 3.28233 epoch total loss 3.23446417\n",
      "Trained batch 1154 batch loss 2.97645617 epoch total loss 3.23424077\n",
      "Trained batch 1155 batch loss 3.06681967 epoch total loss 3.23409581\n",
      "Trained batch 1156 batch loss 2.89305401 epoch total loss 3.23380065\n",
      "Trained batch 1157 batch loss 3.07463765 epoch total loss 3.23366332\n",
      "Trained batch 1158 batch loss 2.96378636 epoch total loss 3.23343039\n",
      "Trained batch 1159 batch loss 3.16027427 epoch total loss 3.23336697\n",
      "Trained batch 1160 batch loss 3.01600146 epoch total loss 3.23317981\n",
      "Trained batch 1161 batch loss 2.96055698 epoch total loss 3.23294497\n",
      "Trained batch 1162 batch loss 2.91635752 epoch total loss 3.23267221\n",
      "Trained batch 1163 batch loss 3.26171684 epoch total loss 3.23269725\n",
      "Trained batch 1164 batch loss 3.02325296 epoch total loss 3.23251724\n",
      "Trained batch 1165 batch loss 2.84131479 epoch total loss 3.23218155\n",
      "Trained batch 1166 batch loss 3.3484354 epoch total loss 3.23228121\n",
      "Trained batch 1167 batch loss 3.16357327 epoch total loss 3.23222232\n",
      "Trained batch 1168 batch loss 3.24057817 epoch total loss 3.23222947\n",
      "Trained batch 1169 batch loss 3.07084084 epoch total loss 3.23209119\n",
      "Trained batch 1170 batch loss 3.10561 epoch total loss 3.23198318\n",
      "Trained batch 1171 batch loss 3.03315592 epoch total loss 3.23181343\n",
      "Trained batch 1172 batch loss 3.60927987 epoch total loss 3.23213577\n",
      "Trained batch 1173 batch loss 3.3581531 epoch total loss 3.23224306\n",
      "Trained batch 1174 batch loss 3.11407089 epoch total loss 3.23214245\n",
      "Trained batch 1175 batch loss 3.16330624 epoch total loss 3.2320838\n",
      "Trained batch 1176 batch loss 3.28877544 epoch total loss 3.23213196\n",
      "Trained batch 1177 batch loss 2.9549849 epoch total loss 3.23189664\n",
      "Trained batch 1178 batch loss 3.13374877 epoch total loss 3.23181343\n",
      "Trained batch 1179 batch loss 3.22678423 epoch total loss 3.23180914\n",
      "Trained batch 1180 batch loss 3.23391819 epoch total loss 3.23181081\n",
      "Trained batch 1181 batch loss 3.20930815 epoch total loss 3.23179173\n",
      "Trained batch 1182 batch loss 3.15526056 epoch total loss 3.23172712\n",
      "Trained batch 1183 batch loss 2.90750217 epoch total loss 3.23145294\n",
      "Trained batch 1184 batch loss 3.05210304 epoch total loss 3.23130131\n",
      "Trained batch 1185 batch loss 3.07544541 epoch total loss 3.23117\n",
      "Trained batch 1186 batch loss 3.02871037 epoch total loss 3.23099923\n",
      "Trained batch 1187 batch loss 2.88253403 epoch total loss 3.23070574\n",
      "Trained batch 1188 batch loss 3.03321147 epoch total loss 3.23053956\n",
      "Trained batch 1189 batch loss 2.95365572 epoch total loss 3.23030663\n",
      "Trained batch 1190 batch loss 2.82249689 epoch total loss 3.22996378\n",
      "Trained batch 1191 batch loss 3.00004244 epoch total loss 3.22977066\n",
      "Trained batch 1192 batch loss 3.25861883 epoch total loss 3.22979498\n",
      "Trained batch 1193 batch loss 2.98519754 epoch total loss 3.2295897\n",
      "Trained batch 1194 batch loss 3.07977223 epoch total loss 3.22946429\n",
      "Trained batch 1195 batch loss 3.19555521 epoch total loss 3.22943592\n",
      "Trained batch 1196 batch loss 3.37272263 epoch total loss 3.22955585\n",
      "Trained batch 1197 batch loss 3.26793933 epoch total loss 3.22958779\n",
      "Trained batch 1198 batch loss 3.23398185 epoch total loss 3.22959137\n",
      "Trained batch 1199 batch loss 3.11549783 epoch total loss 3.22949624\n",
      "Trained batch 1200 batch loss 3.06181335 epoch total loss 3.22935653\n",
      "Trained batch 1201 batch loss 3.37779403 epoch total loss 3.22948\n",
      "Trained batch 1202 batch loss 3.12504315 epoch total loss 3.22939301\n",
      "Trained batch 1203 batch loss 3.09654522 epoch total loss 3.22928262\n",
      "Trained batch 1204 batch loss 3.027426 epoch total loss 3.22911477\n",
      "Trained batch 1205 batch loss 3.04831982 epoch total loss 3.22896481\n",
      "Trained batch 1206 batch loss 3.03618217 epoch total loss 3.22880483\n",
      "Trained batch 1207 batch loss 2.9875567 epoch total loss 3.22860503\n",
      "Trained batch 1208 batch loss 2.8214035 epoch total loss 3.22826791\n",
      "Trained batch 1209 batch loss 2.9867816 epoch total loss 3.22806811\n",
      "Trained batch 1210 batch loss 2.76876235 epoch total loss 3.22768855\n",
      "Trained batch 1211 batch loss 3.32488966 epoch total loss 3.2277689\n",
      "Trained batch 1212 batch loss 3.15321469 epoch total loss 3.22770739\n",
      "Trained batch 1213 batch loss 2.8935976 epoch total loss 3.22743201\n",
      "Trained batch 1214 batch loss 2.95676231 epoch total loss 3.22720909\n",
      "Trained batch 1215 batch loss 2.69002223 epoch total loss 3.22676682\n",
      "Trained batch 1216 batch loss 3.09810114 epoch total loss 3.22666097\n",
      "Trained batch 1217 batch loss 3.19171619 epoch total loss 3.22663236\n",
      "Trained batch 1218 batch loss 3.01426268 epoch total loss 3.22645783\n",
      "Trained batch 1219 batch loss 3.25150919 epoch total loss 3.22647834\n",
      "Trained batch 1220 batch loss 3.17275834 epoch total loss 3.22643447\n",
      "Trained batch 1221 batch loss 3.29444838 epoch total loss 3.22649\n",
      "Trained batch 1222 batch loss 3.08904552 epoch total loss 3.22637773\n",
      "Trained batch 1223 batch loss 3.20842361 epoch total loss 3.22636318\n",
      "Trained batch 1224 batch loss 3.50455213 epoch total loss 3.22659039\n",
      "Trained batch 1225 batch loss 3.61362028 epoch total loss 3.2269063\n",
      "Trained batch 1226 batch loss 3.25492263 epoch total loss 3.22692919\n",
      "Trained batch 1227 batch loss 3.17985034 epoch total loss 3.2268908\n",
      "Trained batch 1228 batch loss 2.89235663 epoch total loss 3.22661829\n",
      "Trained batch 1229 batch loss 2.8799386 epoch total loss 3.22633624\n",
      "Trained batch 1230 batch loss 2.96154499 epoch total loss 3.22612095\n",
      "Trained batch 1231 batch loss 2.99931574 epoch total loss 3.22593665\n",
      "Trained batch 1232 batch loss 2.83432078 epoch total loss 3.2256186\n",
      "Trained batch 1233 batch loss 2.57739019 epoch total loss 3.22509289\n",
      "Trained batch 1234 batch loss 2.41784859 epoch total loss 3.22443891\n",
      "Trained batch 1235 batch loss 2.52506113 epoch total loss 3.22387266\n",
      "Trained batch 1236 batch loss 2.67475176 epoch total loss 3.22342849\n",
      "Trained batch 1237 batch loss 3.12580609 epoch total loss 3.22334933\n",
      "Trained batch 1238 batch loss 3.04204464 epoch total loss 3.22320294\n",
      "Trained batch 1239 batch loss 2.51321268 epoch total loss 3.22262979\n",
      "Trained batch 1240 batch loss 2.50746965 epoch total loss 3.22205329\n",
      "Trained batch 1241 batch loss 2.4435606 epoch total loss 3.22142601\n",
      "Trained batch 1242 batch loss 2.47859478 epoch total loss 3.22082782\n",
      "Trained batch 1243 batch loss 2.5547514 epoch total loss 3.22029185\n",
      "Trained batch 1244 batch loss 2.54864049 epoch total loss 3.21975183\n",
      "Trained batch 1245 batch loss 2.58026314 epoch total loss 3.21923828\n",
      "Trained batch 1246 batch loss 2.70341229 epoch total loss 3.21882415\n",
      "Trained batch 1247 batch loss 2.7820065 epoch total loss 3.21847391\n",
      "Trained batch 1248 batch loss 3.22958 epoch total loss 3.21848273\n",
      "Trained batch 1249 batch loss 2.99710274 epoch total loss 3.21830559\n",
      "Trained batch 1250 batch loss 3.05301189 epoch total loss 3.21817327\n",
      "Trained batch 1251 batch loss 3.03199339 epoch total loss 3.21802449\n",
      "Trained batch 1252 batch loss 3.23392034 epoch total loss 3.21803713\n",
      "Trained batch 1253 batch loss 3.17877293 epoch total loss 3.21800566\n",
      "Trained batch 1254 batch loss 3.24132061 epoch total loss 3.21802425\n",
      "Trained batch 1255 batch loss 3.47633696 epoch total loss 3.21823\n",
      "Trained batch 1256 batch loss 3.31644964 epoch total loss 3.21830821\n",
      "Trained batch 1257 batch loss 3.20219946 epoch total loss 3.21829534\n",
      "Trained batch 1258 batch loss 3.13864756 epoch total loss 3.21823192\n",
      "Trained batch 1259 batch loss 2.98556781 epoch total loss 3.21804714\n",
      "Trained batch 1260 batch loss 2.99918175 epoch total loss 3.21787357\n",
      "Trained batch 1261 batch loss 2.84425354 epoch total loss 3.21757722\n",
      "Trained batch 1262 batch loss 2.87458587 epoch total loss 3.21730542\n",
      "Trained batch 1263 batch loss 3.01159048 epoch total loss 3.21714258\n",
      "Trained batch 1264 batch loss 3.13596296 epoch total loss 3.21707821\n",
      "Trained batch 1265 batch loss 2.78764844 epoch total loss 3.2167387\n",
      "Trained batch 1266 batch loss 2.99228954 epoch total loss 3.21656132\n",
      "Trained batch 1267 batch loss 3.0611887 epoch total loss 3.21643877\n",
      "Trained batch 1268 batch loss 3.14710379 epoch total loss 3.21638417\n",
      "Trained batch 1269 batch loss 2.98624587 epoch total loss 3.21620297\n",
      "Trained batch 1270 batch loss 3.10740447 epoch total loss 3.21611738\n",
      "Trained batch 1271 batch loss 3.00464463 epoch total loss 3.21595097\n",
      "Trained batch 1272 batch loss 2.92835855 epoch total loss 3.21572495\n",
      "Trained batch 1273 batch loss 3.19592595 epoch total loss 3.21570945\n",
      "Trained batch 1274 batch loss 3.05233216 epoch total loss 3.21558118\n",
      "Trained batch 1275 batch loss 3.24910212 epoch total loss 3.2156074\n",
      "Trained batch 1276 batch loss 3.31114984 epoch total loss 3.21568227\n",
      "Trained batch 1277 batch loss 2.91884708 epoch total loss 3.21544981\n",
      "Trained batch 1278 batch loss 2.97650552 epoch total loss 3.21526289\n",
      "Trained batch 1279 batch loss 3.18403 epoch total loss 3.21523857\n",
      "Trained batch 1280 batch loss 3.00363588 epoch total loss 3.21507311\n",
      "Trained batch 1281 batch loss 2.92349482 epoch total loss 3.21484518\n",
      "Trained batch 1282 batch loss 2.97756314 epoch total loss 3.21466017\n",
      "Trained batch 1283 batch loss 2.77232313 epoch total loss 3.21431541\n",
      "Trained batch 1284 batch loss 2.784482 epoch total loss 3.21398091\n",
      "Trained batch 1285 batch loss 2.68056154 epoch total loss 3.21356583\n",
      "Trained batch 1286 batch loss 2.43036056 epoch total loss 3.21295667\n",
      "Trained batch 1287 batch loss 3.11581 epoch total loss 3.21288109\n",
      "Trained batch 1288 batch loss 3.23670959 epoch total loss 3.21289968\n",
      "Trained batch 1289 batch loss 2.97971249 epoch total loss 3.21271873\n",
      "Trained batch 1290 batch loss 2.80986834 epoch total loss 3.2124064\n",
      "Trained batch 1291 batch loss 3.22547102 epoch total loss 3.21241665\n",
      "Trained batch 1292 batch loss 3.11786127 epoch total loss 3.21234345\n",
      "Trained batch 1293 batch loss 3.18542314 epoch total loss 3.21232271\n",
      "Trained batch 1294 batch loss 3.06323981 epoch total loss 3.21220756\n",
      "Trained batch 1295 batch loss 2.97152567 epoch total loss 3.21202183\n",
      "Trained batch 1296 batch loss 3.151057 epoch total loss 3.21197462\n",
      "Trained batch 1297 batch loss 2.57589626 epoch total loss 3.21148419\n",
      "Trained batch 1298 batch loss 2.5037756 epoch total loss 3.21093893\n",
      "Trained batch 1299 batch loss 2.94987655 epoch total loss 3.21073794\n",
      "Trained batch 1300 batch loss 3.07434773 epoch total loss 3.2106328\n",
      "Trained batch 1301 batch loss 3.08128595 epoch total loss 3.21053338\n",
      "Trained batch 1302 batch loss 3.12497354 epoch total loss 3.21046758\n",
      "Trained batch 1303 batch loss 3.18156409 epoch total loss 3.2104454\n",
      "Trained batch 1304 batch loss 2.85694265 epoch total loss 3.21017432\n",
      "Trained batch 1305 batch loss 2.88006949 epoch total loss 3.20992136\n",
      "Trained batch 1306 batch loss 3.0569849 epoch total loss 3.2098043\n",
      "Trained batch 1307 batch loss 3.10533905 epoch total loss 3.20972443\n",
      "Trained batch 1308 batch loss 2.87858796 epoch total loss 3.20947123\n",
      "Trained batch 1309 batch loss 3.25159883 epoch total loss 3.20950317\n",
      "Trained batch 1310 batch loss 3.08543396 epoch total loss 3.20940852\n",
      "Trained batch 1311 batch loss 2.90214109 epoch total loss 3.20917439\n",
      "Trained batch 1312 batch loss 2.79983616 epoch total loss 3.2088623\n",
      "Trained batch 1313 batch loss 2.75827217 epoch total loss 3.20851922\n",
      "Trained batch 1314 batch loss 2.76097417 epoch total loss 3.20817828\n",
      "Trained batch 1315 batch loss 2.96234441 epoch total loss 3.20799136\n",
      "Trained batch 1316 batch loss 3.11128855 epoch total loss 3.20791793\n",
      "Trained batch 1317 batch loss 2.74872208 epoch total loss 3.20756912\n",
      "Trained batch 1318 batch loss 3.23860741 epoch total loss 3.20759296\n",
      "Trained batch 1319 batch loss 3.05339813 epoch total loss 3.2074759\n",
      "Trained batch 1320 batch loss 3.06408858 epoch total loss 3.20736718\n",
      "Trained batch 1321 batch loss 2.97947979 epoch total loss 3.20719457\n",
      "Trained batch 1322 batch loss 2.90306211 epoch total loss 3.20696449\n",
      "Trained batch 1323 batch loss 3.12309361 epoch total loss 3.20690107\n",
      "Trained batch 1324 batch loss 2.89517069 epoch total loss 3.20666552\n",
      "Trained batch 1325 batch loss 3.11272717 epoch total loss 3.20659447\n",
      "Trained batch 1326 batch loss 2.94751573 epoch total loss 3.20639944\n",
      "Trained batch 1327 batch loss 2.87720442 epoch total loss 3.20615149\n",
      "Trained batch 1328 batch loss 2.70676756 epoch total loss 3.20577526\n",
      "Trained batch 1329 batch loss 3.07283926 epoch total loss 3.20567513\n",
      "Trained batch 1330 batch loss 3.10154366 epoch total loss 3.20559692\n",
      "Trained batch 1331 batch loss 3.18254805 epoch total loss 3.20557952\n",
      "Trained batch 1332 batch loss 3.35044098 epoch total loss 3.20568848\n",
      "Trained batch 1333 batch loss 2.96165252 epoch total loss 3.20550513\n",
      "Trained batch 1334 batch loss 2.97103405 epoch total loss 3.20532966\n",
      "Trained batch 1335 batch loss 3.08296728 epoch total loss 3.20523787\n",
      "Trained batch 1336 batch loss 3.08436251 epoch total loss 3.2051475\n",
      "Trained batch 1337 batch loss 3.11041451 epoch total loss 3.20507669\n",
      "Trained batch 1338 batch loss 3.05545807 epoch total loss 3.20496511\n",
      "Trained batch 1339 batch loss 2.99104357 epoch total loss 3.20480537\n",
      "Trained batch 1340 batch loss 2.74230123 epoch total loss 3.20446014\n",
      "Trained batch 1341 batch loss 2.46947789 epoch total loss 3.20391178\n",
      "Trained batch 1342 batch loss 3.00351048 epoch total loss 3.20376253\n",
      "Trained batch 1343 batch loss 2.915663 epoch total loss 3.20354795\n",
      "Trained batch 1344 batch loss 3.35732698 epoch total loss 3.2036624\n",
      "Trained batch 1345 batch loss 2.99989414 epoch total loss 3.203511\n",
      "Trained batch 1346 batch loss 3.16902733 epoch total loss 3.20348525\n",
      "Trained batch 1347 batch loss 2.97765303 epoch total loss 3.2033174\n",
      "Trained batch 1348 batch loss 2.86355448 epoch total loss 3.20306563\n",
      "Trained batch 1349 batch loss 3.0537045 epoch total loss 3.20295477\n",
      "Trained batch 1350 batch loss 3.08488822 epoch total loss 3.20286751\n",
      "Trained batch 1351 batch loss 3.19194365 epoch total loss 3.2028594\n",
      "Trained batch 1352 batch loss 3.07334948 epoch total loss 3.20276356\n",
      "Trained batch 1353 batch loss 2.74544954 epoch total loss 3.20242572\n",
      "Trained batch 1354 batch loss 2.8339963 epoch total loss 3.20215344\n",
      "Trained batch 1355 batch loss 2.92553067 epoch total loss 3.20194912\n",
      "Trained batch 1356 batch loss 2.85607338 epoch total loss 3.20169401\n",
      "Trained batch 1357 batch loss 3.02899027 epoch total loss 3.2015667\n",
      "Trained batch 1358 batch loss 3.1773541 epoch total loss 3.20154858\n",
      "Trained batch 1359 batch loss 2.93678808 epoch total loss 3.20135403\n",
      "Trained batch 1360 batch loss 2.84648371 epoch total loss 3.2010932\n",
      "Trained batch 1361 batch loss 2.87115645 epoch total loss 3.20085073\n",
      "Trained batch 1362 batch loss 2.91025114 epoch total loss 3.20063734\n",
      "Trained batch 1363 batch loss 3.01385617 epoch total loss 3.20050025\n",
      "Trained batch 1364 batch loss 2.88652492 epoch total loss 3.20027018\n",
      "Trained batch 1365 batch loss 2.82312918 epoch total loss 3.19999385\n",
      "Trained batch 1366 batch loss 2.70683312 epoch total loss 3.19963312\n",
      "Trained batch 1367 batch loss 2.85035634 epoch total loss 3.19937778\n",
      "Trained batch 1368 batch loss 3.10596085 epoch total loss 3.19930935\n",
      "Trained batch 1369 batch loss 3.35775876 epoch total loss 3.19942522\n",
      "Trained batch 1370 batch loss 3.20708895 epoch total loss 3.1994307\n",
      "Trained batch 1371 batch loss 2.98462439 epoch total loss 3.1992743\n",
      "Trained batch 1372 batch loss 2.92764759 epoch total loss 3.19907641\n",
      "Trained batch 1373 batch loss 2.9542675 epoch total loss 3.19889808\n",
      "Trained batch 1374 batch loss 3.1338954 epoch total loss 3.19885063\n",
      "Trained batch 1375 batch loss 3.01587415 epoch total loss 3.19871759\n",
      "Trained batch 1376 batch loss 2.5196631 epoch total loss 3.19822407\n",
      "Trained batch 1377 batch loss 2.76273084 epoch total loss 3.19790769\n",
      "Trained batch 1378 batch loss 2.88642931 epoch total loss 3.19768167\n",
      "Trained batch 1379 batch loss 2.96840739 epoch total loss 3.19751525\n",
      "Trained batch 1380 batch loss 2.99278259 epoch total loss 3.19736671\n",
      "Trained batch 1381 batch loss 2.81780601 epoch total loss 3.19709206\n",
      "Trained batch 1382 batch loss 3.03316236 epoch total loss 3.19697332\n",
      "Trained batch 1383 batch loss 2.58884835 epoch total loss 3.19653368\n",
      "Trained batch 1384 batch loss 3.18712568 epoch total loss 3.19652677\n",
      "Trained batch 1385 batch loss 3.03274202 epoch total loss 3.19640851\n",
      "Trained batch 1386 batch loss 3.26725936 epoch total loss 3.19645953\n",
      "Trained batch 1387 batch loss 2.79563904 epoch total loss 3.19617033\n",
      "Trained batch 1388 batch loss 2.98085022 epoch total loss 3.19601536\n",
      "Trained batch 1389 batch loss 2.83975887 epoch total loss 3.19575906\n",
      "Trained batch 1390 batch loss 2.98754072 epoch total loss 3.19560909\n",
      "Trained batch 1391 batch loss 2.93035555 epoch total loss 3.19541812\n",
      "Trained batch 1392 batch loss 2.89556432 epoch total loss 3.19520259\n",
      "Trained batch 1393 batch loss 2.69153142 epoch total loss 3.19484115\n",
      "Trained batch 1394 batch loss 2.66999459 epoch total loss 3.19446445\n",
      "Trained batch 1395 batch loss 2.37788534 epoch total loss 3.19387913\n",
      "Trained batch 1396 batch loss 2.6639185 epoch total loss 3.19349957\n",
      "Trained batch 1397 batch loss 2.87055016 epoch total loss 3.19326854\n",
      "Trained batch 1398 batch loss 2.85617065 epoch total loss 3.19302726\n",
      "Trained batch 1399 batch loss 2.95368433 epoch total loss 3.19285607\n",
      "Trained batch 1400 batch loss 2.63113976 epoch total loss 3.19245505\n",
      "Trained batch 1401 batch loss 2.7526598 epoch total loss 3.19214106\n",
      "Trained batch 1402 batch loss 2.85582399 epoch total loss 3.19190121\n",
      "Trained batch 1403 batch loss 2.98057365 epoch total loss 3.19175053\n",
      "Trained batch 1404 batch loss 2.92075515 epoch total loss 3.19155765\n",
      "Trained batch 1405 batch loss 2.70813155 epoch total loss 3.19121337\n",
      "Trained batch 1406 batch loss 2.98969388 epoch total loss 3.19107\n",
      "Trained batch 1407 batch loss 2.98801708 epoch total loss 3.1909256\n",
      "Trained batch 1408 batch loss 2.90497494 epoch total loss 3.19072247\n",
      "Trained batch 1409 batch loss 2.89903688 epoch total loss 3.19051528\n",
      "Trained batch 1410 batch loss 2.89553523 epoch total loss 3.19030595\n",
      "Trained batch 1411 batch loss 2.76983833 epoch total loss 3.19000816\n",
      "Trained batch 1412 batch loss 2.903337 epoch total loss 3.18980527\n",
      "Trained batch 1413 batch loss 3.02692437 epoch total loss 3.18968987\n",
      "Trained batch 1414 batch loss 2.881464 epoch total loss 3.18947172\n",
      "Trained batch 1415 batch loss 3.1114738 epoch total loss 3.18941665\n",
      "Trained batch 1416 batch loss 3.13729596 epoch total loss 3.18937969\n",
      "Trained batch 1417 batch loss 2.87853956 epoch total loss 3.18916\n",
      "Trained batch 1418 batch loss 2.30005264 epoch total loss 3.18853331\n",
      "Trained batch 1419 batch loss 2.5571475 epoch total loss 3.18808842\n",
      "Trained batch 1420 batch loss 2.18700337 epoch total loss 3.18738341\n",
      "Trained batch 1421 batch loss 2.23620987 epoch total loss 3.18671417\n",
      "Trained batch 1422 batch loss 2.31530237 epoch total loss 3.18610144\n",
      "Trained batch 1423 batch loss 2.87875962 epoch total loss 3.18588567\n",
      "Trained batch 1424 batch loss 3.42104721 epoch total loss 3.18605065\n",
      "Trained batch 1425 batch loss 2.99558973 epoch total loss 3.1859169\n",
      "Trained batch 1426 batch loss 3.41517782 epoch total loss 3.18607759\n",
      "Trained batch 1427 batch loss 3.44501162 epoch total loss 3.18625903\n",
      "Trained batch 1428 batch loss 3.36374617 epoch total loss 3.18638325\n",
      "Trained batch 1429 batch loss 3.34695554 epoch total loss 3.18649578\n",
      "Trained batch 1430 batch loss 3.23618841 epoch total loss 3.18653059\n",
      "Trained batch 1431 batch loss 3.16630697 epoch total loss 3.18651652\n",
      "Trained batch 1432 batch loss 2.91196394 epoch total loss 3.18632507\n",
      "Trained batch 1433 batch loss 3.08620572 epoch total loss 3.18625522\n",
      "Trained batch 1434 batch loss 3.3143 epoch total loss 3.18634462\n",
      "Trained batch 1435 batch loss 2.83402038 epoch total loss 3.18609905\n",
      "Trained batch 1436 batch loss 2.5973115 epoch total loss 3.18568897\n",
      "Trained batch 1437 batch loss 2.70253611 epoch total loss 3.1853528\n",
      "Trained batch 1438 batch loss 2.64114904 epoch total loss 3.18497443\n",
      "Trained batch 1439 batch loss 2.48638272 epoch total loss 3.18448877\n",
      "Trained batch 1440 batch loss 2.65287685 epoch total loss 3.1841197\n",
      "Trained batch 1441 batch loss 2.55257511 epoch total loss 3.18368149\n",
      "Trained batch 1442 batch loss 2.69571495 epoch total loss 3.18334317\n",
      "Trained batch 1443 batch loss 3.18130612 epoch total loss 3.18334174\n",
      "Trained batch 1444 batch loss 3.11911821 epoch total loss 3.18329716\n",
      "Trained batch 1445 batch loss 2.91428089 epoch total loss 3.18311095\n",
      "Trained batch 1446 batch loss 3.07993031 epoch total loss 3.18303967\n",
      "Trained batch 1447 batch loss 3.25828075 epoch total loss 3.18309164\n",
      "Trained batch 1448 batch loss 3.46967602 epoch total loss 3.18328953\n",
      "Trained batch 1449 batch loss 3.42321157 epoch total loss 3.18345523\n",
      "Trained batch 1450 batch loss 3.27751446 epoch total loss 3.18352\n",
      "Trained batch 1451 batch loss 2.78654861 epoch total loss 3.18324637\n",
      "Trained batch 1452 batch loss 3.08449197 epoch total loss 3.18317842\n",
      "Trained batch 1453 batch loss 3.2447257 epoch total loss 3.18322062\n",
      "Trained batch 1454 batch loss 2.88382411 epoch total loss 3.18301487\n",
      "Trained batch 1455 batch loss 3.21408081 epoch total loss 3.18303609\n",
      "Trained batch 1456 batch loss 3.23402023 epoch total loss 3.1830709\n",
      "Trained batch 1457 batch loss 3.01292729 epoch total loss 3.18295407\n",
      "Trained batch 1458 batch loss 3.03698444 epoch total loss 3.18285394\n",
      "Trained batch 1459 batch loss 3.07885814 epoch total loss 3.18278289\n",
      "Trained batch 1460 batch loss 3.49279165 epoch total loss 3.18299508\n",
      "Trained batch 1461 batch loss 3.06675267 epoch total loss 3.18291569\n",
      "Trained batch 1462 batch loss 3.01068091 epoch total loss 3.18279791\n",
      "Trained batch 1463 batch loss 3.17724347 epoch total loss 3.18279409\n",
      "Trained batch 1464 batch loss 3.3742137 epoch total loss 3.18292475\n",
      "Trained batch 1465 batch loss 3.530164 epoch total loss 3.18316174\n",
      "Trained batch 1466 batch loss 3.11944199 epoch total loss 3.18311834\n",
      "Trained batch 1467 batch loss 3.07100177 epoch total loss 3.18304181\n",
      "Trained batch 1468 batch loss 2.9236474 epoch total loss 3.18286538\n",
      "Trained batch 1469 batch loss 3.05955362 epoch total loss 3.18278146\n",
      "Trained batch 1470 batch loss 3.2167182 epoch total loss 3.18280458\n",
      "Trained batch 1471 batch loss 2.78944683 epoch total loss 3.18253708\n",
      "Trained batch 1472 batch loss 2.83641386 epoch total loss 3.182302\n",
      "Trained batch 1473 batch loss 2.56347966 epoch total loss 3.1818819\n",
      "Trained batch 1474 batch loss 2.75754118 epoch total loss 3.18159389\n",
      "Trained batch 1475 batch loss 2.81784248 epoch total loss 3.18134737\n",
      "Trained batch 1476 batch loss 3.04200697 epoch total loss 3.18125296\n",
      "Trained batch 1477 batch loss 2.88048863 epoch total loss 3.18104911\n",
      "Trained batch 1478 batch loss 2.61037111 epoch total loss 3.18066311\n",
      "Trained batch 1479 batch loss 3.02274442 epoch total loss 3.18055654\n",
      "Trained batch 1480 batch loss 2.85449648 epoch total loss 3.18033624\n",
      "Trained batch 1481 batch loss 3.33546066 epoch total loss 3.1804409\n",
      "Trained batch 1482 batch loss 2.81809616 epoch total loss 3.18019629\n",
      "Trained batch 1483 batch loss 3.27176738 epoch total loss 3.18025804\n",
      "Trained batch 1484 batch loss 2.76839805 epoch total loss 3.17998075\n",
      "Trained batch 1485 batch loss 2.63693619 epoch total loss 3.17961478\n",
      "Trained batch 1486 batch loss 2.81848693 epoch total loss 3.17937183\n",
      "Trained batch 1487 batch loss 2.78890896 epoch total loss 3.17910933\n",
      "Trained batch 1488 batch loss 2.78415298 epoch total loss 3.17884374\n",
      "Trained batch 1489 batch loss 2.53214097 epoch total loss 3.17840958\n",
      "Trained batch 1490 batch loss 2.75406885 epoch total loss 3.17812467\n",
      "Trained batch 1491 batch loss 2.75311136 epoch total loss 3.17783952\n",
      "Trained batch 1492 batch loss 2.79624939 epoch total loss 3.17758393\n",
      "Trained batch 1493 batch loss 2.91373658 epoch total loss 3.17740703\n",
      "Trained batch 1494 batch loss 2.98229241 epoch total loss 3.17727637\n",
      "Trained batch 1495 batch loss 3.05172849 epoch total loss 3.17719245\n",
      "Trained batch 1496 batch loss 3.23798037 epoch total loss 3.17723298\n",
      "Trained batch 1497 batch loss 3.16156316 epoch total loss 3.17722249\n",
      "Trained batch 1498 batch loss 2.9875319 epoch total loss 3.17709589\n",
      "Trained batch 1499 batch loss 2.7989831 epoch total loss 3.1768434\n",
      "Trained batch 1500 batch loss 2.82672811 epoch total loss 3.17661\n",
      "Trained batch 1501 batch loss 2.70583916 epoch total loss 3.17629647\n",
      "Trained batch 1502 batch loss 2.70373964 epoch total loss 3.17598176\n",
      "Trained batch 1503 batch loss 2.7883606 epoch total loss 3.17572403\n",
      "Trained batch 1504 batch loss 2.9503212 epoch total loss 3.17557406\n",
      "Trained batch 1505 batch loss 3.18744135 epoch total loss 3.17558193\n",
      "Trained batch 1506 batch loss 3.23363185 epoch total loss 3.17562032\n",
      "Trained batch 1507 batch loss 3.2047627 epoch total loss 3.17563963\n",
      "Trained batch 1508 batch loss 2.97593355 epoch total loss 3.17550731\n",
      "Trained batch 1509 batch loss 2.94523907 epoch total loss 3.17535472\n",
      "Trained batch 1510 batch loss 3.03826499 epoch total loss 3.17526388\n",
      "Trained batch 1511 batch loss 2.82905269 epoch total loss 3.17503476\n",
      "Trained batch 1512 batch loss 2.84033537 epoch total loss 3.17481351\n",
      "Trained batch 1513 batch loss 2.83240867 epoch total loss 3.17458725\n",
      "Trained batch 1514 batch loss 2.73774743 epoch total loss 3.17429876\n",
      "Trained batch 1515 batch loss 2.73364139 epoch total loss 3.17400765\n",
      "Trained batch 1516 batch loss 2.71061134 epoch total loss 3.17370176\n",
      "Trained batch 1517 batch loss 2.82991433 epoch total loss 3.17347527\n",
      "Trained batch 1518 batch loss 2.93829942 epoch total loss 3.17332053\n",
      "Trained batch 1519 batch loss 2.95042896 epoch total loss 3.17317367\n",
      "Trained batch 1520 batch loss 2.96511745 epoch total loss 3.17303681\n",
      "Trained batch 1521 batch loss 3.12645769 epoch total loss 3.1730063\n",
      "Trained batch 1522 batch loss 3.12059021 epoch total loss 3.17297196\n",
      "Trained batch 1523 batch loss 2.75703526 epoch total loss 3.17269874\n",
      "Trained batch 1524 batch loss 3.08942103 epoch total loss 3.1726439\n",
      "Trained batch 1525 batch loss 3.09649301 epoch total loss 3.17259407\n",
      "Trained batch 1526 batch loss 3.13672042 epoch total loss 3.17257071\n",
      "Trained batch 1527 batch loss 3.04599404 epoch total loss 3.17248774\n",
      "Trained batch 1528 batch loss 3.06883383 epoch total loss 3.17241979\n",
      "Trained batch 1529 batch loss 3.07714415 epoch total loss 3.17235756\n",
      "Trained batch 1530 batch loss 3.09659171 epoch total loss 3.17230797\n",
      "Trained batch 1531 batch loss 3.06083846 epoch total loss 3.17223549\n",
      "Trained batch 1532 batch loss 3.00887442 epoch total loss 3.17212868\n",
      "Trained batch 1533 batch loss 2.94823456 epoch total loss 3.17198277\n",
      "Trained batch 1534 batch loss 2.92293763 epoch total loss 3.17182016\n",
      "Trained batch 1535 batch loss 2.85524893 epoch total loss 3.17161417\n",
      "Trained batch 1536 batch loss 2.81524611 epoch total loss 3.17138219\n",
      "Trained batch 1537 batch loss 2.92600584 epoch total loss 3.17122245\n",
      "Trained batch 1538 batch loss 2.61248565 epoch total loss 3.1708591\n",
      "Trained batch 1539 batch loss 3.18185186 epoch total loss 3.17086601\n",
      "Trained batch 1540 batch loss 3.11669016 epoch total loss 3.17083097\n",
      "Trained batch 1541 batch loss 3.10069108 epoch total loss 3.17078543\n",
      "Trained batch 1542 batch loss 3.22515416 epoch total loss 3.17082047\n",
      "Trained batch 1543 batch loss 2.91364956 epoch total loss 3.17065382\n",
      "Trained batch 1544 batch loss 2.84824562 epoch total loss 3.17044497\n",
      "Trained batch 1545 batch loss 2.99990892 epoch total loss 3.17033458\n",
      "Trained batch 1546 batch loss 3.16049027 epoch total loss 3.17032838\n",
      "Trained batch 1547 batch loss 2.96183491 epoch total loss 3.17019367\n",
      "Trained batch 1548 batch loss 3.14653707 epoch total loss 3.17017841\n",
      "Trained batch 1549 batch loss 2.9865706 epoch total loss 3.17005968\n",
      "Trained batch 1550 batch loss 3.27447271 epoch total loss 3.17012691\n",
      "Trained batch 1551 batch loss 2.95532 epoch total loss 3.16998839\n",
      "Trained batch 1552 batch loss 2.93922877 epoch total loss 3.16983986\n",
      "Trained batch 1553 batch loss 2.49594259 epoch total loss 3.16940594\n",
      "Trained batch 1554 batch loss 2.65175629 epoch total loss 3.16907287\n",
      "Trained batch 1555 batch loss 2.71396732 epoch total loss 3.16878\n",
      "Trained batch 1556 batch loss 3.07615805 epoch total loss 3.16872072\n",
      "Trained batch 1557 batch loss 3.07505608 epoch total loss 3.16866064\n",
      "Trained batch 1558 batch loss 3.21709776 epoch total loss 3.16869187\n",
      "Trained batch 1559 batch loss 3.21614909 epoch total loss 3.16872239\n",
      "Trained batch 1560 batch loss 3.04214811 epoch total loss 3.16864109\n",
      "Trained batch 1561 batch loss 3.04196811 epoch total loss 3.16856\n",
      "Trained batch 1562 batch loss 3.29186034 epoch total loss 3.16863894\n",
      "Trained batch 1563 batch loss 3.13729095 epoch total loss 3.16861892\n",
      "Trained batch 1564 batch loss 3.23144197 epoch total loss 3.16865897\n",
      "Trained batch 1565 batch loss 3.03264427 epoch total loss 3.16857219\n",
      "Trained batch 1566 batch loss 2.82728505 epoch total loss 3.16835403\n",
      "Trained batch 1567 batch loss 2.52450871 epoch total loss 3.16794324\n",
      "Trained batch 1568 batch loss 2.56975937 epoch total loss 3.16756177\n",
      "Trained batch 1569 batch loss 2.29671717 epoch total loss 3.16700673\n",
      "Trained batch 1570 batch loss 2.40429878 epoch total loss 3.16652107\n",
      "Trained batch 1571 batch loss 3.06334352 epoch total loss 3.16645551\n",
      "Trained batch 1572 batch loss 3.04181218 epoch total loss 3.16637635\n",
      "Trained batch 1573 batch loss 3.10557652 epoch total loss 3.16633749\n",
      "Trained batch 1574 batch loss 3.2563405 epoch total loss 3.16639471\n",
      "Trained batch 1575 batch loss 2.83223891 epoch total loss 3.16618252\n",
      "Trained batch 1576 batch loss 3.10941553 epoch total loss 3.16614628\n",
      "Trained batch 1577 batch loss 3.0158143 epoch total loss 3.16605091\n",
      "Trained batch 1578 batch loss 2.86642504 epoch total loss 3.16586089\n",
      "Trained batch 1579 batch loss 2.94750357 epoch total loss 3.16572237\n",
      "Trained batch 1580 batch loss 2.87582636 epoch total loss 3.16553903\n",
      "Trained batch 1581 batch loss 3.05321026 epoch total loss 3.16546798\n",
      "Trained batch 1582 batch loss 2.89569569 epoch total loss 3.16529751\n",
      "Trained batch 1583 batch loss 3.09830284 epoch total loss 3.16525507\n",
      "Trained batch 1584 batch loss 2.93929911 epoch total loss 3.1651125\n",
      "Trained batch 1585 batch loss 2.94817686 epoch total loss 3.16497564\n",
      "Trained batch 1586 batch loss 3.0386734 epoch total loss 3.16489601\n",
      "Trained batch 1587 batch loss 2.47757626 epoch total loss 3.1644628\n",
      "Trained batch 1588 batch loss 2.83044052 epoch total loss 3.16425252\n",
      "Trained batch 1589 batch loss 2.85570836 epoch total loss 3.16405821\n",
      "Trained batch 1590 batch loss 2.93081379 epoch total loss 3.16391134\n",
      "Trained batch 1591 batch loss 2.78337717 epoch total loss 3.16367221\n",
      "Trained batch 1592 batch loss 2.91938782 epoch total loss 3.16351867\n",
      "Trained batch 1593 batch loss 2.95745254 epoch total loss 3.16338944\n",
      "Trained batch 1594 batch loss 3.01821375 epoch total loss 3.16329813\n",
      "Trained batch 1595 batch loss 2.93712759 epoch total loss 3.16315627\n",
      "Trained batch 1596 batch loss 2.99673653 epoch total loss 3.16305208\n",
      "Trained batch 1597 batch loss 2.87698793 epoch total loss 3.16287279\n",
      "Trained batch 1598 batch loss 3.01334906 epoch total loss 3.16277909\n",
      "Trained batch 1599 batch loss 2.91216969 epoch total loss 3.16262245\n",
      "Trained batch 1600 batch loss 3.00667238 epoch total loss 3.16252494\n",
      "Trained batch 1601 batch loss 3.05196524 epoch total loss 3.1624558\n",
      "Trained batch 1602 batch loss 2.95805025 epoch total loss 3.16232824\n",
      "Trained batch 1603 batch loss 2.96774578 epoch total loss 3.16220689\n",
      "Trained batch 1604 batch loss 3.0310998 epoch total loss 3.16212511\n",
      "Trained batch 1605 batch loss 2.93470216 epoch total loss 3.16198349\n",
      "Trained batch 1606 batch loss 3.32409167 epoch total loss 3.16208434\n",
      "Trained batch 1607 batch loss 3.1598978 epoch total loss 3.16208291\n",
      "Trained batch 1608 batch loss 2.96627378 epoch total loss 3.16196108\n",
      "Trained batch 1609 batch loss 2.77129602 epoch total loss 3.16171861\n",
      "Trained batch 1610 batch loss 3.03687048 epoch total loss 3.16164112\n",
      "Trained batch 1611 batch loss 3.13625956 epoch total loss 3.16162539\n",
      "Trained batch 1612 batch loss 2.61845541 epoch total loss 3.1612885\n",
      "Trained batch 1613 batch loss 2.79791331 epoch total loss 3.16106319\n",
      "Trained batch 1614 batch loss 2.61928082 epoch total loss 3.1607275\n",
      "Trained batch 1615 batch loss 2.56872129 epoch total loss 3.16036105\n",
      "Trained batch 1616 batch loss 3.00161576 epoch total loss 3.16026258\n",
      "Trained batch 1617 batch loss 3.02780771 epoch total loss 3.16018081\n",
      "Trained batch 1618 batch loss 2.93208742 epoch total loss 3.16003966\n",
      "Trained batch 1619 batch loss 2.57241774 epoch total loss 3.15967679\n",
      "Trained batch 1620 batch loss 2.66587448 epoch total loss 3.15937209\n",
      "Trained batch 1621 batch loss 2.53179502 epoch total loss 3.1589849\n",
      "Trained batch 1622 batch loss 2.54707766 epoch total loss 3.15860748\n",
      "Trained batch 1623 batch loss 2.41129541 epoch total loss 3.15814686\n",
      "Trained batch 1624 batch loss 2.78617358 epoch total loss 3.15791774\n",
      "Trained batch 1625 batch loss 2.64625788 epoch total loss 3.15760303\n",
      "Trained batch 1626 batch loss 2.71443462 epoch total loss 3.15733051\n",
      "Trained batch 1627 batch loss 2.81935167 epoch total loss 3.15712261\n",
      "Trained batch 1628 batch loss 2.82579279 epoch total loss 3.15691924\n",
      "Trained batch 1629 batch loss 2.58158517 epoch total loss 3.1565659\n",
      "Trained batch 1630 batch loss 2.61473227 epoch total loss 3.15623355\n",
      "Trained batch 1631 batch loss 2.8145256 epoch total loss 3.15602398\n",
      "Trained batch 1632 batch loss 2.8742981 epoch total loss 3.15585136\n",
      "Trained batch 1633 batch loss 2.97412157 epoch total loss 3.15574026\n",
      "Trained batch 1634 batch loss 2.71615076 epoch total loss 3.15547132\n",
      "Trained batch 1635 batch loss 2.72615337 epoch total loss 3.15520859\n",
      "Trained batch 1636 batch loss 3.01734829 epoch total loss 3.15512443\n",
      "Trained batch 1637 batch loss 3.07180452 epoch total loss 3.15507364\n",
      "Trained batch 1638 batch loss 2.63250232 epoch total loss 3.1547544\n",
      "Trained batch 1639 batch loss 2.82960558 epoch total loss 3.15455604\n",
      "Trained batch 1640 batch loss 2.40324306 epoch total loss 3.15409803\n",
      "Trained batch 1641 batch loss 2.98776674 epoch total loss 3.15399671\n",
      "Trained batch 1642 batch loss 2.87559509 epoch total loss 3.15382695\n",
      "Trained batch 1643 batch loss 3.12354398 epoch total loss 3.15380859\n",
      "Trained batch 1644 batch loss 2.9803791 epoch total loss 3.15370321\n",
      "Trained batch 1645 batch loss 3.15476322 epoch total loss 3.15370393\n",
      "Trained batch 1646 batch loss 3.13365293 epoch total loss 3.15369177\n",
      "Trained batch 1647 batch loss 3.12290263 epoch total loss 3.15367317\n",
      "Trained batch 1648 batch loss 3.09316254 epoch total loss 3.15363646\n",
      "Trained batch 1649 batch loss 2.94323301 epoch total loss 3.1535089\n",
      "Trained batch 1650 batch loss 2.79193187 epoch total loss 3.15328979\n",
      "Trained batch 1651 batch loss 2.92922974 epoch total loss 3.15315413\n",
      "Trained batch 1652 batch loss 2.83902931 epoch total loss 3.15296388\n",
      "Trained batch 1653 batch loss 2.9218297 epoch total loss 3.15282416\n",
      "Trained batch 1654 batch loss 2.98227501 epoch total loss 3.15272093\n",
      "Trained batch 1655 batch loss 2.75233221 epoch total loss 3.15247917\n",
      "Trained batch 1656 batch loss 2.62819624 epoch total loss 3.15216279\n",
      "Trained batch 1657 batch loss 2.95239973 epoch total loss 3.15204239\n",
      "Trained batch 1658 batch loss 2.75158691 epoch total loss 3.15180063\n",
      "Trained batch 1659 batch loss 2.64533472 epoch total loss 3.15149546\n",
      "Trained batch 1660 batch loss 3.06696725 epoch total loss 3.15144444\n",
      "Trained batch 1661 batch loss 3.05971766 epoch total loss 3.15138912\n",
      "Trained batch 1662 batch loss 2.83392572 epoch total loss 3.15119815\n",
      "Trained batch 1663 batch loss 2.70261717 epoch total loss 3.1509285\n",
      "Trained batch 1664 batch loss 2.53601408 epoch total loss 3.15055895\n",
      "Trained batch 1665 batch loss 3.02463984 epoch total loss 3.15048337\n",
      "Trained batch 1666 batch loss 3.04316783 epoch total loss 3.15041876\n",
      "Trained batch 1667 batch loss 2.6860168 epoch total loss 3.15014029\n",
      "Trained batch 1668 batch loss 2.5653553 epoch total loss 3.14978957\n",
      "Trained batch 1669 batch loss 2.64526129 epoch total loss 3.14948726\n",
      "Trained batch 1670 batch loss 2.74842739 epoch total loss 3.14924717\n",
      "Trained batch 1671 batch loss 2.69759583 epoch total loss 3.1489768\n",
      "Trained batch 1672 batch loss 2.73390484 epoch total loss 3.14872861\n",
      "Trained batch 1673 batch loss 2.55143619 epoch total loss 3.14837146\n",
      "Trained batch 1674 batch loss 2.6010294 epoch total loss 3.14804459\n",
      "Trained batch 1675 batch loss 2.53577757 epoch total loss 3.14767909\n",
      "Trained batch 1676 batch loss 2.98885703 epoch total loss 3.1475842\n",
      "Trained batch 1677 batch loss 2.94114494 epoch total loss 3.14746094\n",
      "Trained batch 1678 batch loss 3.04592562 epoch total loss 3.14740038\n",
      "Trained batch 1679 batch loss 2.81009316 epoch total loss 3.14719939\n",
      "Trained batch 1680 batch loss 2.9608202 epoch total loss 3.14708853\n",
      "Trained batch 1681 batch loss 2.85627341 epoch total loss 3.14691567\n",
      "Trained batch 1682 batch loss 2.93574929 epoch total loss 3.14679\n",
      "Trained batch 1683 batch loss 3.11787128 epoch total loss 3.14677286\n",
      "Trained batch 1684 batch loss 3.08382297 epoch total loss 3.14673543\n",
      "Trained batch 1685 batch loss 3.06524134 epoch total loss 3.14668727\n",
      "Trained batch 1686 batch loss 3.01659322 epoch total loss 3.14661\n",
      "Trained batch 1687 batch loss 2.99973178 epoch total loss 3.14652276\n",
      "Trained batch 1688 batch loss 3.23342776 epoch total loss 3.14657426\n",
      "Trained batch 1689 batch loss 3.05016 epoch total loss 3.14651728\n",
      "Trained batch 1690 batch loss 2.97970271 epoch total loss 3.14641857\n",
      "Trained batch 1691 batch loss 3.06284428 epoch total loss 3.14636922\n",
      "Trained batch 1692 batch loss 2.84218335 epoch total loss 3.14618945\n",
      "Trained batch 1693 batch loss 2.4064033 epoch total loss 3.14575243\n",
      "Trained batch 1694 batch loss 2.72018814 epoch total loss 3.14550114\n",
      "Trained batch 1695 batch loss 3.02044773 epoch total loss 3.14542747\n",
      "Trained batch 1696 batch loss 3.2481935 epoch total loss 3.14548802\n",
      "Trained batch 1697 batch loss 3.45619583 epoch total loss 3.14567089\n",
      "Trained batch 1698 batch loss 3.30928612 epoch total loss 3.14576721\n",
      "Trained batch 1699 batch loss 3.29067421 epoch total loss 3.14585233\n",
      "Trained batch 1700 batch loss 3.1335094 epoch total loss 3.14584494\n",
      "Trained batch 1701 batch loss 2.97592974 epoch total loss 3.14574528\n",
      "Trained batch 1702 batch loss 3.23454952 epoch total loss 3.14579725\n",
      "Trained batch 1703 batch loss 3.3363862 epoch total loss 3.14590931\n",
      "Trained batch 1704 batch loss 3.26531553 epoch total loss 3.14597917\n",
      "Trained batch 1705 batch loss 3.06537151 epoch total loss 3.14593196\n",
      "Trained batch 1706 batch loss 3.13090062 epoch total loss 3.14592314\n",
      "Trained batch 1707 batch loss 2.90142 epoch total loss 3.14577985\n",
      "Trained batch 1708 batch loss 2.61404443 epoch total loss 3.14546871\n",
      "Trained batch 1709 batch loss 2.75223541 epoch total loss 3.14523864\n",
      "Trained batch 1710 batch loss 2.99017215 epoch total loss 3.14514804\n",
      "Trained batch 1711 batch loss 2.8035078 epoch total loss 3.14494848\n",
      "Trained batch 1712 batch loss 3.03796315 epoch total loss 3.14488602\n",
      "Trained batch 1713 batch loss 3.27315664 epoch total loss 3.14496088\n",
      "Trained batch 1714 batch loss 2.93808341 epoch total loss 3.14484\n",
      "Trained batch 1715 batch loss 2.92705488 epoch total loss 3.14471316\n",
      "Trained batch 1716 batch loss 2.8666029 epoch total loss 3.14455128\n",
      "Trained batch 1717 batch loss 3.13364768 epoch total loss 3.14454484\n",
      "Trained batch 1718 batch loss 3.30344486 epoch total loss 3.14463735\n",
      "Trained batch 1719 batch loss 3.15500021 epoch total loss 3.14464307\n",
      "Trained batch 1720 batch loss 2.91615653 epoch total loss 3.14451027\n",
      "Trained batch 1721 batch loss 3.24204707 epoch total loss 3.14456701\n",
      "Trained batch 1722 batch loss 2.77670169 epoch total loss 3.14435339\n",
      "Trained batch 1723 batch loss 3.26959324 epoch total loss 3.14442611\n",
      "Trained batch 1724 batch loss 3.06796479 epoch total loss 3.14438176\n",
      "Trained batch 1725 batch loss 3.13786077 epoch total loss 3.14437795\n",
      "Trained batch 1726 batch loss 3.08940864 epoch total loss 3.144346\n",
      "Trained batch 1727 batch loss 3.38410139 epoch total loss 3.144485\n",
      "Trained batch 1728 batch loss 3.28963757 epoch total loss 3.14456892\n",
      "Trained batch 1729 batch loss 3.03538179 epoch total loss 3.1445055\n",
      "Trained batch 1730 batch loss 2.9774735 epoch total loss 3.14440894\n",
      "Trained batch 1731 batch loss 3.17998052 epoch total loss 3.14442968\n",
      "Trained batch 1732 batch loss 3.14553261 epoch total loss 3.1444304\n",
      "Trained batch 1733 batch loss 3.13639426 epoch total loss 3.14442563\n",
      "Trained batch 1734 batch loss 3.07066202 epoch total loss 3.14438319\n",
      "Trained batch 1735 batch loss 2.9855895 epoch total loss 3.1442914\n",
      "Trained batch 1736 batch loss 3.03518271 epoch total loss 3.1442287\n",
      "Trained batch 1737 batch loss 2.93987274 epoch total loss 3.14411092\n",
      "Trained batch 1738 batch loss 3.18010473 epoch total loss 3.14413166\n",
      "Trained batch 1739 batch loss 2.96764874 epoch total loss 3.14403033\n",
      "Trained batch 1740 batch loss 3.16070652 epoch total loss 3.14403987\n",
      "Trained batch 1741 batch loss 3.1326859 epoch total loss 3.14403343\n",
      "Trained batch 1742 batch loss 3.04129839 epoch total loss 3.14397454\n",
      "Trained batch 1743 batch loss 3.05011106 epoch total loss 3.1439209\n",
      "Trained batch 1744 batch loss 3.02422905 epoch total loss 3.14385223\n",
      "Trained batch 1745 batch loss 2.93439245 epoch total loss 3.14373231\n",
      "Trained batch 1746 batch loss 2.8510406 epoch total loss 3.1435647\n",
      "Trained batch 1747 batch loss 2.91132355 epoch total loss 3.14343166\n",
      "Trained batch 1748 batch loss 3.07189226 epoch total loss 3.14339066\n",
      "Trained batch 1749 batch loss 3.01858282 epoch total loss 3.14331937\n",
      "Trained batch 1750 batch loss 3.07980752 epoch total loss 3.14328289\n",
      "Trained batch 1751 batch loss 3.21151948 epoch total loss 3.14332175\n",
      "Trained batch 1752 batch loss 2.67797565 epoch total loss 3.14305615\n",
      "Trained batch 1753 batch loss 3.03247 epoch total loss 3.14299297\n",
      "Trained batch 1754 batch loss 2.93535495 epoch total loss 3.14287472\n",
      "Trained batch 1755 batch loss 3.32183981 epoch total loss 3.14297652\n",
      "Trained batch 1756 batch loss 3.08973098 epoch total loss 3.14294624\n",
      "Trained batch 1757 batch loss 3.41605926 epoch total loss 3.14310169\n",
      "Trained batch 1758 batch loss 3.00316548 epoch total loss 3.14302206\n",
      "Trained batch 1759 batch loss 2.58880496 epoch total loss 3.14270687\n",
      "Trained batch 1760 batch loss 3.09134483 epoch total loss 3.14267778\n",
      "Trained batch 1761 batch loss 3.05869436 epoch total loss 3.14262986\n",
      "Trained batch 1762 batch loss 3.07366371 epoch total loss 3.14259076\n",
      "Trained batch 1763 batch loss 2.93084908 epoch total loss 3.1424706\n",
      "Trained batch 1764 batch loss 2.88776636 epoch total loss 3.14232612\n",
      "Trained batch 1765 batch loss 3.10896921 epoch total loss 3.14230728\n",
      "Trained batch 1766 batch loss 3.10000443 epoch total loss 3.14228344\n",
      "Trained batch 1767 batch loss 3.17059922 epoch total loss 3.14229941\n",
      "Trained batch 1768 batch loss 3.30364347 epoch total loss 3.14239049\n",
      "Trained batch 1769 batch loss 3.23975229 epoch total loss 3.14244556\n",
      "Trained batch 1770 batch loss 2.8222034 epoch total loss 3.14226484\n",
      "Trained batch 1771 batch loss 2.96100879 epoch total loss 3.14216232\n",
      "Trained batch 1772 batch loss 2.91264415 epoch total loss 3.14203286\n",
      "Trained batch 1773 batch loss 3.26884508 epoch total loss 3.14210439\n",
      "Trained batch 1774 batch loss 2.93077421 epoch total loss 3.14198518\n",
      "Trained batch 1775 batch loss 2.96982217 epoch total loss 3.14188814\n",
      "Trained batch 1776 batch loss 2.99823785 epoch total loss 3.14180732\n",
      "Trained batch 1777 batch loss 3.25419879 epoch total loss 3.1418705\n",
      "Trained batch 1778 batch loss 3.20096183 epoch total loss 3.14190388\n",
      "Trained batch 1779 batch loss 2.96221304 epoch total loss 3.14180303\n",
      "Trained batch 1780 batch loss 3.22128248 epoch total loss 3.14184761\n",
      "Trained batch 1781 batch loss 3.0259347 epoch total loss 3.14178252\n",
      "Trained batch 1782 batch loss 2.81802964 epoch total loss 3.14160085\n",
      "Trained batch 1783 batch loss 2.84681892 epoch total loss 3.14143538\n",
      "Trained batch 1784 batch loss 2.80785751 epoch total loss 3.14124823\n",
      "Trained batch 1785 batch loss 2.80695748 epoch total loss 3.14106107\n",
      "Trained batch 1786 batch loss 2.63845253 epoch total loss 3.14077973\n",
      "Trained batch 1787 batch loss 3.00452518 epoch total loss 3.14070344\n",
      "Trained batch 1788 batch loss 2.79605603 epoch total loss 3.14051056\n",
      "Trained batch 1789 batch loss 2.89426517 epoch total loss 3.14037275\n",
      "Trained batch 1790 batch loss 3.03433895 epoch total loss 3.14031339\n",
      "Trained batch 1791 batch loss 2.82130313 epoch total loss 3.14013529\n",
      "Trained batch 1792 batch loss 2.99692082 epoch total loss 3.14005542\n",
      "Trained batch 1793 batch loss 2.78670549 epoch total loss 3.13985848\n",
      "Trained batch 1794 batch loss 2.69490719 epoch total loss 3.13961029\n",
      "Trained batch 1795 batch loss 2.95940804 epoch total loss 3.13951\n",
      "Trained batch 1796 batch loss 2.72324371 epoch total loss 3.13927817\n",
      "Trained batch 1797 batch loss 2.97269869 epoch total loss 3.13918543\n",
      "Trained batch 1798 batch loss 2.97599506 epoch total loss 3.13909459\n",
      "Trained batch 1799 batch loss 2.94280434 epoch total loss 3.13898563\n",
      "Trained batch 1800 batch loss 3.08965015 epoch total loss 3.13895822\n",
      "Trained batch 1801 batch loss 2.69509912 epoch total loss 3.13871193\n",
      "Trained batch 1802 batch loss 3.00487614 epoch total loss 3.13863778\n",
      "Trained batch 1803 batch loss 3.02876234 epoch total loss 3.13857675\n",
      "Trained batch 1804 batch loss 3.08598542 epoch total loss 3.13854766\n",
      "Trained batch 1805 batch loss 2.97779131 epoch total loss 3.13845873\n",
      "Trained batch 1806 batch loss 2.58416915 epoch total loss 3.13815165\n",
      "Trained batch 1807 batch loss 2.55289936 epoch total loss 3.13782763\n",
      "Trained batch 1808 batch loss 2.80330491 epoch total loss 3.13764262\n",
      "Trained batch 1809 batch loss 2.79421663 epoch total loss 3.13745284\n",
      "Trained batch 1810 batch loss 2.85526752 epoch total loss 3.13729715\n",
      "Trained batch 1811 batch loss 2.93837404 epoch total loss 3.13718724\n",
      "Trained batch 1812 batch loss 3.11181259 epoch total loss 3.13717341\n",
      "Trained batch 1813 batch loss 3.2990253 epoch total loss 3.13726258\n",
      "Trained batch 1814 batch loss 3.34764266 epoch total loss 3.13737845\n",
      "Trained batch 1815 batch loss 3.29550338 epoch total loss 3.13746548\n",
      "Trained batch 1816 batch loss 2.98869848 epoch total loss 3.1373837\n",
      "Trained batch 1817 batch loss 3.03164124 epoch total loss 3.13732553\n",
      "Trained batch 1818 batch loss 2.98652601 epoch total loss 3.13724256\n",
      "Trained batch 1819 batch loss 2.79312468 epoch total loss 3.13705325\n",
      "Trained batch 1820 batch loss 2.81888533 epoch total loss 3.13687849\n",
      "Trained batch 1821 batch loss 2.61620116 epoch total loss 3.13659239\n",
      "Trained batch 1822 batch loss 2.83232784 epoch total loss 3.1364255\n",
      "Trained batch 1823 batch loss 2.71408749 epoch total loss 3.13619375\n",
      "Trained batch 1824 batch loss 3.10316253 epoch total loss 3.13617563\n",
      "Trained batch 1825 batch loss 3.15161204 epoch total loss 3.13618422\n",
      "Trained batch 1826 batch loss 2.21432018 epoch total loss 3.13567924\n",
      "Trained batch 1827 batch loss 3.01041365 epoch total loss 3.13561058\n",
      "Trained batch 1828 batch loss 3.001441 epoch total loss 3.13553739\n",
      "Trained batch 1829 batch loss 2.97276974 epoch total loss 3.13544822\n",
      "Trained batch 1830 batch loss 2.92613554 epoch total loss 3.13533401\n",
      "Trained batch 1831 batch loss 3.17306709 epoch total loss 3.13535452\n",
      "Trained batch 1832 batch loss 2.50929785 epoch total loss 3.13501263\n",
      "Trained batch 1833 batch loss 2.55955362 epoch total loss 3.13469887\n",
      "Trained batch 1834 batch loss 2.39764261 epoch total loss 3.13429689\n",
      "Trained batch 1835 batch loss 3.04273391 epoch total loss 3.13424706\n",
      "Trained batch 1836 batch loss 3.0034914 epoch total loss 3.13417578\n",
      "Trained batch 1837 batch loss 3.1862278 epoch total loss 3.13420391\n",
      "Trained batch 1838 batch loss 3.19084597 epoch total loss 3.13423491\n",
      "Trained batch 1839 batch loss 3.23501897 epoch total loss 3.1342895\n",
      "Trained batch 1840 batch loss 2.82718801 epoch total loss 3.13412261\n",
      "Trained batch 1841 batch loss 2.89142179 epoch total loss 3.133991\n",
      "Trained batch 1842 batch loss 2.66513634 epoch total loss 3.13373637\n",
      "Trained batch 1843 batch loss 2.82899714 epoch total loss 3.13357091\n",
      "Trained batch 1844 batch loss 2.67187881 epoch total loss 3.13332057\n",
      "Trained batch 1845 batch loss 2.75173426 epoch total loss 3.13311386\n",
      "Trained batch 1846 batch loss 2.73696756 epoch total loss 3.13289928\n",
      "Trained batch 1847 batch loss 2.63929844 epoch total loss 3.13263202\n",
      "Trained batch 1848 batch loss 2.67912102 epoch total loss 3.13238668\n",
      "Trained batch 1849 batch loss 2.58394289 epoch total loss 3.13209\n",
      "Trained batch 1850 batch loss 2.63040352 epoch total loss 3.13181877\n",
      "Trained batch 1851 batch loss 2.73411942 epoch total loss 3.13160372\n",
      "Trained batch 1852 batch loss 2.64827657 epoch total loss 3.13134289\n",
      "Trained batch 1853 batch loss 2.85050488 epoch total loss 3.13119149\n",
      "Trained batch 1854 batch loss 2.68642807 epoch total loss 3.13095164\n",
      "Trained batch 1855 batch loss 2.88671184 epoch total loss 3.1308198\n",
      "Trained batch 1856 batch loss 3.01089907 epoch total loss 3.13075519\n",
      "Trained batch 1857 batch loss 3.06997156 epoch total loss 3.13072228\n",
      "Trained batch 1858 batch loss 3.05847955 epoch total loss 3.13068366\n",
      "Trained batch 1859 batch loss 2.93500662 epoch total loss 3.13057828\n",
      "Trained batch 1860 batch loss 2.95969892 epoch total loss 3.13048625\n",
      "Trained batch 1861 batch loss 3.19779706 epoch total loss 3.13052249\n",
      "Trained batch 1862 batch loss 2.84239531 epoch total loss 3.13036776\n",
      "Trained batch 1863 batch loss 3.0087626 epoch total loss 3.13030243\n",
      "Trained batch 1864 batch loss 2.78893 epoch total loss 3.13011932\n",
      "Trained batch 1865 batch loss 2.82955718 epoch total loss 3.12995815\n",
      "Trained batch 1866 batch loss 3.09184504 epoch total loss 3.12993765\n",
      "Trained batch 1867 batch loss 2.91543436 epoch total loss 3.12982297\n",
      "Trained batch 1868 batch loss 3.06924677 epoch total loss 3.12979054\n",
      "Trained batch 1869 batch loss 3.12515283 epoch total loss 3.12978792\n",
      "Trained batch 1870 batch loss 3.04515719 epoch total loss 3.12974262\n",
      "Trained batch 1871 batch loss 2.86054468 epoch total loss 3.12959862\n",
      "Trained batch 1872 batch loss 2.88913655 epoch total loss 3.12947\n",
      "Trained batch 1873 batch loss 2.77671957 epoch total loss 3.129282\n",
      "Trained batch 1874 batch loss 2.89292431 epoch total loss 3.12915587\n",
      "Trained batch 1875 batch loss 2.84311485 epoch total loss 3.12900329\n",
      "Trained batch 1876 batch loss 2.72975636 epoch total loss 3.12879062\n",
      "Trained batch 1877 batch loss 2.52806664 epoch total loss 3.12847042\n",
      "Trained batch 1878 batch loss 3.08915305 epoch total loss 3.12844968\n",
      "Trained batch 1879 batch loss 3.15760565 epoch total loss 3.12846518\n",
      "Trained batch 1880 batch loss 3.00562716 epoch total loss 3.1284\n",
      "Trained batch 1881 batch loss 3.02277279 epoch total loss 3.12834406\n",
      "Trained batch 1882 batch loss 3.14236212 epoch total loss 3.12835145\n",
      "Trained batch 1883 batch loss 3.52102184 epoch total loss 3.12856\n",
      "Trained batch 1884 batch loss 3.24150181 epoch total loss 3.12862015\n",
      "Trained batch 1885 batch loss 3.39212799 epoch total loss 3.12875986\n",
      "Trained batch 1886 batch loss 3.16483021 epoch total loss 3.12877917\n",
      "Trained batch 1887 batch loss 3.11204195 epoch total loss 3.12877\n",
      "Trained batch 1888 batch loss 3.2220192 epoch total loss 3.1288197\n",
      "Trained batch 1889 batch loss 2.97611785 epoch total loss 3.12873864\n",
      "Trained batch 1890 batch loss 3.07877517 epoch total loss 3.12871218\n",
      "Trained batch 1891 batch loss 3.06164408 epoch total loss 3.12867665\n",
      "Trained batch 1892 batch loss 2.79129958 epoch total loss 3.12849855\n",
      "Trained batch 1893 batch loss 2.82933474 epoch total loss 3.12834024\n",
      "Trained batch 1894 batch loss 2.58024263 epoch total loss 3.1280508\n",
      "Trained batch 1895 batch loss 2.92548943 epoch total loss 3.12794375\n",
      "Trained batch 1896 batch loss 2.57448578 epoch total loss 3.12765217\n",
      "Trained batch 1897 batch loss 3.44061136 epoch total loss 3.12781692\n",
      "Trained batch 1898 batch loss 3.11681604 epoch total loss 3.12781119\n",
      "Trained batch 1899 batch loss 2.9986167 epoch total loss 3.12774301\n",
      "Trained batch 1900 batch loss 3.19460297 epoch total loss 3.12777829\n",
      "Trained batch 1901 batch loss 3.16260338 epoch total loss 3.12779665\n",
      "Trained batch 1902 batch loss 3.0765295 epoch total loss 3.12776971\n",
      "Trained batch 1903 batch loss 2.87035084 epoch total loss 3.12763429\n",
      "Trained batch 1904 batch loss 2.90460634 epoch total loss 3.12751722\n",
      "Trained batch 1905 batch loss 2.83321571 epoch total loss 3.12736273\n",
      "Trained batch 1906 batch loss 3.08300352 epoch total loss 3.12733936\n",
      "Trained batch 1907 batch loss 2.98279333 epoch total loss 3.12726378\n",
      "Trained batch 1908 batch loss 3.00453544 epoch total loss 3.12719941\n",
      "Trained batch 1909 batch loss 3.00418425 epoch total loss 3.12713504\n",
      "Trained batch 1910 batch loss 2.8462708 epoch total loss 3.12698793\n",
      "Trained batch 1911 batch loss 2.5549531 epoch total loss 3.12668872\n",
      "Trained batch 1912 batch loss 2.58774137 epoch total loss 3.12640691\n",
      "Trained batch 1913 batch loss 3.1236186 epoch total loss 3.12640548\n",
      "Trained batch 1914 batch loss 3.37348652 epoch total loss 3.12653446\n",
      "Trained batch 1915 batch loss 2.75854516 epoch total loss 3.12634254\n",
      "Trained batch 1916 batch loss 2.94299984 epoch total loss 3.12624669\n",
      "Trained batch 1917 batch loss 3.00220346 epoch total loss 3.12618208\n",
      "Trained batch 1918 batch loss 3.30837369 epoch total loss 3.12627721\n",
      "Trained batch 1919 batch loss 3.06390905 epoch total loss 3.12624478\n",
      "Trained batch 1920 batch loss 3.31604815 epoch total loss 3.12634349\n",
      "Trained batch 1921 batch loss 3.20866609 epoch total loss 3.1263864\n",
      "Trained batch 1922 batch loss 2.94252205 epoch total loss 3.12629056\n",
      "Trained batch 1923 batch loss 2.82744026 epoch total loss 3.12613535\n",
      "Trained batch 1924 batch loss 2.84988141 epoch total loss 3.12599182\n",
      "Trained batch 1925 batch loss 2.98846197 epoch total loss 3.1259203\n",
      "Trained batch 1926 batch loss 3.00025 epoch total loss 3.12585521\n",
      "Trained batch 1927 batch loss 2.8105979 epoch total loss 3.12569141\n",
      "Trained batch 1928 batch loss 2.52766562 epoch total loss 3.12538147\n",
      "Trained batch 1929 batch loss 2.98906708 epoch total loss 3.1253109\n",
      "Trained batch 1930 batch loss 2.96621 epoch total loss 3.1252284\n",
      "Trained batch 1931 batch loss 3.0488596 epoch total loss 3.12518883\n",
      "Trained batch 1932 batch loss 3.15392637 epoch total loss 3.12520361\n",
      "Trained batch 1933 batch loss 3.1679287 epoch total loss 3.12522578\n",
      "Trained batch 1934 batch loss 2.98134756 epoch total loss 3.1251514\n",
      "Trained batch 1935 batch loss 3.05777097 epoch total loss 3.12511659\n",
      "Trained batch 1936 batch loss 3.07495952 epoch total loss 3.12509084\n",
      "Trained batch 1937 batch loss 3.21313882 epoch total loss 3.12513638\n",
      "Trained batch 1938 batch loss 3.28087854 epoch total loss 3.12521672\n",
      "Trained batch 1939 batch loss 3.25694 epoch total loss 3.12528467\n",
      "Trained batch 1940 batch loss 3.0563364 epoch total loss 3.12524891\n",
      "Trained batch 1941 batch loss 3.13666129 epoch total loss 3.12525487\n",
      "Trained batch 1942 batch loss 3.28467822 epoch total loss 3.12533689\n",
      "Trained batch 1943 batch loss 3.18078 epoch total loss 3.1253655\n",
      "Trained batch 1944 batch loss 3.11680365 epoch total loss 3.12536097\n",
      "Trained batch 1945 batch loss 3.05406046 epoch total loss 3.12532425\n",
      "Trained batch 1946 batch loss 3.12135386 epoch total loss 3.12532234\n",
      "Trained batch 1947 batch loss 3.16439557 epoch total loss 3.12534261\n",
      "Trained batch 1948 batch loss 2.98242569 epoch total loss 3.12526917\n",
      "Trained batch 1949 batch loss 3.08257866 epoch total loss 3.12524724\n",
      "Trained batch 1950 batch loss 2.89426446 epoch total loss 3.12512875\n",
      "Trained batch 1951 batch loss 2.95437241 epoch total loss 3.12504125\n",
      "Trained batch 1952 batch loss 3.24008083 epoch total loss 3.12510037\n",
      "Trained batch 1953 batch loss 3.12377405 epoch total loss 3.12509942\n",
      "Trained batch 1954 batch loss 3.08729887 epoch total loss 3.12508\n",
      "Trained batch 1955 batch loss 2.94788909 epoch total loss 3.12498951\n",
      "Trained batch 1956 batch loss 2.8932333 epoch total loss 3.12487102\n",
      "Trained batch 1957 batch loss 2.937464 epoch total loss 3.12477517\n",
      "Trained batch 1958 batch loss 3.04354954 epoch total loss 3.12473369\n",
      "Trained batch 1959 batch loss 2.94443774 epoch total loss 3.12464166\n",
      "Trained batch 1960 batch loss 2.96416831 epoch total loss 3.12455988\n",
      "Trained batch 1961 batch loss 2.95344353 epoch total loss 3.12447262\n",
      "Trained batch 1962 batch loss 3.07861328 epoch total loss 3.12444925\n",
      "Trained batch 1963 batch loss 3.33456898 epoch total loss 3.1245563\n",
      "Trained batch 1964 batch loss 3.15640402 epoch total loss 3.12457228\n",
      "Trained batch 1965 batch loss 2.64971876 epoch total loss 3.12433076\n",
      "Trained batch 1966 batch loss 2.81871891 epoch total loss 3.12417555\n",
      "Trained batch 1967 batch loss 2.70618296 epoch total loss 3.12396288\n",
      "Trained batch 1968 batch loss 3.09696412 epoch total loss 3.12394929\n",
      "Trained batch 1969 batch loss 3.33812881 epoch total loss 3.12405801\n",
      "Trained batch 1970 batch loss 3.2309413 epoch total loss 3.12411213\n",
      "Trained batch 1971 batch loss 3.12543082 epoch total loss 3.12411284\n",
      "Trained batch 1972 batch loss 3.13439465 epoch total loss 3.12411809\n",
      "Trained batch 1973 batch loss 2.93971777 epoch total loss 3.12402463\n",
      "Trained batch 1974 batch loss 2.86281228 epoch total loss 3.12389231\n",
      "Trained batch 1975 batch loss 2.71173429 epoch total loss 3.12368369\n",
      "Trained batch 1976 batch loss 2.86534452 epoch total loss 3.12355304\n",
      "Trained batch 1977 batch loss 2.89173985 epoch total loss 3.12343574\n",
      "Trained batch 1978 batch loss 2.79263711 epoch total loss 3.12326837\n",
      "Trained batch 1979 batch loss 2.67840314 epoch total loss 3.12304354\n",
      "Trained batch 1980 batch loss 3.01236248 epoch total loss 3.12298751\n",
      "Trained batch 1981 batch loss 2.79657507 epoch total loss 3.12282252\n",
      "Trained batch 1982 batch loss 2.94852352 epoch total loss 3.12273479\n",
      "Trained batch 1983 batch loss 2.94785905 epoch total loss 3.12264657\n",
      "Trained batch 1984 batch loss 2.84317088 epoch total loss 3.12250566\n",
      "Trained batch 1985 batch loss 2.98510456 epoch total loss 3.12243629\n",
      "Trained batch 1986 batch loss 2.89395666 epoch total loss 3.12232137\n",
      "Trained batch 1987 batch loss 2.66749954 epoch total loss 3.12209249\n",
      "Trained batch 1988 batch loss 2.90448427 epoch total loss 3.12198281\n",
      "Trained batch 1989 batch loss 2.80340552 epoch total loss 3.1218226\n",
      "Trained batch 1990 batch loss 2.88763285 epoch total loss 3.12170506\n",
      "Trained batch 1991 batch loss 2.77483892 epoch total loss 3.12153077\n",
      "Trained batch 1992 batch loss 2.86528683 epoch total loss 3.12140203\n",
      "Trained batch 1993 batch loss 2.73689127 epoch total loss 3.12120914\n",
      "Trained batch 1994 batch loss 2.82499695 epoch total loss 3.12106061\n",
      "Trained batch 1995 batch loss 2.7846725 epoch total loss 3.12089205\n",
      "Trained batch 1996 batch loss 2.69218874 epoch total loss 3.12067747\n",
      "Trained batch 1997 batch loss 2.95070624 epoch total loss 3.12059236\n",
      "Trained batch 1998 batch loss 2.84227 epoch total loss 3.12045288\n",
      "Trained batch 1999 batch loss 3.09056783 epoch total loss 3.12043786\n",
      "Trained batch 2000 batch loss 3.07524824 epoch total loss 3.12041521\n",
      "Trained batch 2001 batch loss 3.03142166 epoch total loss 3.12037063\n",
      "Trained batch 2002 batch loss 3.22648716 epoch total loss 3.12042379\n",
      "Trained batch 2003 batch loss 3.49355221 epoch total loss 3.12061\n",
      "Trained batch 2004 batch loss 3.76707911 epoch total loss 3.12093258\n",
      "Trained batch 2005 batch loss 3.50738931 epoch total loss 3.12112546\n",
      "Trained batch 2006 batch loss 3.15184641 epoch total loss 3.12114072\n",
      "Trained batch 2007 batch loss 3.00053453 epoch total loss 3.12108064\n",
      "Trained batch 2008 batch loss 3.25776792 epoch total loss 3.12114882\n",
      "Trained batch 2009 batch loss 2.95383096 epoch total loss 3.12106538\n",
      "Trained batch 2010 batch loss 3.12739229 epoch total loss 3.12106848\n",
      "Trained batch 2011 batch loss 3.00363302 epoch total loss 3.12101\n",
      "Trained batch 2012 batch loss 3.12592459 epoch total loss 3.12101245\n",
      "Trained batch 2013 batch loss 2.71769595 epoch total loss 3.12081218\n",
      "Trained batch 2014 batch loss 2.41208839 epoch total loss 3.12046027\n",
      "Trained batch 2015 batch loss 2.78650546 epoch total loss 3.12029457\n",
      "Trained batch 2016 batch loss 2.82652974 epoch total loss 3.1201489\n",
      "Trained batch 2017 batch loss 3.02693415 epoch total loss 3.12010264\n",
      "Trained batch 2018 batch loss 3.09968519 epoch total loss 3.12009239\n",
      "Trained batch 2019 batch loss 3.12059879 epoch total loss 3.12009287\n",
      "Trained batch 2020 batch loss 3.3310523 epoch total loss 3.1201973\n",
      "Trained batch 2021 batch loss 2.91895914 epoch total loss 3.12009764\n",
      "Trained batch 2022 batch loss 2.93051338 epoch total loss 3.12000394\n",
      "Trained batch 2023 batch loss 2.83289242 epoch total loss 3.11986208\n",
      "Trained batch 2024 batch loss 2.81018591 epoch total loss 3.11970901\n",
      "Trained batch 2025 batch loss 2.60625815 epoch total loss 3.11945558\n",
      "Trained batch 2026 batch loss 2.82726383 epoch total loss 3.11931133\n",
      "Trained batch 2027 batch loss 2.97127104 epoch total loss 3.11923814\n",
      "Trained batch 2028 batch loss 2.99356079 epoch total loss 3.11917615\n",
      "Trained batch 2029 batch loss 2.86103868 epoch total loss 3.11904883\n",
      "Trained batch 2030 batch loss 2.50000191 epoch total loss 3.1187439\n",
      "Trained batch 2031 batch loss 2.65322495 epoch total loss 3.11851478\n",
      "Trained batch 2032 batch loss 2.80783653 epoch total loss 3.11836171\n",
      "Trained batch 2033 batch loss 2.83627963 epoch total loss 3.11822319\n",
      "Trained batch 2034 batch loss 2.86864161 epoch total loss 3.1181004\n",
      "Trained batch 2035 batch loss 2.6156137 epoch total loss 3.11785364\n",
      "Trained batch 2036 batch loss 2.80238128 epoch total loss 3.11769867\n",
      "Trained batch 2037 batch loss 2.86259079 epoch total loss 3.1175735\n",
      "Trained batch 2038 batch loss 2.97751284 epoch total loss 3.1175046\n",
      "Trained batch 2039 batch loss 3.11213708 epoch total loss 3.11750221\n",
      "Trained batch 2040 batch loss 3.03122187 epoch total loss 3.11745977\n",
      "Trained batch 2041 batch loss 2.93709922 epoch total loss 3.11737156\n",
      "Trained batch 2042 batch loss 3.25563622 epoch total loss 3.11743927\n",
      "Trained batch 2043 batch loss 3.11724091 epoch total loss 3.11743927\n",
      "Trained batch 2044 batch loss 3.41260123 epoch total loss 3.11758351\n",
      "Trained batch 2045 batch loss 3.16422558 epoch total loss 3.1176064\n",
      "Trained batch 2046 batch loss 3.23646808 epoch total loss 3.11766434\n",
      "Trained batch 2047 batch loss 2.90607452 epoch total loss 3.1175611\n",
      "Trained batch 2048 batch loss 3.12283301 epoch total loss 3.11756372\n",
      "Trained batch 2049 batch loss 3.03678489 epoch total loss 3.11752415\n",
      "Trained batch 2050 batch loss 3.3647666 epoch total loss 3.11764479\n",
      "Trained batch 2051 batch loss 3.02196741 epoch total loss 3.11759806\n",
      "Trained batch 2052 batch loss 3.04235506 epoch total loss 3.11756158\n",
      "Trained batch 2053 batch loss 2.9352808 epoch total loss 3.11747265\n",
      "Trained batch 2054 batch loss 2.73985 epoch total loss 3.11728883\n",
      "Trained batch 2055 batch loss 2.86859226 epoch total loss 3.11716771\n",
      "Trained batch 2056 batch loss 2.82973409 epoch total loss 3.117028\n",
      "Trained batch 2057 batch loss 3.00218487 epoch total loss 3.11697197\n",
      "Trained batch 2058 batch loss 3.19252062 epoch total loss 3.11700869\n",
      "Trained batch 2059 batch loss 3.28541613 epoch total loss 3.11709046\n",
      "Trained batch 2060 batch loss 3.28015447 epoch total loss 3.11716962\n",
      "Trained batch 2061 batch loss 3.24614048 epoch total loss 3.11723232\n",
      "Trained batch 2062 batch loss 3.21059465 epoch total loss 3.11727738\n",
      "Trained batch 2063 batch loss 2.92300892 epoch total loss 3.11718321\n",
      "Trained batch 2064 batch loss 2.7522974 epoch total loss 3.11700654\n",
      "Trained batch 2065 batch loss 2.89485359 epoch total loss 3.11689901\n",
      "Trained batch 2066 batch loss 3.1263082 epoch total loss 3.11690354\n",
      "Trained batch 2067 batch loss 3.11455536 epoch total loss 3.11690259\n",
      "Trained batch 2068 batch loss 3.07672715 epoch total loss 3.11688304\n",
      "Trained batch 2069 batch loss 2.94100595 epoch total loss 3.11679816\n",
      "Trained batch 2070 batch loss 2.86991167 epoch total loss 3.11667895\n",
      "Trained batch 2071 batch loss 2.99345493 epoch total loss 3.11661959\n",
      "Trained batch 2072 batch loss 3.25363493 epoch total loss 3.11668563\n",
      "Trained batch 2073 batch loss 2.89937115 epoch total loss 3.11658072\n",
      "Trained batch 2074 batch loss 3.16050434 epoch total loss 3.11660194\n",
      "Trained batch 2075 batch loss 3.28650427 epoch total loss 3.11668396\n",
      "Trained batch 2076 batch loss 2.9984107 epoch total loss 3.11662698\n",
      "Trained batch 2077 batch loss 3.0928545 epoch total loss 3.11661553\n",
      "Trained batch 2078 batch loss 2.7894268 epoch total loss 3.11645818\n",
      "Trained batch 2079 batch loss 2.42499542 epoch total loss 3.11612535\n",
      "Trained batch 2080 batch loss 2.53985667 epoch total loss 3.11584854\n",
      "Trained batch 2081 batch loss 2.6030612 epoch total loss 3.11560202\n",
      "Trained batch 2082 batch loss 2.28650951 epoch total loss 3.11520386\n",
      "Trained batch 2083 batch loss 2.40258121 epoch total loss 3.11486173\n",
      "Trained batch 2084 batch loss 2.67361784 epoch total loss 3.11465\n",
      "Trained batch 2085 batch loss 2.36047053 epoch total loss 3.11428833\n",
      "Trained batch 2086 batch loss 2.51373148 epoch total loss 3.11400032\n",
      "Trained batch 2087 batch loss 2.82882166 epoch total loss 3.11386347\n",
      "Trained batch 2088 batch loss 3.02265072 epoch total loss 3.11381984\n",
      "Trained batch 2089 batch loss 3.24239421 epoch total loss 3.11388135\n",
      "Trained batch 2090 batch loss 3.11297655 epoch total loss 3.11388063\n",
      "Trained batch 2091 batch loss 3.02039623 epoch total loss 3.11383605\n",
      "Trained batch 2092 batch loss 2.93066573 epoch total loss 3.11374855\n",
      "Trained batch 2093 batch loss 2.90151238 epoch total loss 3.11364698\n",
      "Trained batch 2094 batch loss 2.97015882 epoch total loss 3.11357856\n",
      "Trained batch 2095 batch loss 2.80238867 epoch total loss 3.11343\n",
      "Trained batch 2096 batch loss 2.84970808 epoch total loss 3.11330414\n",
      "Trained batch 2097 batch loss 2.49849629 epoch total loss 3.11301088\n",
      "Trained batch 2098 batch loss 2.77806926 epoch total loss 3.11285114\n",
      "Trained batch 2099 batch loss 2.77002931 epoch total loss 3.11268783\n",
      "Trained batch 2100 batch loss 2.92009592 epoch total loss 3.11259604\n",
      "Trained batch 2101 batch loss 3.06423378 epoch total loss 3.11257315\n",
      "Trained batch 2102 batch loss 2.91079164 epoch total loss 3.11247706\n",
      "Trained batch 2103 batch loss 3.09330797 epoch total loss 3.112468\n",
      "Trained batch 2104 batch loss 2.84889746 epoch total loss 3.11234283\n",
      "Trained batch 2105 batch loss 2.6385541 epoch total loss 3.11211777\n",
      "Trained batch 2106 batch loss 2.7981019 epoch total loss 3.11196876\n",
      "Trained batch 2107 batch loss 2.42662525 epoch total loss 3.11164355\n",
      "Trained batch 2108 batch loss 2.92662525 epoch total loss 3.11155581\n",
      "Trained batch 2109 batch loss 2.78424287 epoch total loss 3.1114006\n",
      "Trained batch 2110 batch loss 2.84301 epoch total loss 3.11127329\n",
      "Trained batch 2111 batch loss 2.86034942 epoch total loss 3.11115432\n",
      "Trained batch 2112 batch loss 3.13459492 epoch total loss 3.11116552\n",
      "Trained batch 2113 batch loss 3.20469379 epoch total loss 3.11120987\n",
      "Trained batch 2114 batch loss 3.02751803 epoch total loss 3.11117\n",
      "Trained batch 2115 batch loss 2.90040159 epoch total loss 3.11107039\n",
      "Trained batch 2116 batch loss 3.08731 epoch total loss 3.11105919\n",
      "Trained batch 2117 batch loss 2.77955 epoch total loss 3.11090279\n",
      "Trained batch 2118 batch loss 2.61262512 epoch total loss 3.11066771\n",
      "Trained batch 2119 batch loss 2.74593925 epoch total loss 3.11049557\n",
      "Trained batch 2120 batch loss 2.66971445 epoch total loss 3.11028767\n",
      "Trained batch 2121 batch loss 2.64933777 epoch total loss 3.11007047\n",
      "Trained batch 2122 batch loss 2.64314961 epoch total loss 3.10985041\n",
      "Trained batch 2123 batch loss 2.6444993 epoch total loss 3.1096313\n",
      "Trained batch 2124 batch loss 2.64736462 epoch total loss 3.10941362\n",
      "Trained batch 2125 batch loss 2.76865339 epoch total loss 3.10925317\n",
      "Trained batch 2126 batch loss 2.9355545 epoch total loss 3.10917163\n",
      "Trained batch 2127 batch loss 2.85131121 epoch total loss 3.10905027\n",
      "Trained batch 2128 batch loss 2.89782953 epoch total loss 3.10895085\n",
      "Trained batch 2129 batch loss 2.9359746 epoch total loss 3.10886979\n",
      "Trained batch 2130 batch loss 2.81634712 epoch total loss 3.10873246\n",
      "Trained batch 2131 batch loss 2.79952693 epoch total loss 3.10858727\n",
      "Trained batch 2132 batch loss 2.66028619 epoch total loss 3.10837698\n",
      "Trained batch 2133 batch loss 2.73207355 epoch total loss 3.10820031\n",
      "Trained batch 2134 batch loss 2.46840596 epoch total loss 3.10790062\n",
      "Trained batch 2135 batch loss 2.66820765 epoch total loss 3.10769439\n",
      "Trained batch 2136 batch loss 2.60996675 epoch total loss 3.10746145\n",
      "Trained batch 2137 batch loss 2.698843 epoch total loss 3.10727024\n",
      "Trained batch 2138 batch loss 2.85115862 epoch total loss 3.10715032\n",
      "Trained batch 2139 batch loss 2.75039959 epoch total loss 3.10698366\n",
      "Trained batch 2140 batch loss 2.81597 epoch total loss 3.10684752\n",
      "Trained batch 2141 batch loss 3.16113138 epoch total loss 3.10687304\n",
      "Trained batch 2142 batch loss 2.90086174 epoch total loss 3.10677671\n",
      "Trained batch 2143 batch loss 2.76790476 epoch total loss 3.10661864\n",
      "Trained batch 2144 batch loss 2.96047187 epoch total loss 3.10655046\n",
      "Trained batch 2145 batch loss 2.89899206 epoch total loss 3.10645366\n",
      "Trained batch 2146 batch loss 2.80871582 epoch total loss 3.1063149\n",
      "Trained batch 2147 batch loss 2.6429143 epoch total loss 3.10609913\n",
      "Trained batch 2148 batch loss 3.08327127 epoch total loss 3.10608864\n",
      "Trained batch 2149 batch loss 2.91751146 epoch total loss 3.1060009\n",
      "Trained batch 2150 batch loss 3.10766649 epoch total loss 3.10600185\n",
      "Trained batch 2151 batch loss 2.96027613 epoch total loss 3.10593414\n",
      "Trained batch 2152 batch loss 2.86080766 epoch total loss 3.10582018\n",
      "Trained batch 2153 batch loss 3.00745273 epoch total loss 3.1057744\n",
      "Trained batch 2154 batch loss 2.82365012 epoch total loss 3.10564351\n",
      "Trained batch 2155 batch loss 2.86938119 epoch total loss 3.10553384\n",
      "Trained batch 2156 batch loss 2.90632439 epoch total loss 3.10544133\n",
      "Trained batch 2157 batch loss 2.93204141 epoch total loss 3.10536098\n",
      "Trained batch 2158 batch loss 2.80154 epoch total loss 3.10522032\n",
      "Trained batch 2159 batch loss 2.7264328 epoch total loss 3.10504484\n",
      "Trained batch 2160 batch loss 2.7723825 epoch total loss 3.10489106\n",
      "Trained batch 2161 batch loss 2.49283361 epoch total loss 3.10460758\n",
      "Trained batch 2162 batch loss 2.8366766 epoch total loss 3.10448384\n",
      "Trained batch 2163 batch loss 2.64967012 epoch total loss 3.10427356\n",
      "Trained batch 2164 batch loss 3.01606369 epoch total loss 3.10423303\n",
      "Trained batch 2165 batch loss 2.95951056 epoch total loss 3.10416603\n",
      "Trained batch 2166 batch loss 2.62054276 epoch total loss 3.10394287\n",
      "Trained batch 2167 batch loss 2.56151104 epoch total loss 3.10369253\n",
      "Trained batch 2168 batch loss 2.90834951 epoch total loss 3.10360241\n",
      "Trained batch 2169 batch loss 2.72959137 epoch total loss 3.10342979\n",
      "Trained batch 2170 batch loss 3.10252428 epoch total loss 3.10342956\n",
      "Trained batch 2171 batch loss 3.06951189 epoch total loss 3.10341382\n",
      "Trained batch 2172 batch loss 2.99992537 epoch total loss 3.10336614\n",
      "Trained batch 2173 batch loss 2.89040732 epoch total loss 3.10326815\n",
      "Trained batch 2174 batch loss 2.93318 epoch total loss 3.10319\n",
      "Trained batch 2175 batch loss 2.72473192 epoch total loss 3.1030159\n",
      "Trained batch 2176 batch loss 2.50288534 epoch total loss 3.10274\n",
      "Trained batch 2177 batch loss 2.44624043 epoch total loss 3.10243869\n",
      "Trained batch 2178 batch loss 2.76009583 epoch total loss 3.10228157\n",
      "Trained batch 2179 batch loss 2.86218977 epoch total loss 3.10217142\n",
      "Trained batch 2180 batch loss 2.84954309 epoch total loss 3.10205555\n",
      "Trained batch 2181 batch loss 2.79949284 epoch total loss 3.10191679\n",
      "Trained batch 2182 batch loss 2.92433763 epoch total loss 3.10183525\n",
      "Trained batch 2183 batch loss 2.68274641 epoch total loss 3.10164332\n",
      "Trained batch 2184 batch loss 2.81685901 epoch total loss 3.10151291\n",
      "Trained batch 2185 batch loss 2.86151838 epoch total loss 3.101403\n",
      "Trained batch 2186 batch loss 2.86648965 epoch total loss 3.10129547\n",
      "Trained batch 2187 batch loss 3.14662194 epoch total loss 3.10131621\n",
      "Trained batch 2188 batch loss 2.44072866 epoch total loss 3.10101438\n",
      "Trained batch 2189 batch loss 3.00576544 epoch total loss 3.10097098\n",
      "Trained batch 2190 batch loss 2.84633613 epoch total loss 3.10085464\n",
      "Trained batch 2191 batch loss 2.93669271 epoch total loss 3.10077953\n",
      "Trained batch 2192 batch loss 2.92649055 epoch total loss 3.1007\n",
      "Trained batch 2193 batch loss 2.97552 epoch total loss 3.10064292\n",
      "Trained batch 2194 batch loss 3.06573248 epoch total loss 3.10062718\n",
      "Trained batch 2195 batch loss 3.17638445 epoch total loss 3.10066152\n",
      "Trained batch 2196 batch loss 3.13863945 epoch total loss 3.10067892\n",
      "Trained batch 2197 batch loss 3.37480044 epoch total loss 3.10080385\n",
      "Trained batch 2198 batch loss 3.07470155 epoch total loss 3.10079193\n",
      "Trained batch 2199 batch loss 3.30706692 epoch total loss 3.10088563\n",
      "Trained batch 2200 batch loss 3.24578238 epoch total loss 3.10095143\n",
      "Trained batch 2201 batch loss 2.83211327 epoch total loss 3.10082936\n",
      "Trained batch 2202 batch loss 2.64337683 epoch total loss 3.1006217\n",
      "Trained batch 2203 batch loss 2.57691312 epoch total loss 3.100384\n",
      "Trained batch 2204 batch loss 2.64347196 epoch total loss 3.10017681\n",
      "Trained batch 2205 batch loss 2.4986372 epoch total loss 3.09990382\n",
      "Trained batch 2206 batch loss 3.01496696 epoch total loss 3.09986544\n",
      "Trained batch 2207 batch loss 3.27035165 epoch total loss 3.09994268\n",
      "Trained batch 2208 batch loss 3.37873054 epoch total loss 3.10006905\n",
      "Trained batch 2209 batch loss 3.40548587 epoch total loss 3.10020733\n",
      "Trained batch 2210 batch loss 3.10725832 epoch total loss 3.10021067\n",
      "Trained batch 2211 batch loss 2.92962861 epoch total loss 3.10013342\n",
      "Trained batch 2212 batch loss 3.03497791 epoch total loss 3.10010409\n",
      "Trained batch 2213 batch loss 3.05501223 epoch total loss 3.10008383\n",
      "Trained batch 2214 batch loss 3.07833052 epoch total loss 3.10007381\n",
      "Trained batch 2215 batch loss 2.92226696 epoch total loss 3.09999371\n",
      "Trained batch 2216 batch loss 3.05077124 epoch total loss 3.09997129\n",
      "Trained batch 2217 batch loss 3.12327909 epoch total loss 3.09998178\n",
      "Trained batch 2218 batch loss 3.15018868 epoch total loss 3.10000443\n",
      "Trained batch 2219 batch loss 3.2342248 epoch total loss 3.10006499\n",
      "Trained batch 2220 batch loss 3.13311 epoch total loss 3.10008\n",
      "Trained batch 2221 batch loss 2.64467907 epoch total loss 3.09987497\n",
      "Trained batch 2222 batch loss 2.74995708 epoch total loss 3.09971738\n",
      "Trained batch 2223 batch loss 2.97095561 epoch total loss 3.09965968\n",
      "Trained batch 2224 batch loss 3.00441408 epoch total loss 3.09961677\n",
      "Trained batch 2225 batch loss 2.81303358 epoch total loss 3.09948802\n",
      "Trained batch 2226 batch loss 3.0224514 epoch total loss 3.09945345\n",
      "Trained batch 2227 batch loss 2.77446651 epoch total loss 3.09930754\n",
      "Trained batch 2228 batch loss 2.90144706 epoch total loss 3.09921861\n",
      "Trained batch 2229 batch loss 2.88956165 epoch total loss 3.09912467\n",
      "Trained batch 2230 batch loss 2.94079971 epoch total loss 3.09905362\n",
      "Trained batch 2231 batch loss 2.87200761 epoch total loss 3.09895182\n",
      "Trained batch 2232 batch loss 2.8821516 epoch total loss 3.09885478\n",
      "Trained batch 2233 batch loss 2.84430599 epoch total loss 3.09874082\n",
      "Trained batch 2234 batch loss 3.09163737 epoch total loss 3.09873772\n",
      "Trained batch 2235 batch loss 3.23274517 epoch total loss 3.0987978\n",
      "Trained batch 2236 batch loss 3.06320882 epoch total loss 3.09878182\n",
      "Trained batch 2237 batch loss 3.28699589 epoch total loss 3.09886599\n",
      "Trained batch 2238 batch loss 2.96939349 epoch total loss 3.09880805\n",
      "Trained batch 2239 batch loss 3.02812433 epoch total loss 3.09877658\n",
      "Trained batch 2240 batch loss 3.04287815 epoch total loss 3.09875154\n",
      "Trained batch 2241 batch loss 2.88352156 epoch total loss 3.09865546\n",
      "Trained batch 2242 batch loss 3.08485079 epoch total loss 3.09864926\n",
      "Trained batch 2243 batch loss 2.82582831 epoch total loss 3.09852767\n",
      "Trained batch 2244 batch loss 2.88918257 epoch total loss 3.09843445\n",
      "Trained batch 2245 batch loss 2.71480131 epoch total loss 3.0982635\n",
      "Trained batch 2246 batch loss 2.70726728 epoch total loss 3.09808922\n",
      "Trained batch 2247 batch loss 2.76966667 epoch total loss 3.09794307\n",
      "Trained batch 2248 batch loss 2.89415121 epoch total loss 3.09785247\n",
      "Trained batch 2249 batch loss 2.6786871 epoch total loss 3.09766603\n",
      "Trained batch 2250 batch loss 2.88410378 epoch total loss 3.09757113\n",
      "Trained batch 2251 batch loss 2.89161682 epoch total loss 3.09747958\n",
      "Trained batch 2252 batch loss 2.72562051 epoch total loss 3.0973146\n",
      "Trained batch 2253 batch loss 2.7528379 epoch total loss 3.09716177\n",
      "Trained batch 2254 batch loss 2.85305429 epoch total loss 3.09705329\n",
      "Trained batch 2255 batch loss 3.00952864 epoch total loss 3.09701467\n",
      "Trained batch 2256 batch loss 2.86504555 epoch total loss 3.09691191\n",
      "Trained batch 2257 batch loss 2.89546514 epoch total loss 3.09682274\n",
      "Trained batch 2258 batch loss 2.91011047 epoch total loss 3.09674\n",
      "Trained batch 2259 batch loss 2.80356026 epoch total loss 3.09661031\n",
      "Trained batch 2260 batch loss 2.87157774 epoch total loss 3.09651065\n",
      "Trained batch 2261 batch loss 2.61668301 epoch total loss 3.09629846\n",
      "Trained batch 2262 batch loss 2.59259 epoch total loss 3.09607601\n",
      "Trained batch 2263 batch loss 2.6640377 epoch total loss 3.09588504\n",
      "Trained batch 2264 batch loss 2.80993366 epoch total loss 3.09575868\n",
      "Trained batch 2265 batch loss 3.25514436 epoch total loss 3.09582925\n",
      "Trained batch 2266 batch loss 3.17122078 epoch total loss 3.09586263\n",
      "Trained batch 2267 batch loss 3.13875508 epoch total loss 3.09588146\n",
      "Trained batch 2268 batch loss 2.74743772 epoch total loss 3.09572792\n",
      "Trained batch 2269 batch loss 2.85062551 epoch total loss 3.09562\n",
      "Trained batch 2270 batch loss 2.53985786 epoch total loss 3.09537506\n",
      "Trained batch 2271 batch loss 2.6060462 epoch total loss 3.09515953\n",
      "Trained batch 2272 batch loss 2.72510576 epoch total loss 3.09499669\n",
      "Trained batch 2273 batch loss 2.74705648 epoch total loss 3.09484363\n",
      "Trained batch 2274 batch loss 2.91051626 epoch total loss 3.09476256\n",
      "Trained batch 2275 batch loss 2.83997035 epoch total loss 3.09465051\n",
      "Trained batch 2276 batch loss 2.88027692 epoch total loss 3.09455633\n",
      "Trained batch 2277 batch loss 3.13509727 epoch total loss 3.09457421\n",
      "Trained batch 2278 batch loss 3.14328337 epoch total loss 3.09459567\n",
      "Trained batch 2279 batch loss 3.10370016 epoch total loss 3.09459949\n",
      "Trained batch 2280 batch loss 2.93183947 epoch total loss 3.09452796\n",
      "Trained batch 2281 batch loss 2.82502842 epoch total loss 3.09441\n",
      "Trained batch 2282 batch loss 2.87749195 epoch total loss 3.09431481\n",
      "Trained batch 2283 batch loss 2.90689588 epoch total loss 3.0942328\n",
      "Trained batch 2284 batch loss 2.83052397 epoch total loss 3.09411716\n",
      "Trained batch 2285 batch loss 2.77495742 epoch total loss 3.09397769\n",
      "Trained batch 2286 batch loss 2.7957058 epoch total loss 3.09384727\n",
      "Trained batch 2287 batch loss 2.65150046 epoch total loss 3.09365368\n",
      "Trained batch 2288 batch loss 2.84929132 epoch total loss 3.09354687\n",
      "Trained batch 2289 batch loss 2.94959617 epoch total loss 3.09348392\n",
      "Trained batch 2290 batch loss 2.74286175 epoch total loss 3.09333086\n",
      "Trained batch 2291 batch loss 3.14863181 epoch total loss 3.09335494\n",
      "Trained batch 2292 batch loss 3.2185061 epoch total loss 3.09340954\n",
      "Trained batch 2293 batch loss 3.14214897 epoch total loss 3.09343076\n",
      "Trained batch 2294 batch loss 3.05574274 epoch total loss 3.09341431\n",
      "Trained batch 2295 batch loss 3.12425327 epoch total loss 3.09342766\n",
      "Trained batch 2296 batch loss 2.97520065 epoch total loss 3.09337616\n",
      "Trained batch 2297 batch loss 2.66613603 epoch total loss 3.09319019\n",
      "Trained batch 2298 batch loss 2.73296547 epoch total loss 3.09303331\n",
      "Trained batch 2299 batch loss 2.87350392 epoch total loss 3.09293771\n",
      "Trained batch 2300 batch loss 2.68252 epoch total loss 3.09275937\n",
      "Trained batch 2301 batch loss 2.65519762 epoch total loss 3.09256935\n",
      "Trained batch 2302 batch loss 2.61555099 epoch total loss 3.09236217\n",
      "Trained batch 2303 batch loss 3.1699903 epoch total loss 3.09239578\n",
      "Trained batch 2304 batch loss 2.52047491 epoch total loss 3.09214759\n",
      "Trained batch 2305 batch loss 2.97206855 epoch total loss 3.09209561\n",
      "Trained batch 2306 batch loss 2.7913413 epoch total loss 3.0919652\n",
      "Trained batch 2307 batch loss 3.0697279 epoch total loss 3.09195566\n",
      "Trained batch 2308 batch loss 3.26188374 epoch total loss 3.09202909\n",
      "Trained batch 2309 batch loss 3.06381083 epoch total loss 3.09201694\n",
      "Trained batch 2310 batch loss 2.68513179 epoch total loss 3.09184074\n",
      "Trained batch 2311 batch loss 2.9636445 epoch total loss 3.09178543\n",
      "Trained batch 2312 batch loss 3.11180949 epoch total loss 3.09179401\n",
      "Trained batch 2313 batch loss 3.16526437 epoch total loss 3.09182572\n",
      "Trained batch 2314 batch loss 2.94448805 epoch total loss 3.09176207\n",
      "Trained batch 2315 batch loss 2.83766222 epoch total loss 3.09165239\n",
      "Trained batch 2316 batch loss 2.82350397 epoch total loss 3.09153676\n",
      "Trained batch 2317 batch loss 2.86696696 epoch total loss 3.09144\n",
      "Trained batch 2318 batch loss 3.10609245 epoch total loss 3.09144616\n",
      "Trained batch 2319 batch loss 2.68227029 epoch total loss 3.09126973\n",
      "Trained batch 2320 batch loss 2.73031282 epoch total loss 3.09111404\n",
      "Trained batch 2321 batch loss 3.03898764 epoch total loss 3.09109163\n",
      "Trained batch 2322 batch loss 2.90160513 epoch total loss 3.09101\n",
      "Trained batch 2323 batch loss 3.10483885 epoch total loss 3.09101605\n",
      "Trained batch 2324 batch loss 2.73475623 epoch total loss 3.09086275\n",
      "Trained batch 2325 batch loss 2.76019144 epoch total loss 3.09072065\n",
      "Trained batch 2326 batch loss 2.95809221 epoch total loss 3.09066343\n",
      "Trained batch 2327 batch loss 2.91111541 epoch total loss 3.09058642\n",
      "Trained batch 2328 batch loss 2.93397236 epoch total loss 3.09051919\n",
      "Trained batch 2329 batch loss 2.88239074 epoch total loss 3.09042978\n",
      "Trained batch 2330 batch loss 2.78834534 epoch total loss 3.0903\n",
      "Trained batch 2331 batch loss 2.48561 epoch total loss 3.09004092\n",
      "Trained batch 2332 batch loss 2.71388674 epoch total loss 3.08987951\n",
      "Trained batch 2333 batch loss 2.80795479 epoch total loss 3.08975887\n",
      "Trained batch 2334 batch loss 2.65138102 epoch total loss 3.089571\n",
      "Trained batch 2335 batch loss 2.39381146 epoch total loss 3.08927298\n",
      "Trained batch 2336 batch loss 2.4478724 epoch total loss 3.08899856\n",
      "Trained batch 2337 batch loss 2.4644444 epoch total loss 3.08873129\n",
      "Trained batch 2338 batch loss 2.59950161 epoch total loss 3.08852196\n",
      "Trained batch 2339 batch loss 2.6633718 epoch total loss 3.08834028\n",
      "Trained batch 2340 batch loss 3.11229706 epoch total loss 3.08835053\n",
      "Trained batch 2341 batch loss 3.09867287 epoch total loss 3.08835483\n",
      "Trained batch 2342 batch loss 3.01438761 epoch total loss 3.08832335\n",
      "Trained batch 2343 batch loss 3.00007725 epoch total loss 3.08828545\n",
      "Trained batch 2344 batch loss 3.36336279 epoch total loss 3.08840275\n",
      "Trained batch 2345 batch loss 3.22284698 epoch total loss 3.08846021\n",
      "Trained batch 2346 batch loss 3.05619 epoch total loss 3.08844638\n",
      "Trained batch 2347 batch loss 3.15449572 epoch total loss 3.08847451\n",
      "Trained batch 2348 batch loss 2.99432182 epoch total loss 3.08843422\n",
      "Trained batch 2349 batch loss 2.8403492 epoch total loss 3.0883286\n",
      "Trained batch 2350 batch loss 2.89240694 epoch total loss 3.08824539\n",
      "Trained batch 2351 batch loss 2.70904398 epoch total loss 3.08808398\n",
      "Trained batch 2352 batch loss 2.80466938 epoch total loss 3.08796358\n",
      "Trained batch 2353 batch loss 3.17825294 epoch total loss 3.08800197\n",
      "Trained batch 2354 batch loss 2.76741838 epoch total loss 3.08786583\n",
      "Trained batch 2355 batch loss 2.80081844 epoch total loss 3.08774376\n",
      "Trained batch 2356 batch loss 3.06156445 epoch total loss 3.08773279\n",
      "Trained batch 2357 batch loss 2.95720863 epoch total loss 3.08767724\n",
      "Trained batch 2358 batch loss 2.93598318 epoch total loss 3.08761287\n",
      "Trained batch 2359 batch loss 2.66889763 epoch total loss 3.08743548\n",
      "Trained batch 2360 batch loss 2.88114834 epoch total loss 3.08734822\n",
      "Trained batch 2361 batch loss 2.80966449 epoch total loss 3.08723044\n",
      "Trained batch 2362 batch loss 2.67432189 epoch total loss 3.08705568\n",
      "Trained batch 2363 batch loss 2.81080723 epoch total loss 3.08693886\n",
      "Trained batch 2364 batch loss 2.69578481 epoch total loss 3.0867734\n",
      "Trained batch 2365 batch loss 2.79388642 epoch total loss 3.08664966\n",
      "Trained batch 2366 batch loss 2.67726755 epoch total loss 3.08647656\n",
      "Trained batch 2367 batch loss 2.58590126 epoch total loss 3.08626509\n",
      "Trained batch 2368 batch loss 2.64615607 epoch total loss 3.08607912\n",
      "Trained batch 2369 batch loss 2.76702738 epoch total loss 3.08594441\n",
      "Trained batch 2370 batch loss 2.81885386 epoch total loss 3.08583188\n",
      "Trained batch 2371 batch loss 2.58177066 epoch total loss 3.08561921\n",
      "Trained batch 2372 batch loss 2.82156396 epoch total loss 3.08550787\n",
      "Trained batch 2373 batch loss 2.80726838 epoch total loss 3.08539057\n",
      "Trained batch 2374 batch loss 2.7946403 epoch total loss 3.08526802\n",
      "Trained batch 2375 batch loss 2.54212141 epoch total loss 3.08503938\n",
      "Trained batch 2376 batch loss 2.57757902 epoch total loss 3.08482575\n",
      "Trained batch 2377 batch loss 2.79316711 epoch total loss 3.08470297\n",
      "Trained batch 2378 batch loss 2.74880791 epoch total loss 3.08456182\n",
      "Trained batch 2379 batch loss 2.94094563 epoch total loss 3.0845015\n",
      "Trained batch 2380 batch loss 2.70070243 epoch total loss 3.08434\n",
      "Trained batch 2381 batch loss 2.65723276 epoch total loss 3.0841608\n",
      "Trained batch 2382 batch loss 2.60504818 epoch total loss 3.08395958\n",
      "Trained batch 2383 batch loss 3.03439856 epoch total loss 3.0839386\n",
      "Trained batch 2384 batch loss 3.13075542 epoch total loss 3.08395839\n",
      "Trained batch 2385 batch loss 2.97931623 epoch total loss 3.08391452\n",
      "Trained batch 2386 batch loss 2.86390471 epoch total loss 3.08382225\n",
      "Trained batch 2387 batch loss 2.84485769 epoch total loss 3.08372211\n",
      "Trained batch 2388 batch loss 2.8684094 epoch total loss 3.08363199\n",
      "Trained batch 2389 batch loss 2.904881 epoch total loss 3.08355713\n",
      "Trained batch 2390 batch loss 2.80836105 epoch total loss 3.08344221\n",
      "Trained batch 2391 batch loss 2.95783567 epoch total loss 3.08338976\n",
      "Trained batch 2392 batch loss 2.67885423 epoch total loss 3.08322048\n",
      "Trained batch 2393 batch loss 3.09351254 epoch total loss 3.08322501\n",
      "Trained batch 2394 batch loss 2.83290148 epoch total loss 3.08312035\n",
      "Trained batch 2395 batch loss 2.81814599 epoch total loss 3.08301\n",
      "Trained batch 2396 batch loss 2.67802477 epoch total loss 3.08284092\n",
      "Trained batch 2397 batch loss 2.71498752 epoch total loss 3.08268738\n",
      "Trained batch 2398 batch loss 2.76432371 epoch total loss 3.08255458\n",
      "Trained batch 2399 batch loss 2.69861865 epoch total loss 3.0823946\n",
      "Trained batch 2400 batch loss 2.42888117 epoch total loss 3.08212209\n",
      "Trained batch 2401 batch loss 3.04087496 epoch total loss 3.08210516\n",
      "Trained batch 2402 batch loss 2.84066677 epoch total loss 3.08200455\n",
      "Trained batch 2403 batch loss 2.99051237 epoch total loss 3.08196664\n",
      "Trained batch 2404 batch loss 3.12479687 epoch total loss 3.08198452\n",
      "Trained batch 2405 batch loss 3.09698486 epoch total loss 3.08199096\n",
      "Trained batch 2406 batch loss 2.9488256 epoch total loss 3.08193541\n",
      "Trained batch 2407 batch loss 2.56541753 epoch total loss 3.08172083\n",
      "Trained batch 2408 batch loss 2.67547798 epoch total loss 3.08155203\n",
      "Trained batch 2409 batch loss 2.70984101 epoch total loss 3.08139777\n",
      "Trained batch 2410 batch loss 2.82212687 epoch total loss 3.08129025\n",
      "Trained batch 2411 batch loss 2.88221908 epoch total loss 3.08120775\n",
      "Trained batch 2412 batch loss 2.74515295 epoch total loss 3.08106852\n",
      "Trained batch 2413 batch loss 2.78546619 epoch total loss 3.08094597\n",
      "Trained batch 2414 batch loss 2.7110939 epoch total loss 3.08079267\n",
      "Trained batch 2415 batch loss 2.84084797 epoch total loss 3.08069348\n",
      "Trained batch 2416 batch loss 2.61090231 epoch total loss 3.08049893\n",
      "Trained batch 2417 batch loss 2.71621418 epoch total loss 3.08034825\n",
      "Trained batch 2418 batch loss 2.7364881 epoch total loss 3.08020592\n",
      "Trained batch 2419 batch loss 2.66318274 epoch total loss 3.08003354\n",
      "Trained batch 2420 batch loss 2.91954017 epoch total loss 3.07996726\n",
      "Trained batch 2421 batch loss 2.87358904 epoch total loss 3.07988191\n",
      "Trained batch 2422 batch loss 2.74606895 epoch total loss 3.0797441\n",
      "Trained batch 2423 batch loss 2.86341906 epoch total loss 3.07965469\n",
      "Trained batch 2424 batch loss 3.15391779 epoch total loss 3.07968521\n",
      "Trained batch 2425 batch loss 2.82209802 epoch total loss 3.07957911\n",
      "Trained batch 2426 batch loss 2.90033293 epoch total loss 3.07950521\n",
      "Trained batch 2427 batch loss 2.8476 epoch total loss 3.07940984\n",
      "Trained batch 2428 batch loss 3.10667 epoch total loss 3.0794208\n",
      "Trained batch 2429 batch loss 2.73606038 epoch total loss 3.07927942\n",
      "Trained batch 2430 batch loss 2.98326969 epoch total loss 3.07924\n",
      "Trained batch 2431 batch loss 3.04400396 epoch total loss 3.07922554\n",
      "Trained batch 2432 batch loss 2.99325061 epoch total loss 3.07919\n",
      "Trained batch 2433 batch loss 2.76382208 epoch total loss 3.07906032\n",
      "Trained batch 2434 batch loss 2.37322903 epoch total loss 3.0787704\n",
      "Trained batch 2435 batch loss 2.92678213 epoch total loss 3.07870793\n",
      "Trained batch 2436 batch loss 2.92994356 epoch total loss 3.0786469\n",
      "Trained batch 2437 batch loss 2.88682532 epoch total loss 3.07856822\n",
      "Trained batch 2438 batch loss 3.19241905 epoch total loss 3.07861495\n",
      "Trained batch 2439 batch loss 2.85212398 epoch total loss 3.07852197\n",
      "Trained batch 2440 batch loss 2.87309623 epoch total loss 3.07843781\n",
      "Trained batch 2441 batch loss 3.05676937 epoch total loss 3.07842875\n",
      "Trained batch 2442 batch loss 3.02758741 epoch total loss 3.078408\n",
      "Trained batch 2443 batch loss 3.08615208 epoch total loss 3.0784111\n",
      "Trained batch 2444 batch loss 2.99047756 epoch total loss 3.07837486\n",
      "Trained batch 2445 batch loss 2.89532518 epoch total loss 3.07830024\n",
      "Trained batch 2446 batch loss 2.74824548 epoch total loss 3.07816505\n",
      "Trained batch 2447 batch loss 3.08200216 epoch total loss 3.07816672\n",
      "Trained batch 2448 batch loss 2.89857149 epoch total loss 3.07809329\n",
      "Trained batch 2449 batch loss 2.91156721 epoch total loss 3.07802534\n",
      "Trained batch 2450 batch loss 3.15070724 epoch total loss 3.07805514\n",
      "Trained batch 2451 batch loss 2.88274574 epoch total loss 3.07797527\n",
      "Trained batch 2452 batch loss 2.86783552 epoch total loss 3.07788968\n",
      "Trained batch 2453 batch loss 2.80504584 epoch total loss 3.07777834\n",
      "Trained batch 2454 batch loss 2.85391617 epoch total loss 3.07768726\n",
      "Trained batch 2455 batch loss 2.70798492 epoch total loss 3.07753658\n",
      "Trained batch 2456 batch loss 2.85687137 epoch total loss 3.07744694\n",
      "Trained batch 2457 batch loss 2.79718709 epoch total loss 3.07733297\n",
      "Trained batch 2458 batch loss 2.94535708 epoch total loss 3.07727909\n",
      "Trained batch 2459 batch loss 2.78721714 epoch total loss 3.07716107\n",
      "Trained batch 2460 batch loss 2.78048658 epoch total loss 3.07704043\n",
      "Trained batch 2461 batch loss 2.80633593 epoch total loss 3.07693028\n",
      "Trained batch 2462 batch loss 2.77079535 epoch total loss 3.07680607\n",
      "Trained batch 2463 batch loss 2.81974554 epoch total loss 3.07670188\n",
      "Trained batch 2464 batch loss 2.82778263 epoch total loss 3.07660079\n",
      "Trained batch 2465 batch loss 2.75117397 epoch total loss 3.07646871\n",
      "Trained batch 2466 batch loss 2.6787889 epoch total loss 3.0763073\n",
      "Trained batch 2467 batch loss 2.82217836 epoch total loss 3.0762043\n",
      "Trained batch 2468 batch loss 2.77085686 epoch total loss 3.07608056\n",
      "Trained batch 2469 batch loss 2.91725707 epoch total loss 3.07601643\n",
      "Trained batch 2470 batch loss 2.85557461 epoch total loss 3.07592726\n",
      "Trained batch 2471 batch loss 3.04057 epoch total loss 3.07591271\n",
      "Trained batch 2472 batch loss 2.88017154 epoch total loss 3.0758338\n",
      "Trained batch 2473 batch loss 2.7101202 epoch total loss 3.07568574\n",
      "Trained batch 2474 batch loss 3.09490347 epoch total loss 3.07569337\n",
      "Trained batch 2475 batch loss 3.03311205 epoch total loss 3.0756762\n",
      "Trained batch 2476 batch loss 2.88534069 epoch total loss 3.07559943\n",
      "Trained batch 2477 batch loss 3.16585636 epoch total loss 3.07563591\n",
      "Trained batch 2478 batch loss 2.81747341 epoch total loss 3.07553172\n",
      "Trained batch 2479 batch loss 3.02730083 epoch total loss 3.07551217\n",
      "Trained batch 2480 batch loss 3.02515483 epoch total loss 3.07549191\n",
      "Trained batch 2481 batch loss 3.08480263 epoch total loss 3.07549572\n",
      "Trained batch 2482 batch loss 2.80938077 epoch total loss 3.07538867\n",
      "Trained batch 2483 batch loss 2.71002054 epoch total loss 3.07524157\n",
      "Trained batch 2484 batch loss 2.78377247 epoch total loss 3.07512426\n",
      "Trained batch 2485 batch loss 2.99901223 epoch total loss 3.07509351\n",
      "Trained batch 2486 batch loss 2.90640306 epoch total loss 3.07502556\n",
      "Trained batch 2487 batch loss 3.10861254 epoch total loss 3.07503915\n",
      "Trained batch 2488 batch loss 2.93690181 epoch total loss 3.0749836\n",
      "Trained batch 2489 batch loss 2.91903019 epoch total loss 3.07492089\n",
      "Trained batch 2490 batch loss 2.73614 epoch total loss 3.07478499\n",
      "Trained batch 2491 batch loss 3.07954717 epoch total loss 3.0747869\n",
      "Trained batch 2492 batch loss 2.61754394 epoch total loss 3.07460332\n",
      "Trained batch 2493 batch loss 2.86344981 epoch total loss 3.07451868\n",
      "Trained batch 2494 batch loss 2.43971848 epoch total loss 3.07426429\n",
      "Trained batch 2495 batch loss 2.91649842 epoch total loss 3.07420087\n",
      "Trained batch 2496 batch loss 2.72626781 epoch total loss 3.07406139\n",
      "Trained batch 2497 batch loss 2.75572181 epoch total loss 3.07393408\n",
      "Trained batch 2498 batch loss 2.82529974 epoch total loss 3.07383442\n",
      "Trained batch 2499 batch loss 2.84337282 epoch total loss 3.07374215\n",
      "Trained batch 2500 batch loss 2.77510166 epoch total loss 3.0736227\n",
      "Trained batch 2501 batch loss 2.74086189 epoch total loss 3.07348967\n",
      "Trained batch 2502 batch loss 3.01928043 epoch total loss 3.07346773\n",
      "Trained batch 2503 batch loss 3.30993176 epoch total loss 3.07356238\n",
      "Trained batch 2504 batch loss 3.1570611 epoch total loss 3.07359576\n",
      "Trained batch 2505 batch loss 3.1725297 epoch total loss 3.0736351\n",
      "Trained batch 2506 batch loss 3.19358206 epoch total loss 3.07368302\n",
      "Trained batch 2507 batch loss 3.04692411 epoch total loss 3.07367229\n",
      "Trained batch 2508 batch loss 3.18491173 epoch total loss 3.07371664\n",
      "Trained batch 2509 batch loss 3.22023749 epoch total loss 3.07377505\n",
      "Trained batch 2510 batch loss 3.02124453 epoch total loss 3.07375431\n",
      "Trained batch 2511 batch loss 3.26371861 epoch total loss 3.07383\n",
      "Trained batch 2512 batch loss 2.65335321 epoch total loss 3.07366252\n",
      "Trained batch 2513 batch loss 2.91670275 epoch total loss 3.07359982\n",
      "Trained batch 2514 batch loss 2.65213156 epoch total loss 3.07343245\n",
      "Trained batch 2515 batch loss 2.33903289 epoch total loss 3.07314014\n",
      "Trained batch 2516 batch loss 2.31407881 epoch total loss 3.07283854\n",
      "Trained batch 2517 batch loss 2.21085691 epoch total loss 3.07249618\n",
      "Trained batch 2518 batch loss 2.3623054 epoch total loss 3.07221413\n",
      "Trained batch 2519 batch loss 2.38645077 epoch total loss 3.07194161\n",
      "Trained batch 2520 batch loss 2.46631384 epoch total loss 3.07170129\n",
      "Trained batch 2521 batch loss 2.28875589 epoch total loss 3.07139063\n",
      "Trained batch 2522 batch loss 2.45076537 epoch total loss 3.07114458\n",
      "Trained batch 2523 batch loss 2.18906593 epoch total loss 3.07079506\n",
      "Trained batch 2524 batch loss 2.31507683 epoch total loss 3.07049561\n",
      "Trained batch 2525 batch loss 2.74217796 epoch total loss 3.07036543\n",
      "Trained batch 2526 batch loss 2.89368129 epoch total loss 3.07029557\n",
      "Trained batch 2527 batch loss 3.24521303 epoch total loss 3.07036471\n",
      "Trained batch 2528 batch loss 3.16743803 epoch total loss 3.0704031\n",
      "Trained batch 2529 batch loss 3.00859094 epoch total loss 3.07037878\n",
      "Trained batch 2530 batch loss 3.2279582 epoch total loss 3.07044101\n",
      "Trained batch 2531 batch loss 3.11514878 epoch total loss 3.07045865\n",
      "Trained batch 2532 batch loss 2.92016721 epoch total loss 3.07039952\n",
      "Trained batch 2533 batch loss 2.94230127 epoch total loss 3.07034898\n",
      "Trained batch 2534 batch loss 3.04810953 epoch total loss 3.07034016\n",
      "Trained batch 2535 batch loss 3.30067658 epoch total loss 3.07043123\n",
      "Trained batch 2536 batch loss 3.10918856 epoch total loss 3.07044649\n",
      "Trained batch 2537 batch loss 3.0971334 epoch total loss 3.07045698\n",
      "Trained batch 2538 batch loss 2.81590271 epoch total loss 3.07035685\n",
      "Trained batch 2539 batch loss 2.98214602 epoch total loss 3.07032204\n",
      "Trained batch 2540 batch loss 3.1188457 epoch total loss 3.07034087\n",
      "Trained batch 2541 batch loss 3.10331678 epoch total loss 3.07035398\n",
      "Trained batch 2542 batch loss 3.13280821 epoch total loss 3.07037854\n",
      "Trained batch 2543 batch loss 3.13413763 epoch total loss 3.07040381\n",
      "Trained batch 2544 batch loss 3.08690071 epoch total loss 3.07041025\n",
      "Trained batch 2545 batch loss 2.73101711 epoch total loss 3.07027674\n",
      "Trained batch 2546 batch loss 2.8702383 epoch total loss 3.0701983\n",
      "Trained batch 2547 batch loss 2.82501245 epoch total loss 3.07010198\n",
      "Trained batch 2548 batch loss 2.82031727 epoch total loss 3.07000399\n",
      "Trained batch 2549 batch loss 2.49275517 epoch total loss 3.06977749\n",
      "Trained batch 2550 batch loss 2.51468 epoch total loss 3.06955981\n",
      "Trained batch 2551 batch loss 2.63679981 epoch total loss 3.06939\n",
      "Trained batch 2552 batch loss 2.51981401 epoch total loss 3.06917477\n",
      "Trained batch 2553 batch loss 2.83337665 epoch total loss 3.0690825\n",
      "Trained batch 2554 batch loss 2.95052242 epoch total loss 3.06903625\n",
      "Trained batch 2555 batch loss 3.1799159 epoch total loss 3.0690794\n",
      "Trained batch 2556 batch loss 2.82489491 epoch total loss 3.06898379\n",
      "Trained batch 2557 batch loss 2.9508431 epoch total loss 3.06893754\n",
      "Trained batch 2558 batch loss 2.76594305 epoch total loss 3.06881928\n",
      "Trained batch 2559 batch loss 2.98859167 epoch total loss 3.06878805\n",
      "Trained batch 2560 batch loss 3.23366547 epoch total loss 3.06885242\n",
      "Trained batch 2561 batch loss 3.22460246 epoch total loss 3.06891322\n",
      "Trained batch 2562 batch loss 2.85512781 epoch total loss 3.06882977\n",
      "Trained batch 2563 batch loss 2.79004622 epoch total loss 3.06872106\n",
      "Trained batch 2564 batch loss 2.88024664 epoch total loss 3.06864762\n",
      "Trained batch 2565 batch loss 2.98301911 epoch total loss 3.06861401\n",
      "Trained batch 2566 batch loss 3.04150939 epoch total loss 3.06860352\n",
      "Trained batch 2567 batch loss 2.76979494 epoch total loss 3.06848717\n",
      "Trained batch 2568 batch loss 2.80461454 epoch total loss 3.06838441\n",
      "Trained batch 2569 batch loss 2.8585465 epoch total loss 3.06830263\n",
      "Trained batch 2570 batch loss 2.75416732 epoch total loss 3.06818056\n",
      "Trained batch 2571 batch loss 2.8797555 epoch total loss 3.06810737\n",
      "Trained batch 2572 batch loss 3.06407762 epoch total loss 3.0681057\n",
      "Trained batch 2573 batch loss 2.90270066 epoch total loss 3.06804156\n",
      "Trained batch 2574 batch loss 2.8989954 epoch total loss 3.06797576\n",
      "Trained batch 2575 batch loss 3.00725579 epoch total loss 3.06795216\n",
      "Trained batch 2576 batch loss 2.75976515 epoch total loss 3.06783271\n",
      "Trained batch 2577 batch loss 2.76746798 epoch total loss 3.06771612\n",
      "Trained batch 2578 batch loss 2.74042177 epoch total loss 3.06758904\n",
      "Trained batch 2579 batch loss 2.76225305 epoch total loss 3.06747055\n",
      "Trained batch 2580 batch loss 2.79061913 epoch total loss 3.06736326\n",
      "Trained batch 2581 batch loss 2.69709373 epoch total loss 3.06722\n",
      "Trained batch 2582 batch loss 2.70309424 epoch total loss 3.06707883\n",
      "Trained batch 2583 batch loss 2.84767699 epoch total loss 3.06699395\n",
      "Trained batch 2584 batch loss 2.91205764 epoch total loss 3.06693411\n",
      "Trained batch 2585 batch loss 2.95973396 epoch total loss 3.06689262\n",
      "Trained batch 2586 batch loss 3.04228473 epoch total loss 3.06688309\n",
      "Trained batch 2587 batch loss 3.15283871 epoch total loss 3.06691647\n",
      "Trained batch 2588 batch loss 2.96065331 epoch total loss 3.06687522\n",
      "Trained batch 2589 batch loss 3.24285364 epoch total loss 3.06694317\n",
      "Trained batch 2590 batch loss 3.01577711 epoch total loss 3.06692338\n",
      "Trained batch 2591 batch loss 2.747967 epoch total loss 3.06680036\n",
      "Trained batch 2592 batch loss 2.94991326 epoch total loss 3.06675506\n",
      "Trained batch 2593 batch loss 3.09220982 epoch total loss 3.06676507\n",
      "Trained batch 2594 batch loss 2.9393363 epoch total loss 3.06671596\n",
      "Trained batch 2595 batch loss 2.97591186 epoch total loss 3.06668091\n",
      "Trained batch 2596 batch loss 2.9412241 epoch total loss 3.06663275\n",
      "Trained batch 2597 batch loss 2.84125686 epoch total loss 3.06654596\n",
      "Trained batch 2598 batch loss 2.94285083 epoch total loss 3.06649828\n",
      "Trained batch 2599 batch loss 2.86526489 epoch total loss 3.06642079\n",
      "Trained batch 2600 batch loss 2.80306196 epoch total loss 3.0663197\n",
      "Trained batch 2601 batch loss 2.89910078 epoch total loss 3.06625533\n",
      "Trained batch 2602 batch loss 2.81404543 epoch total loss 3.06615829\n",
      "Trained batch 2603 batch loss 2.95633054 epoch total loss 3.06611633\n",
      "Trained batch 2604 batch loss 2.71682286 epoch total loss 3.0659821\n",
      "Trained batch 2605 batch loss 2.68126798 epoch total loss 3.06583428\n",
      "Trained batch 2606 batch loss 2.89563131 epoch total loss 3.06576896\n",
      "Trained batch 2607 batch loss 2.92585564 epoch total loss 3.06571531\n",
      "Trained batch 2608 batch loss 2.9850316 epoch total loss 3.06568432\n",
      "Trained batch 2609 batch loss 2.7705574 epoch total loss 3.06557107\n",
      "Trained batch 2610 batch loss 2.83896208 epoch total loss 3.06548429\n",
      "Trained batch 2611 batch loss 2.90269923 epoch total loss 3.06542206\n",
      "Trained batch 2612 batch loss 3.07821465 epoch total loss 3.06542683\n",
      "Trained batch 2613 batch loss 2.84552 epoch total loss 3.0653429\n",
      "Trained batch 2614 batch loss 2.89082336 epoch total loss 3.06527591\n",
      "Trained batch 2615 batch loss 2.9801743 epoch total loss 3.06524324\n",
      "Trained batch 2616 batch loss 2.91644835 epoch total loss 3.0651865\n",
      "Trained batch 2617 batch loss 3.15532446 epoch total loss 3.06522083\n",
      "Trained batch 2618 batch loss 2.79695845 epoch total loss 3.06511831\n",
      "Trained batch 2619 batch loss 2.8095274 epoch total loss 3.0650208\n",
      "Trained batch 2620 batch loss 2.78402281 epoch total loss 3.06491375\n",
      "Trained batch 2621 batch loss 2.57762313 epoch total loss 3.06472778\n",
      "Trained batch 2622 batch loss 2.91227412 epoch total loss 3.06466961\n",
      "Trained batch 2623 batch loss 2.87402534 epoch total loss 3.06459689\n",
      "Trained batch 2624 batch loss 2.78268886 epoch total loss 3.06448936\n",
      "Trained batch 2625 batch loss 2.72249508 epoch total loss 3.06435919\n",
      "Trained batch 2626 batch loss 2.9361794 epoch total loss 3.06431031\n",
      "Trained batch 2627 batch loss 2.91472387 epoch total loss 3.06425333\n",
      "Trained batch 2628 batch loss 3.12098598 epoch total loss 3.06427503\n",
      "Trained batch 2629 batch loss 3.06751156 epoch total loss 3.06427622\n",
      "Trained batch 2630 batch loss 2.5303812 epoch total loss 3.06407309\n",
      "Trained batch 2631 batch loss 2.73988533 epoch total loss 3.06394982\n",
      "Trained batch 2632 batch loss 2.65486217 epoch total loss 3.06379437\n",
      "Trained batch 2633 batch loss 2.56966853 epoch total loss 3.06360674\n",
      "Trained batch 2634 batch loss 2.75197053 epoch total loss 3.06348848\n",
      "Trained batch 2635 batch loss 2.9664197 epoch total loss 3.06345153\n",
      "Trained batch 2636 batch loss 2.95983553 epoch total loss 3.06341219\n",
      "Trained batch 2637 batch loss 2.99617338 epoch total loss 3.06338668\n",
      "Trained batch 2638 batch loss 2.9641552 epoch total loss 3.06334925\n",
      "Trained batch 2639 batch loss 3.00134325 epoch total loss 3.06332588\n",
      "Trained batch 2640 batch loss 2.649719 epoch total loss 3.06316924\n",
      "Trained batch 2641 batch loss 3.1914618 epoch total loss 3.06321764\n",
      "Trained batch 2642 batch loss 3.08432174 epoch total loss 3.06322575\n",
      "Trained batch 2643 batch loss 3.09577537 epoch total loss 3.06323814\n",
      "Trained batch 2644 batch loss 3.00628 epoch total loss 3.06321645\n",
      "Trained batch 2645 batch loss 2.83201265 epoch total loss 3.06312919\n",
      "Trained batch 2646 batch loss 2.8258 epoch total loss 3.0630393\n",
      "Trained batch 2647 batch loss 2.89886069 epoch total loss 3.06297731\n",
      "Trained batch 2648 batch loss 3.12883949 epoch total loss 3.06300235\n",
      "Trained batch 2649 batch loss 2.74021196 epoch total loss 3.06288052\n",
      "Trained batch 2650 batch loss 2.85514355 epoch total loss 3.06280208\n",
      "Trained batch 2651 batch loss 2.96250653 epoch total loss 3.06276417\n",
      "Trained batch 2652 batch loss 3.05812407 epoch total loss 3.06276226\n",
      "Trained batch 2653 batch loss 3.2012713 epoch total loss 3.06281447\n",
      "Trained batch 2654 batch loss 2.79409647 epoch total loss 3.06271315\n",
      "Trained batch 2655 batch loss 2.80785036 epoch total loss 3.06261706\n",
      "Trained batch 2656 batch loss 2.99847841 epoch total loss 3.06259298\n",
      "Trained batch 2657 batch loss 2.8565712 epoch total loss 3.0625155\n",
      "Trained batch 2658 batch loss 2.7820096 epoch total loss 3.06240988\n",
      "Trained batch 2659 batch loss 2.901788 epoch total loss 3.06234956\n",
      "Trained batch 2660 batch loss 2.88777137 epoch total loss 3.06228399\n",
      "Trained batch 2661 batch loss 3.08837032 epoch total loss 3.06229377\n",
      "Trained batch 2662 batch loss 3.04885817 epoch total loss 3.06228876\n",
      "Trained batch 2663 batch loss 2.89078474 epoch total loss 3.06222415\n",
      "Trained batch 2664 batch loss 2.82933903 epoch total loss 3.06213665\n",
      "Trained batch 2665 batch loss 2.65547562 epoch total loss 3.06198406\n",
      "Trained batch 2666 batch loss 2.88057065 epoch total loss 3.06191587\n",
      "Trained batch 2667 batch loss 2.71465397 epoch total loss 3.0617857\n",
      "Trained batch 2668 batch loss 2.65594697 epoch total loss 3.06163359\n",
      "Trained batch 2669 batch loss 2.59521055 epoch total loss 3.06145883\n",
      "Trained batch 2670 batch loss 2.44633579 epoch total loss 3.06122851\n",
      "Trained batch 2671 batch loss 2.51786804 epoch total loss 3.06102514\n",
      "Trained batch 2672 batch loss 2.26239514 epoch total loss 3.06072617\n",
      "Trained batch 2673 batch loss 2.43241978 epoch total loss 3.06049109\n",
      "Trained batch 2674 batch loss 2.62000775 epoch total loss 3.06032658\n",
      "Trained batch 2675 batch loss 2.62776661 epoch total loss 3.06016493\n",
      "Trained batch 2676 batch loss 2.60618377 epoch total loss 3.05999517\n",
      "Trained batch 2677 batch loss 2.24758744 epoch total loss 3.05969167\n",
      "Trained batch 2678 batch loss 2.31006432 epoch total loss 3.05941176\n",
      "Trained batch 2679 batch loss 2.23760939 epoch total loss 3.05910492\n",
      "Trained batch 2680 batch loss 2.27501869 epoch total loss 3.05881238\n",
      "Trained batch 2681 batch loss 2.32764959 epoch total loss 3.05853987\n",
      "Trained batch 2682 batch loss 2.12329316 epoch total loss 3.05819106\n",
      "Trained batch 2683 batch loss 2.20362329 epoch total loss 3.05787277\n",
      "Trained batch 2684 batch loss 2.82417822 epoch total loss 3.05778575\n",
      "Trained batch 2685 batch loss 2.93021321 epoch total loss 3.0577383\n",
      "Trained batch 2686 batch loss 3.14353728 epoch total loss 3.05777025\n",
      "Trained batch 2687 batch loss 2.96372104 epoch total loss 3.0577352\n",
      "Trained batch 2688 batch loss 2.96721268 epoch total loss 3.05770159\n",
      "Trained batch 2689 batch loss 2.62612367 epoch total loss 3.05754089\n",
      "Trained batch 2690 batch loss 2.7316947 epoch total loss 3.05741978\n",
      "Trained batch 2691 batch loss 2.68380833 epoch total loss 3.05728078\n",
      "Trained batch 2692 batch loss 2.52331734 epoch total loss 3.05708241\n",
      "Trained batch 2693 batch loss 2.96873736 epoch total loss 3.05704975\n",
      "Trained batch 2694 batch loss 2.80056739 epoch total loss 3.05695462\n",
      "Trained batch 2695 batch loss 2.7500186 epoch total loss 3.05684066\n",
      "Trained batch 2696 batch loss 2.85455227 epoch total loss 3.05676556\n",
      "Trained batch 2697 batch loss 2.89413762 epoch total loss 3.05670547\n",
      "Trained batch 2698 batch loss 3.00239778 epoch total loss 3.05668521\n",
      "Trained batch 2699 batch loss 2.98083282 epoch total loss 3.05665684\n",
      "Trained batch 2700 batch loss 3.15795708 epoch total loss 3.05669451\n",
      "Trained batch 2701 batch loss 2.79726982 epoch total loss 3.05659842\n",
      "Trained batch 2702 batch loss 2.80638599 epoch total loss 3.05650592\n",
      "Trained batch 2703 batch loss 2.65296483 epoch total loss 3.05635667\n",
      "Trained batch 2704 batch loss 2.77107549 epoch total loss 3.05625129\n",
      "Trained batch 2705 batch loss 2.87093949 epoch total loss 3.05618286\n",
      "Trained batch 2706 batch loss 2.73245335 epoch total loss 3.05606318\n",
      "Trained batch 2707 batch loss 2.74413919 epoch total loss 3.05594802\n",
      "Trained batch 2708 batch loss 2.67256308 epoch total loss 3.0558064\n",
      "Trained batch 2709 batch loss 2.50206113 epoch total loss 3.05560207\n",
      "Trained batch 2710 batch loss 2.99375725 epoch total loss 3.05557942\n",
      "Trained batch 2711 batch loss 3.13924 epoch total loss 3.05561042\n",
      "Trained batch 2712 batch loss 2.83554316 epoch total loss 3.05552936\n",
      "Trained batch 2713 batch loss 2.77302146 epoch total loss 3.05542541\n",
      "Trained batch 2714 batch loss 2.87806463 epoch total loss 3.05536\n",
      "Trained batch 2715 batch loss 2.51256347 epoch total loss 3.05516\n",
      "Trained batch 2716 batch loss 2.86781931 epoch total loss 3.05509138\n",
      "Trained batch 2717 batch loss 2.85751057 epoch total loss 3.05501842\n",
      "Trained batch 2718 batch loss 2.80818701 epoch total loss 3.05492783\n",
      "Trained batch 2719 batch loss 2.76912689 epoch total loss 3.05482292\n",
      "Trained batch 2720 batch loss 2.59482336 epoch total loss 3.05465364\n",
      "Trained batch 2721 batch loss 2.79765 epoch total loss 3.05455947\n",
      "Trained batch 2722 batch loss 2.84657693 epoch total loss 3.05448294\n",
      "Trained batch 2723 batch loss 2.82845736 epoch total loss 3.0544\n",
      "Trained batch 2724 batch loss 2.98921847 epoch total loss 3.05437589\n",
      "Trained batch 2725 batch loss 2.5219593 epoch total loss 3.05418038\n",
      "Trained batch 2726 batch loss 2.69848275 epoch total loss 3.05404973\n",
      "Trained batch 2727 batch loss 2.58130407 epoch total loss 3.0538764\n",
      "Trained batch 2728 batch loss 2.49893761 epoch total loss 3.05367303\n",
      "Trained batch 2729 batch loss 2.5984149 epoch total loss 3.05350614\n",
      "Trained batch 2730 batch loss 2.77732134 epoch total loss 3.05340505\n",
      "Trained batch 2731 batch loss 2.87475896 epoch total loss 3.05333972\n",
      "Trained batch 2732 batch loss 2.67167759 epoch total loss 3.05320024\n",
      "Trained batch 2733 batch loss 2.51416111 epoch total loss 3.05300307\n",
      "Trained batch 2734 batch loss 2.81154442 epoch total loss 3.05291486\n",
      "Trained batch 2735 batch loss 3.12090397 epoch total loss 3.05293965\n",
      "Trained batch 2736 batch loss 3.11398983 epoch total loss 3.05296206\n",
      "Trained batch 2737 batch loss 2.6722126 epoch total loss 3.05282283\n",
      "Trained batch 2738 batch loss 2.81071353 epoch total loss 3.05273438\n",
      "Trained batch 2739 batch loss 2.72102261 epoch total loss 3.05261326\n",
      "Trained batch 2740 batch loss 2.53753591 epoch total loss 3.05242491\n",
      "Trained batch 2741 batch loss 2.73233938 epoch total loss 3.05230832\n",
      "Trained batch 2742 batch loss 2.75958538 epoch total loss 3.05220151\n",
      "Trained batch 2743 batch loss 2.8471272 epoch total loss 3.05212665\n",
      "Trained batch 2744 batch loss 3.28866434 epoch total loss 3.05221295\n",
      "Trained batch 2745 batch loss 2.80694366 epoch total loss 3.05212355\n",
      "Trained batch 2746 batch loss 2.60631132 epoch total loss 3.05196118\n",
      "Trained batch 2747 batch loss 2.3935194 epoch total loss 3.05172157\n",
      "Trained batch 2748 batch loss 2.80023551 epoch total loss 3.05162978\n",
      "Trained batch 2749 batch loss 2.85476613 epoch total loss 3.05155826\n",
      "Trained batch 2750 batch loss 2.63114166 epoch total loss 3.05140519\n",
      "Trained batch 2751 batch loss 2.76542 epoch total loss 3.05130124\n",
      "Trained batch 2752 batch loss 2.96092319 epoch total loss 3.05126858\n",
      "Trained batch 2753 batch loss 2.91797304 epoch total loss 3.05122\n",
      "Trained batch 2754 batch loss 3.15377808 epoch total loss 3.05125713\n",
      "Trained batch 2755 batch loss 2.84346199 epoch total loss 3.05118179\n",
      "Trained batch 2756 batch loss 2.45379496 epoch total loss 3.05096507\n",
      "Trained batch 2757 batch loss 2.21058035 epoch total loss 3.05066037\n",
      "Trained batch 2758 batch loss 2.56327343 epoch total loss 3.0504837\n",
      "Trained batch 2759 batch loss 2.21369529 epoch total loss 3.05018067\n",
      "Trained batch 2760 batch loss 2.38073683 epoch total loss 3.04993796\n",
      "Trained batch 2761 batch loss 2.48305511 epoch total loss 3.04973292\n",
      "Trained batch 2762 batch loss 2.85999441 epoch total loss 3.04966426\n",
      "Trained batch 2763 batch loss 3.05412483 epoch total loss 3.04966569\n",
      "Trained batch 2764 batch loss 2.45840263 epoch total loss 3.04945183\n",
      "Trained batch 2765 batch loss 2.46040726 epoch total loss 3.04923844\n",
      "Trained batch 2766 batch loss 2.54170895 epoch total loss 3.0490551\n",
      "Trained batch 2767 batch loss 2.72830057 epoch total loss 3.04893923\n",
      "Trained batch 2768 batch loss 2.80683708 epoch total loss 3.04885173\n",
      "Trained batch 2769 batch loss 2.805161 epoch total loss 3.04876351\n",
      "Trained batch 2770 batch loss 2.85533094 epoch total loss 3.0486939\n",
      "Trained batch 2771 batch loss 2.71082687 epoch total loss 3.04857183\n",
      "Trained batch 2772 batch loss 3.01573777 epoch total loss 3.04856014\n",
      "Trained batch 2773 batch loss 2.90460134 epoch total loss 3.04850793\n",
      "Trained batch 2774 batch loss 3.05826807 epoch total loss 3.04851174\n",
      "Trained batch 2775 batch loss 2.69074345 epoch total loss 3.04838252\n",
      "Trained batch 2776 batch loss 2.95522261 epoch total loss 3.0483489\n",
      "Trained batch 2777 batch loss 3.2723341 epoch total loss 3.04842973\n",
      "Trained batch 2778 batch loss 3.26247072 epoch total loss 3.04850674\n",
      "Trained batch 2779 batch loss 3.4199996 epoch total loss 3.04864049\n",
      "Trained batch 2780 batch loss 3.4591186 epoch total loss 3.04878807\n",
      "Trained batch 2781 batch loss 3.34450197 epoch total loss 3.04889441\n",
      "Epoch 1 train loss 3.0488944053649902\n",
      "Validated batch 1 batch loss 2.77648616\n",
      "Validated batch 2 batch loss 2.6564312\n",
      "Validated batch 3 batch loss 2.69435573\n",
      "Validated batch 4 batch loss 2.67377186\n",
      "Validated batch 5 batch loss 2.87890172\n",
      "Validated batch 6 batch loss 2.58809233\n",
      "Validated batch 7 batch loss 2.89513254\n",
      "Validated batch 8 batch loss 2.60886407\n",
      "Validated batch 9 batch loss 2.75247931\n",
      "Validated batch 10 batch loss 2.77789974\n",
      "Validated batch 11 batch loss 2.78982115\n",
      "Validated batch 12 batch loss 2.88471127\n",
      "Validated batch 13 batch loss 2.63549614\n",
      "Validated batch 14 batch loss 2.95210052\n",
      "Validated batch 15 batch loss 2.80663633\n",
      "Validated batch 16 batch loss 2.84273863\n",
      "Validated batch 17 batch loss 2.91639876\n",
      "Validated batch 18 batch loss 2.70706\n",
      "Validated batch 19 batch loss 2.67225552\n",
      "Validated batch 20 batch loss 3.25353622\n",
      "Validated batch 21 batch loss 2.96109\n",
      "Validated batch 22 batch loss 2.88201857\n",
      "Validated batch 23 batch loss 3.05275536\n",
      "Validated batch 24 batch loss 2.61321\n",
      "Validated batch 25 batch loss 2.63538671\n",
      "Validated batch 26 batch loss 2.975106\n",
      "Validated batch 27 batch loss 2.6533165\n",
      "Validated batch 28 batch loss 2.88881874\n",
      "Validated batch 29 batch loss 2.85112429\n",
      "Validated batch 30 batch loss 2.89357972\n",
      "Validated batch 31 batch loss 2.77131724\n",
      "Validated batch 32 batch loss 2.96831799\n",
      "Validated batch 33 batch loss 2.97180629\n",
      "Validated batch 34 batch loss 2.9906559\n",
      "Validated batch 35 batch loss 2.63826156\n",
      "Validated batch 36 batch loss 3.06373\n",
      "Validated batch 37 batch loss 2.87184715\n",
      "Validated batch 38 batch loss 2.56409\n",
      "Validated batch 39 batch loss 2.84944534\n",
      "Validated batch 40 batch loss 3.12810326\n",
      "Validated batch 41 batch loss 2.80943608\n",
      "Validated batch 42 batch loss 2.67656\n",
      "Validated batch 43 batch loss 3.01588035\n",
      "Validated batch 44 batch loss 2.55555344\n",
      "Validated batch 45 batch loss 2.74521065\n",
      "Validated batch 46 batch loss 3.02409077\n",
      "Validated batch 47 batch loss 2.86354351\n",
      "Validated batch 48 batch loss 2.95865178\n",
      "Validated batch 49 batch loss 2.61374378\n",
      "Validated batch 50 batch loss 3.00770807\n",
      "Validated batch 51 batch loss 2.57894897\n",
      "Validated batch 52 batch loss 2.79880309\n",
      "Validated batch 53 batch loss 2.48966932\n",
      "Validated batch 54 batch loss 2.85393858\n",
      "Validated batch 55 batch loss 2.73562241\n",
      "Validated batch 56 batch loss 2.8738935\n",
      "Validated batch 57 batch loss 3.23107719\n",
      "Validated batch 58 batch loss 2.73648214\n",
      "Validated batch 59 batch loss 2.48325348\n",
      "Validated batch 60 batch loss 2.80068541\n",
      "Validated batch 61 batch loss 2.70857263\n",
      "Validated batch 62 batch loss 2.93252659\n",
      "Validated batch 63 batch loss 2.7284348\n",
      "Validated batch 64 batch loss 3.00155187\n",
      "Validated batch 65 batch loss 2.75878572\n",
      "Validated batch 66 batch loss 2.83663702\n",
      "Validated batch 67 batch loss 2.71650147\n",
      "Validated batch 68 batch loss 2.90787506\n",
      "Validated batch 69 batch loss 2.638798\n",
      "Validated batch 70 batch loss 2.51015711\n",
      "Validated batch 71 batch loss 2.77991438\n",
      "Validated batch 72 batch loss 3.34753895\n",
      "Validated batch 73 batch loss 2.86470699\n",
      "Validated batch 74 batch loss 2.46264791\n",
      "Validated batch 75 batch loss 2.97943878\n",
      "Validated batch 76 batch loss 2.84181118\n",
      "Validated batch 77 batch loss 2.77398968\n",
      "Validated batch 78 batch loss 2.88480425\n",
      "Validated batch 79 batch loss 2.82287931\n",
      "Validated batch 80 batch loss 2.91791701\n",
      "Validated batch 81 batch loss 2.74499297\n",
      "Validated batch 82 batch loss 2.34244394\n",
      "Validated batch 83 batch loss 2.85475159\n",
      "Validated batch 84 batch loss 2.6569066\n",
      "Validated batch 85 batch loss 2.84082031\n",
      "Validated batch 86 batch loss 2.62624502\n",
      "Validated batch 87 batch loss 2.91430783\n",
      "Validated batch 88 batch loss 2.65499878\n",
      "Validated batch 89 batch loss 2.67968893\n",
      "Validated batch 90 batch loss 2.89272928\n",
      "Validated batch 91 batch loss 2.81982541\n",
      "Validated batch 92 batch loss 2.8096118\n",
      "Validated batch 93 batch loss 2.97887635\n",
      "Validated batch 94 batch loss 3.05335927\n",
      "Validated batch 95 batch loss 3.07924771\n",
      "Validated batch 96 batch loss 2.70987844\n",
      "Validated batch 97 batch loss 2.53237152\n",
      "Validated batch 98 batch loss 2.93519926\n",
      "Validated batch 99 batch loss 2.80154181\n",
      "Validated batch 100 batch loss 2.51043892\n",
      "Validated batch 101 batch loss 2.97039127\n",
      "Validated batch 102 batch loss 2.74332213\n",
      "Validated batch 103 batch loss 2.74558687\n",
      "Validated batch 104 batch loss 2.88723636\n",
      "Validated batch 105 batch loss 2.81460238\n",
      "Validated batch 106 batch loss 2.53966475\n",
      "Validated batch 107 batch loss 2.6562767\n",
      "Validated batch 108 batch loss 2.8886106\n",
      "Validated batch 109 batch loss 2.34645319\n",
      "Validated batch 110 batch loss 3.07942581\n",
      "Validated batch 111 batch loss 3.03961\n",
      "Validated batch 112 batch loss 2.87300348\n",
      "Validated batch 113 batch loss 2.69409609\n",
      "Validated batch 114 batch loss 2.58125114\n",
      "Validated batch 115 batch loss 2.63379693\n",
      "Validated batch 116 batch loss 2.54658318\n",
      "Validated batch 117 batch loss 2.86581612\n",
      "Validated batch 118 batch loss 2.93188953\n",
      "Validated batch 119 batch loss 3.00300527\n",
      "Validated batch 120 batch loss 2.92165327\n",
      "Validated batch 121 batch loss 2.91180658\n",
      "Validated batch 122 batch loss 3.03321624\n",
      "Validated batch 123 batch loss 2.94418287\n",
      "Validated batch 124 batch loss 2.94748664\n",
      "Validated batch 125 batch loss 2.61681938\n",
      "Validated batch 126 batch loss 2.82371902\n",
      "Validated batch 127 batch loss 3.06743789\n",
      "Validated batch 128 batch loss 2.9992466\n",
      "Validated batch 129 batch loss 2.72091365\n",
      "Validated batch 130 batch loss 2.88643694\n",
      "Validated batch 131 batch loss 3.16441274\n",
      "Validated batch 132 batch loss 2.61099815\n",
      "Validated batch 133 batch loss 3.0129261\n",
      "Validated batch 134 batch loss 2.85537839\n",
      "Validated batch 135 batch loss 2.42935896\n",
      "Validated batch 136 batch loss 2.37401247\n",
      "Validated batch 137 batch loss 2.79274797\n",
      "Validated batch 138 batch loss 3.11696506\n",
      "Validated batch 139 batch loss 2.8754034\n",
      "Validated batch 140 batch loss 2.40554619\n",
      "Validated batch 141 batch loss 2.93812275\n",
      "Validated batch 142 batch loss 3.04882622\n",
      "Validated batch 143 batch loss 3.1570518\n",
      "Validated batch 144 batch loss 2.70436716\n",
      "Validated batch 145 batch loss 2.90780544\n",
      "Validated batch 146 batch loss 2.54809713\n",
      "Validated batch 147 batch loss 2.80806875\n",
      "Validated batch 148 batch loss 2.96214151\n",
      "Validated batch 149 batch loss 3.20756459\n",
      "Validated batch 150 batch loss 2.89694595\n",
      "Validated batch 151 batch loss 2.67137456\n",
      "Validated batch 152 batch loss 2.51273799\n",
      "Validated batch 153 batch loss 2.95392132\n",
      "Validated batch 154 batch loss 2.84199953\n",
      "Validated batch 155 batch loss 2.9411664\n",
      "Validated batch 156 batch loss 2.58490896\n",
      "Validated batch 157 batch loss 2.85640812\n",
      "Validated batch 158 batch loss 2.84500813\n",
      "Validated batch 159 batch loss 2.82097292\n",
      "Validated batch 160 batch loss 2.83202243\n",
      "Validated batch 161 batch loss 2.6592474\n",
      "Validated batch 162 batch loss 2.61314344\n",
      "Validated batch 163 batch loss 2.19050169\n",
      "Validated batch 164 batch loss 2.80138445\n",
      "Validated batch 165 batch loss 2.83927751\n",
      "Validated batch 166 batch loss 2.75737095\n",
      "Validated batch 167 batch loss 2.82744408\n",
      "Validated batch 168 batch loss 2.8445425\n",
      "Validated batch 169 batch loss 2.82188654\n",
      "Validated batch 170 batch loss 2.65362811\n",
      "Validated batch 171 batch loss 2.98728824\n",
      "Validated batch 172 batch loss 2.75664568\n",
      "Validated batch 173 batch loss 2.76964045\n",
      "Validated batch 174 batch loss 2.69972563\n",
      "Validated batch 175 batch loss 3.01780128\n",
      "Validated batch 176 batch loss 2.64585209\n",
      "Validated batch 177 batch loss 3.11971664\n",
      "Validated batch 178 batch loss 2.86486769\n",
      "Validated batch 179 batch loss 2.9052124\n",
      "Validated batch 180 batch loss 2.82263851\n",
      "Validated batch 181 batch loss 2.96758533\n",
      "Validated batch 182 batch loss 2.70478582\n",
      "Validated batch 183 batch loss 2.57826614\n",
      "Validated batch 184 batch loss 2.58061695\n",
      "Validated batch 185 batch loss 2.64199352\n",
      "Validated batch 186 batch loss 2.67673206\n",
      "Validated batch 187 batch loss 2.80513239\n",
      "Validated batch 188 batch loss 2.5557673\n",
      "Validated batch 189 batch loss 2.73139\n",
      "Validated batch 190 batch loss 2.74981523\n",
      "Validated batch 191 batch loss 2.84922266\n",
      "Validated batch 192 batch loss 2.6025691\n",
      "Validated batch 193 batch loss 2.80819035\n",
      "Validated batch 194 batch loss 2.59271789\n",
      "Validated batch 195 batch loss 2.83151317\n",
      "Validated batch 196 batch loss 3.06958246\n",
      "Validated batch 197 batch loss 2.67090249\n",
      "Validated batch 198 batch loss 2.96265483\n",
      "Validated batch 199 batch loss 2.77365685\n",
      "Validated batch 200 batch loss 3.16072893\n",
      "Validated batch 201 batch loss 3.15125155\n",
      "Validated batch 202 batch loss 2.8135035\n",
      "Validated batch 203 batch loss 3.04631734\n",
      "Validated batch 204 batch loss 2.7292037\n",
      "Validated batch 205 batch loss 2.87059116\n",
      "Validated batch 206 batch loss 2.79290342\n",
      "Validated batch 207 batch loss 2.91224098\n",
      "Validated batch 208 batch loss 3.15824556\n",
      "Validated batch 209 batch loss 2.90049553\n",
      "Validated batch 210 batch loss 2.91331959\n",
      "Validated batch 211 batch loss 2.91414356\n",
      "Validated batch 212 batch loss 2.96322727\n",
      "Validated batch 213 batch loss 2.95776081\n",
      "Validated batch 214 batch loss 3.05273151\n",
      "Validated batch 215 batch loss 3.0328517\n",
      "Validated batch 216 batch loss 2.89746547\n",
      "Validated batch 217 batch loss 3.07232308\n",
      "Validated batch 218 batch loss 2.84947205\n",
      "Validated batch 219 batch loss 2.89909029\n",
      "Validated batch 220 batch loss 2.32698059\n",
      "Validated batch 221 batch loss 2.82624531\n",
      "Validated batch 222 batch loss 2.89874697\n",
      "Validated batch 223 batch loss 2.96537566\n",
      "Validated batch 224 batch loss 2.81437826\n",
      "Validated batch 225 batch loss 3.12183952\n",
      "Validated batch 226 batch loss 2.75195551\n",
      "Validated batch 227 batch loss 2.7591176\n",
      "Validated batch 228 batch loss 2.95908499\n",
      "Validated batch 229 batch loss 2.8815167\n",
      "Validated batch 230 batch loss 2.66265965\n",
      "Validated batch 231 batch loss 2.83447337\n",
      "Validated batch 232 batch loss 2.82398176\n",
      "Validated batch 233 batch loss 2.97234249\n",
      "Validated batch 234 batch loss 2.79206467\n",
      "Validated batch 235 batch loss 2.61737013\n",
      "Validated batch 236 batch loss 2.67331\n",
      "Validated batch 237 batch loss 2.72798967\n",
      "Validated batch 238 batch loss 3.09132242\n",
      "Validated batch 239 batch loss 3.02837658\n",
      "Validated batch 240 batch loss 2.71690273\n",
      "Validated batch 241 batch loss 2.93524551\n",
      "Validated batch 242 batch loss 2.87502789\n",
      "Validated batch 243 batch loss 2.93771791\n",
      "Validated batch 244 batch loss 3.00376177\n",
      "Validated batch 245 batch loss 2.8642478\n",
      "Validated batch 246 batch loss 3.03368735\n",
      "Validated batch 247 batch loss 2.86637354\n",
      "Validated batch 248 batch loss 2.96196365\n",
      "Validated batch 249 batch loss 2.88360667\n",
      "Validated batch 250 batch loss 2.84069729\n",
      "Validated batch 251 batch loss 2.91851759\n",
      "Validated batch 252 batch loss 3.02558184\n",
      "Validated batch 253 batch loss 3.04636455\n",
      "Validated batch 254 batch loss 2.85678625\n",
      "Validated batch 255 batch loss 2.48937035\n",
      "Validated batch 256 batch loss 3.00583696\n",
      "Validated batch 257 batch loss 2.98218966\n",
      "Validated batch 258 batch loss 2.95655441\n",
      "Validated batch 259 batch loss 3.06795549\n",
      "Validated batch 260 batch loss 2.64438319\n",
      "Validated batch 261 batch loss 2.91323376\n",
      "Validated batch 262 batch loss 2.93573928\n",
      "Validated batch 263 batch loss 2.77197027\n",
      "Validated batch 264 batch loss 3.27071548\n",
      "Validated batch 265 batch loss 2.68870687\n",
      "Validated batch 266 batch loss 2.56238914\n",
      "Validated batch 267 batch loss 2.74966764\n",
      "Validated batch 268 batch loss 2.56548738\n",
      "Validated batch 269 batch loss 2.93595719\n",
      "Validated batch 270 batch loss 2.58544683\n",
      "Validated batch 271 batch loss 2.54185\n",
      "Validated batch 272 batch loss 2.77600837\n",
      "Validated batch 273 batch loss 3.23145294\n",
      "Validated batch 274 batch loss 2.95490026\n",
      "Validated batch 275 batch loss 2.80101776\n",
      "Validated batch 276 batch loss 2.73868155\n",
      "Validated batch 277 batch loss 2.87703872\n",
      "Validated batch 278 batch loss 2.69794226\n",
      "Validated batch 279 batch loss 2.84424138\n",
      "Validated batch 280 batch loss 2.88798714\n",
      "Validated batch 281 batch loss 2.82059193\n",
      "Validated batch 282 batch loss 2.73810101\n",
      "Validated batch 283 batch loss 2.3413806\n",
      "Validated batch 284 batch loss 2.60196185\n",
      "Validated batch 285 batch loss 2.77015924\n",
      "Validated batch 286 batch loss 2.61270189\n",
      "Validated batch 287 batch loss 2.79654455\n",
      "Validated batch 288 batch loss 3.15935278\n",
      "Validated batch 289 batch loss 2.59361982\n",
      "Validated batch 290 batch loss 2.69808912\n",
      "Validated batch 291 batch loss 2.94524288\n",
      "Validated batch 292 batch loss 2.55114794\n",
      "Validated batch 293 batch loss 2.76042128\n",
      "Validated batch 294 batch loss 2.86264396\n",
      "Validated batch 295 batch loss 2.79504418\n",
      "Validated batch 296 batch loss 2.8694706\n",
      "Validated batch 297 batch loss 2.67930341\n",
      "Validated batch 298 batch loss 2.71018839\n",
      "Validated batch 299 batch loss 2.67612553\n",
      "Validated batch 300 batch loss 2.96144176\n",
      "Validated batch 301 batch loss 2.56371832\n",
      "Validated batch 302 batch loss 2.70992\n",
      "Validated batch 303 batch loss 3.13579249\n",
      "Validated batch 304 batch loss 2.64152956\n",
      "Validated batch 305 batch loss 2.8488555\n",
      "Validated batch 306 batch loss 2.95010495\n",
      "Validated batch 307 batch loss 3.04506087\n",
      "Validated batch 308 batch loss 3.03824425\n",
      "Validated batch 309 batch loss 2.73899078\n",
      "Validated batch 310 batch loss 2.74796271\n",
      "Validated batch 311 batch loss 2.9595046\n",
      "Validated batch 312 batch loss 3.0890696\n",
      "Validated batch 313 batch loss 2.72135878\n",
      "Validated batch 314 batch loss 2.61758566\n",
      "Validated batch 315 batch loss 2.53101921\n",
      "Validated batch 316 batch loss 2.7250998\n",
      "Validated batch 317 batch loss 3.09403658\n",
      "Validated batch 318 batch loss 2.68488097\n",
      "Validated batch 319 batch loss 2.67016935\n",
      "Validated batch 320 batch loss 3.02421188\n",
      "Validated batch 321 batch loss 3.04672527\n",
      "Validated batch 322 batch loss 2.98229575\n",
      "Validated batch 323 batch loss 2.98587441\n",
      "Validated batch 324 batch loss 2.99860525\n",
      "Validated batch 325 batch loss 3.02914023\n",
      "Validated batch 326 batch loss 2.85959864\n",
      "Validated batch 327 batch loss 2.80614209\n",
      "Validated batch 328 batch loss 2.8781321\n",
      "Validated batch 329 batch loss 2.72255468\n",
      "Validated batch 330 batch loss 2.82067156\n",
      "Validated batch 331 batch loss 2.77204347\n",
      "Validated batch 332 batch loss 2.85967708\n",
      "Validated batch 333 batch loss 2.6210351\n",
      "Validated batch 334 batch loss 3.00468373\n",
      "Validated batch 335 batch loss 2.713274\n",
      "Validated batch 336 batch loss 2.76407957\n",
      "Validated batch 337 batch loss 2.7900598\n",
      "Validated batch 338 batch loss 2.84173489\n",
      "Validated batch 339 batch loss 2.5993638\n",
      "Validated batch 340 batch loss 2.65297937\n",
      "Validated batch 341 batch loss 2.79576731\n",
      "Validated batch 342 batch loss 2.90138364\n",
      "Validated batch 343 batch loss 2.59995937\n",
      "Validated batch 344 batch loss 2.87833261\n",
      "Validated batch 345 batch loss 2.76038575\n",
      "Validated batch 346 batch loss 2.93865752\n",
      "Validated batch 347 batch loss 2.78775215\n",
      "Validated batch 348 batch loss 2.77127576\n",
      "Validated batch 349 batch loss 2.81625128\n",
      "Validated batch 350 batch loss 2.54201746\n",
      "Validated batch 351 batch loss 2.7901752\n",
      "Validated batch 352 batch loss 2.96447182\n",
      "Validated batch 353 batch loss 2.47228241\n",
      "Validated batch 354 batch loss 2.91459799\n",
      "Validated batch 355 batch loss 2.75854874\n",
      "Validated batch 356 batch loss 2.6671195\n",
      "Validated batch 357 batch loss 3.01036406\n",
      "Validated batch 358 batch loss 2.80833864\n",
      "Validated batch 359 batch loss 3.00771451\n",
      "Validated batch 360 batch loss 2.91393566\n",
      "Validated batch 361 batch loss 3.04385781\n",
      "Validated batch 362 batch loss 3.18050289\n",
      "Validated batch 363 batch loss 3.33225965\n",
      "Validated batch 364 batch loss 3.24284744\n",
      "Validated batch 365 batch loss 2.88653374\n",
      "Validated batch 366 batch loss 2.63988566\n",
      "Validated batch 367 batch loss 3.01638103\n",
      "Validated batch 368 batch loss 2.65975189\n",
      "Validated batch 369 batch loss 2.66461778\n",
      "Validated batch 370 batch loss 2.56972861\n",
      "Epoch 1 val loss 2.822326898574829\n",
      "Model ./models/model-v0.0.1-epoch-1-loss-2.8223.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simplebaseline 모델 학습하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
